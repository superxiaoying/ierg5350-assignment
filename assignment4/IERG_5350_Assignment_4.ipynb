{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "IERG 5350 Assignment 4.ipynb",
      "provenance": [],
      "collapsed_sections": [
        "dw1XwPRD9OLu",
        "KboX9hB5eEVv"
      ]
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dw1XwPRD9OLu"
      },
      "source": [
        "# IERG 5350 Assignment 4: Advanced Algorithms for Continuous Control in RL\n",
        "\n",
        "### Welcome to assignment 4 of our RL course!\n",
        "*2020-2021 Term 1, IERG 5350: Reinforcement Learning. Department of Information Engineering, The Chinese University of Hong Kong. Course Instructor: Professor ZHOU Bolei. Assignment author: PENG Zhenghao, SUN Hao, ZHAN Xiaohang.*\n",
        "\n",
        "\n",
        "| Student Name | Student ID |\n",
        "| :----: | :----: |\n",
        "| Yingjie CAI | 1155139431 |\n",
        "\n",
        "------\n",
        "\n",
        "\n",
        "In this assignment, we will implement a system of RL that allows us to train and evaluate RL agents formally and efficiently.\n",
        "\n",
        "In this notebook, you will go through the following components of the whole system:\n",
        "- Preparation: Colab, and Environment\n",
        "- Section 1: Training with algorithm PPO\n",
        "- Section 2: Training with algorithm DDPG\n",
        "- Section 3: Training with algorithm TD3\n",
        "- Section 4: Transfer your PPO/ DDPG/ TD3 to another task: Four-Solution-Maze\n",
        "\n",
        "The author of this assignment is SUN, Hao (sh018 AT ie.cuhk.edu.hk)."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MyLEr-yOpxvD"
      },
      "source": [
        "# Colab\n",
        "\n",
        "### Introduction to Google Colab: \n",
        "From now on, our assignment as well as the final project will be based on the Google Colab, where you can apply for free GPU resources to accelerate the learning of your RL models. \n",
        "\n",
        "Here are some resources as intro to the Colab.\n",
        "\n",
        "- YouTube Video: https://www.youtube.com/watch?v=inN8seMm7UI\n",
        "- Colab Intro: https://colab.research.google.com/notebooks/intro.ipynb\n",
        "(you may need to login with your google account)\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "# Gym Continuous Control Tasks\n",
        "\n",
        "### Introduction to the Gym Continuous Control Envirionments\n",
        "\n",
        "In the last assignment, you have already used the gym[atari] benchmarks, where the action space is discrete so that normal approach is value-based methods e.g., DQN.\n",
        "\n",
        "In this assignment, we will try to implement three prevailing RL algorithms for continuous control tasks, namely the PPO(https://arxiv.org/abs/1707.06347), DDPG(https://arxiv.org/abs/1509.02971) and TD3(https://arxiv.org/abs/1802.09477).\n",
        "\n",
        "We will now begin with a gym environment for continuous control,\n",
        "\n",
        "The Pendulum-v0\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 69
        },
        "id": "2CjdPG_oqT1l",
        "outputId": "ec780675-8ec6-44ee-d64c-82ceada63ccc"
      },
      "source": [
        "import gym\n",
        "ENV_NAME = \"Pendulum-v0\"\n",
        "env = gym.make(ENV_NAME)\n",
        "state = env.reset()\n",
        "print('the state space is like', state)\n",
        "print('the max and min action is: ',env.action_space.high,env.action_space.low)\n",
        "\n",
        "'''so that you may need to use action value re-size if you want to use the tanh activation functions'''"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "the state space is like [-0.16154317  0.98686565 -0.98187255]\n",
            "the max and min action is:  [2.] [-2.]\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'so that you may need to use action value re-size if you want to use the tanh activation functions'"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 1
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "b7IHoYJWur1S"
      },
      "source": [
        "# PPO \n",
        "\n",
        "The Proximal Policy Optimization Algorithms is the most prevailing on-policy learning method. Although its sample efficiency is not as high as the off-policy methods, the PPO is relatively easy to implement and the learning is much more stable than off-policy methods. Whenever you have a task you want to try whether RL works, you may try to run a PPO agent at first. It is worth mentioning even the most challenging game, the StarCraftII agent AlphaStar is trained based on PPO (with lots of improvements, ofcourse).\n",
        "\n",
        "\n",
        "## TODOs for You\n",
        "The ppo has the benfitsof trust region policy optimization (TRPO) but is much simpler to implement, and with some implementation engeneering, the sample complexity of TRPO is further improved.\n",
        "\n",
        "The key idea of PPO optimization is *Not Optimize the Policy Too Much in a Certain Step*, which follows the key insight of the method of TRPO.\n",
        "\n",
        "In TRPO, the optimization objective of policy is to learn a policy such that \n",
        "\n",
        "$$\\max_\\theta \\hat{\\mathbb{E}}_t [\\frac{\\pi_\\theta(a_t|s_t)}{\\pi_{\\theta_{old}}(a_t|s_t)}\\hat{A}_t]$$\n",
        "\n",
        "subject to \n",
        "\n",
        "$$\\hat{\\mathbb{E}}_t[KL[\\pi_{\\theta_{old}}(\\cdot|s_t),\\pi_\\theta(\\cdot|s_t)]] \\le \\delta$$\n",
        "\n",
        "where $\\hat{A}$ denotes the advantage function, rather than optimize the objective function of \n",
        "\n",
        "$$L^{PG}(\\theta) = \\hat{\\mathbb{E}}_t[\\log \\pi_\\theta(a_t|s_t)\\hat{A}_t]$$\n",
        "\n",
        "in the normal policy gradint methods.\n",
        "\n",
        "The PPO proposed two alternative approaches to solve the constrained optimization above, namely the Clipped Surrogated Objective and the Adaptive KL penalty Coefficient. The former one is more generally used in practice as it's more convenient to implement, more efficient and owns stable performance.\n",
        "\n",
        "The Clipped Surrogated Objective approach replace the surrogate objective\n",
        "\n",
        "$$L^{CPI}(\\theta) = \\hat{\\mathbb{E}}_t[\\frac{\\pi_\\theta(a_t|s_t)}{\\pi_{\\theta_{old}}(a_t|s_t)}\\hat{A}_t] = \\hat{\\mathbb{E}}_t[r_t(\\theta)\\hat{A}_t]$$\n",
        "\n",
        "of TRPO (CPI: Conservative Policy Iteration) by \n",
        "\n",
        "$$L^{CLIP}(\\theta) = \\hat{\\mathbb{E}}_t[\\min(r_t(\\theta)\\hat{A}_t,clip(r_t(\\theta),1-\\epsilon, 1+\\epsilon)\\hat{A}_t)]$$\n",
        "\n",
        "You can check that $L^{CLIP}(\\theta) = L^{CPI}(\\theta)$ around the old policy parameter $\\theta_{old}$, i.e., when r = 1.\n",
        "\n",
        "## TODOs here:\n",
        "\n",
        "In this section, your task is to finish the code of a PPO algorithm and evaluate its performance in the Pendulum-v0 environment.\n",
        "\n",
        "Specifically, you need to\n",
        "- Q1. finish building up the ActorCritic ''\\__init__'' function, i.e., build up the neural network.\n",
        "- Q2. finish the foward function, in this part, there are two functions need to finish: the \\_forward_actor function and the \\_forward_critic function\n",
        "- Q3. finish the select_action function, which is called during interacting with the environment, so that you may need to return an action as well as the (log-)probability of getting that action for future optimization\n",
        "- Q4. finish the optimization steps for your PPO agent, that means you need to build up the surrogate loss through your saved tuples in previous episodes and optimize it with current network parameters.\n",
        "- Q5. finally, you may need to optimize some of the hyper-parameters to have a better task performance. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hCkX9dWFvXxa"
      },
      "source": [
        "# You need not to rivese this unless you want to try other hyper-parameter settings\n",
        "# in which case you may revise the default values of class args()\n",
        "from IPython import display\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as opt\n",
        "from torch import Tensor\n",
        "from torch.autograd import Variable\n",
        "from collections import namedtuple\n",
        "from itertools import count\n",
        "import torch.nn.functional as F\n",
        "import matplotlib.pyplot as plt\n",
        "from os.path import join as joindir\n",
        "from os import makedirs as mkdir\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import argparse\n",
        "import datetime\n",
        "import math\n",
        "import random\n",
        "\n",
        "Transition = namedtuple('Transition', ('state', 'value', 'action', 'logproba', 'mask', 'next_state', 'reward'))\n",
        "env = gym.make(ENV_NAME)\n",
        "env.reset()\n",
        "\n",
        "EPS = 1e-10 # you may need this tiny value somewhere, and think about why?\n",
        "RESULT_DIR = 'Result_PPO'\n",
        "mkdir(RESULT_DIR, exist_ok=True)\n",
        "mkdir(ENV_NAME.split('-')[0]+'/CheckPoints', exist_ok=True)\n",
        "mkdir(ENV_NAME.split('-')[0]+'/Rwds', exist_ok=True)\n",
        "rwds = []\n",
        "rwds_history = []\n",
        "\n",
        "class args(object):\n",
        "    repeat = 'repeat'\n",
        "    hid_num = 256\n",
        "    drop_prob = 0.1\n",
        "    env_name = ENV_NAME\n",
        "    seed = 1234\n",
        "    num_episode = 1000\n",
        "    batch_size = 5120\n",
        "    max_step_per_round = 2000\n",
        "    gamma = 0.995\n",
        "    lamda = 0.97\n",
        "    log_num_episode = 1\n",
        "    num_epoch = 10\n",
        "    minibatch_size = 256\n",
        "    clip = 0.2\n",
        "    loss_coeff_value = 0.5\n",
        "    loss_coeff_entropy = 0.01\n",
        "    lr = 3e-4 # 3e-4\n",
        "    num_parallel_run = 1\n",
        "    # tricks\n",
        "    schedule_adam = 'linear'\n",
        "    schedule_clip = 'linear'\n",
        "    layer_norm = True\n",
        "    state_norm = False\n",
        "    advantage_norm = True\n",
        "    lossvalue_norm = True"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "H0zwluYyx-y_"
      },
      "source": [
        " # You need not to rivese this, these classes are used for normalization\n",
        "class RunningStat(object):\n",
        "    def __init__(self, shape):\n",
        "        self._n = 0\n",
        "        self._M = np.zeros(shape)\n",
        "        self._S = np.zeros(shape)\n",
        "\n",
        "    def push(self, x):\n",
        "        x = np.asarray(x)\n",
        "        assert x.shape == self._M.shape\n",
        "        self._n += 1\n",
        "        if self._n == 1:\n",
        "            self._M[...] = x\n",
        "        else:\n",
        "            oldM = self._M.copy()\n",
        "            self._M[...] = oldM + (x - oldM) / self._n\n",
        "            self._S[...] = self._S + (x - oldM) * (x - self._M)\n",
        "\n",
        "    @property\n",
        "    def n(self):\n",
        "        return self._n\n",
        "\n",
        "    @property\n",
        "    def mean(self):\n",
        "        return self._M\n",
        "\n",
        "    @property\n",
        "    def var(self):\n",
        "        return self._S / (self._n - 1) if self._n > 1 else np.square(self._M)\n",
        "\n",
        "    @property\n",
        "    def std(self):\n",
        "        return np.sqrt(self.var)\n",
        "\n",
        "    @property\n",
        "    def shape(self):\n",
        "        return self._M.shape\n",
        "\n",
        "\n",
        "class ZFilter:\n",
        "    \"\"\"\n",
        "    y = (x-mean)/std\n",
        "    using running estimates of mean,std\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, shape, demean=True, destd=True, clip=10.0):\n",
        "        self.demean = demean\n",
        "        self.destd = destd\n",
        "        self.clip = clip\n",
        "\n",
        "        self.rs = RunningStat(shape)\n",
        "\n",
        "    def __call__(self, x, update=True):\n",
        "        if update: self.rs.push(x)\n",
        "        if self.demean:\n",
        "            x = x - self.rs.mean\n",
        "        if self.destd:\n",
        "            x = x / (self.rs.std + 1e-8)\n",
        "        if self.clip:\n",
        "            x = np.clip(x, -self.clip, self.clip)\n",
        "        return x\n",
        "\n",
        "    def output_shape(self, input_space):\n",
        "        return input_space.shape"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "JDqYISHHyQve",
        "outputId": "22b168b5-7d74-4112-d8ee-2b0e1ce09875"
      },
      "source": [
        "# Here, you need to finish the first 5 tasks.\n",
        "class ActorCritic(nn.Module):\n",
        "    def __init__(self, num_inputs, num_outputs, layer_norm=True):\n",
        "        super(ActorCritic, self).__init__()\n",
        "        '''\n",
        "        Q1:\n",
        "        Initialize your networks\n",
        "        '''\n",
        "        self.actor_fc1 = nn.Linear(num_inputs, 32)\n",
        "        self.actor_fc2 = nn.Linear(32, args.hid_num)\n",
        "        self.actor_fc3 = nn.Linear(args.hid_num, num_outputs)\n",
        "        self.actor_logstd = nn.Parameter(torch.zeros(1, num_outputs))\n",
        "\n",
        "        self.critic_fc1 = nn.Linear(num_inputs, 64)\n",
        "        self.critic_fc2 = nn.Linear(64, 64)\n",
        "        self.critic_fc3 = nn.Linear(64, 1)\n",
        "\n",
        "        if layer_norm:\n",
        "            self.layer_norm(self.actor_fc1, std=1.0)\n",
        "            self.layer_norm(self.actor_fc2, std=1.0)\n",
        "            self.layer_norm(self.actor_fc3, std=0.01)\n",
        "\n",
        "            self.layer_norm(self.critic_fc1, std=1.0)\n",
        "            self.layer_norm(self.critic_fc2, std=1.0)\n",
        "            self.layer_norm(self.critic_fc3, std=1.0)\n",
        "\n",
        "    @staticmethod\n",
        "    def layer_norm(layer, std=1.0, bias_const=0.0):\n",
        "        torch.nn.init.orthogonal_(layer.weight, std)\n",
        "        torch.nn.init.constant_(layer.bias, bias_const)\n",
        "\n",
        "    def forward(self, states):\n",
        "        \"\"\"\n",
        "        Q2.1:\n",
        "        run policy network (actor) as well as value network (critic)\n",
        "        :param states: a tensor represents states\n",
        "        :return: 3 Tensor2\n",
        "        your _forward_actor() function should return both the mean value of action and the log-standard deviation of the action\n",
        "        \"\"\"\n",
        "\n",
        "        action_mean, action_logstd = self._forward_actor(states)\n",
        "        critic_value = self._forward_critic(states)\n",
        "        return action_mean, action_logstd, critic_value\n",
        "\n",
        "    def _forward_actor(self, states):\n",
        "        '''\n",
        "        Q2.2:\n",
        "        build something like \n",
        "        x = activation (actor_fc(state))\n",
        "        the logstd output has already been provided\n",
        "        '''\n",
        "        x = torch.tanh(self.actor_fc1(states))\n",
        "        x = torch.tanh(self.actor_fc2(x))\n",
        "        x = F.dropout(x, p=args.drop_prob, training=self.training)\n",
        "        action_mean = torch.tanh(self.actor_fc3(x))\n",
        "        action_logstd = self.actor_logstd.expand_as(action_mean)\n",
        "        return action_mean, action_logstd\n",
        "\n",
        "    def _forward_critic(self, states):\n",
        "        '''\n",
        "        Q2.3:\n",
        "        build something like \n",
        "        x = activation (critic_fc(state))'''\n",
        "        x = torch.tanh(self.critic_fc1(states))\n",
        "        x = torch.tanh(self.critic_fc2(x))\n",
        "        critic_value = self.critic_fc3(x)\n",
        "        return critic_value\n",
        "\n",
        "    def select_action(self, action_mean, action_logstd, return_logproba=True):\n",
        "        \"\"\"\n",
        "        Q3.1:\n",
        "        given mean and std, sample an action from normal(mean, std)\n",
        "        also returns probability of the given chosen\n",
        "        \"\"\"\n",
        "        action_std = torch.exp(action_logstd)\n",
        "        action = torch.normal(action_mean, action_std)\n",
        "        if return_logproba:\n",
        "            logproba = self._normal_logproba(action, action_mean, action_logstd, action_std)\n",
        "        return action, logproba\n",
        "\n",
        "    @staticmethod\n",
        "    def _normal_logproba(x, mean, logstd, std=None):\n",
        "        '''\n",
        "        Q3.2:\n",
        "        given a mean and logstd of a gaussian,\n",
        "        calculate the log-probability of a given x'''\n",
        "        if std is None:\n",
        "            std = torch.exp(logstd)\n",
        "\n",
        "        std_sq = std.pow(2)\n",
        "        logproba = - 0.5 * math.log(2 * math.pi) - logstd - (x - mean).pow(2) / (2 * std_sq)\n",
        "        return logproba.sum(1)\n",
        "\n",
        "    def get_logproba(self, states, actions):\n",
        "        \"\"\"\n",
        "        return probability of chosen the given actions under corresponding states of current network\n",
        "        :param states: Tensor\n",
        "        :param actions: Tensor\n",
        "        \"\"\"\n",
        "        action_mean, action_logstd = self._forward_actor(states)\n",
        "        action_mean = action_mean.cpu()\n",
        "        action_logstd = action_logstd.cpu()\n",
        "        logproba = self._normal_logproba(actions, action_mean, action_logstd)\n",
        "        return logproba\n",
        "\n",
        "\n",
        "class Memory(object):\n",
        "    def __init__(self):\n",
        "        self.memory = []\n",
        "\n",
        "    def push(self, *args):\n",
        "        self.memory.append(Transition(*args))\n",
        "\n",
        "    def sample(self):\n",
        "        return Transition(*zip(*self.memory))\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.memory)\n",
        "\n",
        "env = gym.make(ENV_NAME)  \n",
        "num_inputs = env.observation_space.shape[0]\n",
        "num_actions = env.action_space.shape[0]\n",
        "network = ActorCritic(num_inputs, num_actions, layer_norm=args.layer_norm)\n",
        "network.train()\n",
        "def ppo(args):\n",
        "    env = gym.make(args.env_name)\n",
        "    num_inputs = env.observation_space.shape[0]\n",
        "    num_actions = env.action_space.shape[0]\n",
        "\n",
        "    env.seed(args.seed)\n",
        "    torch.manual_seed(args.seed)\n",
        "\n",
        "    #network = ActorCritic(num_inputs, num_actions, layer_norm=args.layer_norm)\n",
        "    optimizer = opt.Adam(network.parameters(), lr=args.lr)\n",
        "\n",
        "    running_state = ZFilter((num_inputs,), clip=5.0)\n",
        "\n",
        "    # record average 1-round cumulative reward in every episode\n",
        "    reward_record = []\n",
        "    global_steps = 0\n",
        "\n",
        "    lr_now = args.lr\n",
        "    clip_now = args.clip\n",
        "\n",
        "    for i_episode in range(args.num_episode):\n",
        "        # step1: perform current policy to collect trajectories\n",
        "        # this is an on-policy method!\n",
        "        memory = Memory()\n",
        "        num_steps = 0\n",
        "        reward_list = []\n",
        "        len_list = []\n",
        "        while num_steps < args.batch_size:\n",
        "            state = env.reset()\n",
        "            if args.state_norm:\n",
        "                state = running_state(state)\n",
        "            reward_sum = 0\n",
        "            for t in range(args.max_step_per_round):\n",
        "                action_mean, action_logstd, value = network(Tensor(state).unsqueeze(0))\n",
        "                action, logproba = network.select_action(action_mean, action_logstd)\n",
        "                action = action.cpu().data.numpy()[0]\n",
        "                logproba = logproba.cpu().data.numpy()[0]\n",
        "                next_state, reward, done, _ = env.step(action)\n",
        "\n",
        "                reward_sum += reward\n",
        "                if args.state_norm:\n",
        "                    next_state = running_state(next_state)\n",
        "                mask = 0 if done else 1\n",
        "\n",
        "                memory.push(state, value, action, logproba, mask, next_state, reward)\n",
        "\n",
        "                if done:\n",
        "                    break\n",
        "\n",
        "                state = next_state\n",
        "\n",
        "            num_steps += (t + 1)\n",
        "            global_steps += (t + 1)\n",
        "            reward_list.append(reward_sum)\n",
        "            len_list.append(t + 1)\n",
        "        reward_record.append({\n",
        "            'episode': i_episode, \n",
        "            'steps': global_steps, \n",
        "            'meanepreward': np.mean(reward_list), \n",
        "            'meaneplen': np.mean(len_list)})\n",
        "        rwds.extend(reward_list)\n",
        "        batch = memory.sample()\n",
        "        batch_size = len(memory)\n",
        "\n",
        "        # step2: extract variables from trajectories\n",
        "        rewards = Tensor(batch.reward)\n",
        "        values = Tensor(batch.value)\n",
        "        masks = Tensor(batch.mask)\n",
        "        actions = Tensor(batch.action)\n",
        "        states = Tensor(batch.state)\n",
        "        oldlogproba = Tensor(batch.logproba)\n",
        "\n",
        "        returns = Tensor(batch_size)\n",
        "        deltas = Tensor(batch_size)\n",
        "        advantages = Tensor(batch_size)\n",
        "\n",
        "        prev_return = 0\n",
        "        prev_value = 0\n",
        "        prev_advantage = 0\n",
        "        for i in reversed(range(batch_size)):\n",
        "            returns[i] = rewards[i] + args.gamma * prev_return * masks[i]\n",
        "            deltas[i] = rewards[i] + args.gamma * prev_value * masks[i] - values[i]\n",
        "            # ref: https://arxiv.org/pdf/1506.02438.pdf (generalization advantage estimate)\n",
        "            advantages[i] = deltas[i] + args.gamma * args.lamda * prev_advantage * masks[i]\n",
        "\n",
        "            prev_return = returns[i]\n",
        "            prev_value = values[i]\n",
        "            prev_advantage = advantages[i]\n",
        "        if args.advantage_norm:\n",
        "            advantages = (advantages - advantages.mean()) / (advantages.std() + EPS)\n",
        "\n",
        "        for i_epoch in range(int(args.num_epoch * batch_size / args.minibatch_size)):\n",
        "            # sample from current batch\n",
        "            minibatch_ind = np.random.choice(batch_size, args.minibatch_size, replace=False)\n",
        "            minibatch_states = states[minibatch_ind]\n",
        "            minibatch_actions = actions[minibatch_ind]\n",
        "            minibatch_oldlogproba = oldlogproba[minibatch_ind]\n",
        "            minibatch_newlogproba = network.get_logproba(minibatch_states, minibatch_actions)\n",
        "            minibatch_advantages = advantages[minibatch_ind]\n",
        "            minibatch_returns = returns[minibatch_ind]\n",
        "            minibatch_newvalues = network._forward_critic(minibatch_states).flatten()\n",
        "\n",
        "\n",
        "\n",
        "            '''\n",
        "            Q4: \n",
        "\n",
        "            HERE: \n",
        "            now you have the advantages, and log-probabilities (both pi_new and pi_old)\n",
        "            you need to do optimization according to the CLIP loss\n",
        "            \n",
        "            '''\n",
        "            ratio =  torch.exp(minibatch_newlogproba - minibatch_oldlogproba)\n",
        "            surr1 = ratio * minibatch_advantages\n",
        "            surr2 = ratio.clamp(1 - clip_now, 1 + clip_now) * minibatch_advantages\n",
        "            loss_surr = - torch.mean(torch.min(surr1, surr2))\n",
        "\n",
        "            if args.lossvalue_norm:\n",
        "                minibatch_return_6std = 6 * minibatch_returns.std()\n",
        "                loss_value = torch.mean((minibatch_newvalues - minibatch_returns).pow(2)) / minibatch_return_6std\n",
        "            else:\n",
        "                loss_value = torch.mean((minibatch_newvalues - minibatch_returns).pow(2))\n",
        "\n",
        "            loss_entropy = torch.mean(torch.exp(minibatch_newlogproba) * minibatch_newlogproba)\n",
        "\n",
        "            total_loss = loss_surr + args.loss_coeff_value * loss_value + args.loss_coeff_entropy * loss_entropy\n",
        "            optimizer.zero_grad()\n",
        "            total_loss.backward()\n",
        "            optimizer.step()\n",
        "\n",
        "        if args.schedule_clip == 'linear':\n",
        "            ep_ratio = 1 - (i_episode / args.num_episode)\n",
        "            clip_now = args.clip * ep_ratio\n",
        "\n",
        "        if args.schedule_adam == 'linear':\n",
        "            ep_ratio = 1 - (i_episode / args.num_episode)\n",
        "            lr_now = args.lr * ep_ratio\n",
        "            for g in optimizer.param_groups:\n",
        "                g['lr'] = lr_now\n",
        "\n",
        "        if i_episode % args.log_num_episode == 0:\n",
        "            print('Finished episode: {} Reward: {:.4f} total_loss = {:.4f} = {:.4f} + {} * {:.4f} + {} * {:.4f}' \\\n",
        "                .format(i_episode, reward_record[-1]['meanepreward'], total_loss.data, loss_surr.data, args.loss_coeff_value, \n",
        "                loss_value.data, args.loss_coeff_entropy, loss_entropy.data))\n",
        "            print('-----------------')\n",
        "\n",
        "    return reward_record\n",
        "\n",
        "def test(args):\n",
        "    record_dfs = []\n",
        "    for i in range(args.num_parallel_run):\n",
        "        args.seed += 1\n",
        "        reward_record = pd.DataFrame(ppo(args))\n",
        "        reward_record['#parallel_run'] = i\n",
        "        record_dfs.append(reward_record)\n",
        "    record_dfs = pd.concat(record_dfs, axis=0)\n",
        "    record_dfs.to_csv(joindir(RESULT_DIR, 'ppo-record-{}.csv'.format(args.env_name)))\n",
        "\n",
        "if __name__ == '__main__':\n",
        "    for envname in [ENV_NAME]:\n",
        "        args.env_name = envname\n",
        "        test(args)\n",
        "\n",
        "torch.save(network.state_dict(),args.env_name.split('-')[0]+'/CheckPoints/checkpoint_new_{0}hidden_{1}drop_prob_{2}repeat'.format(args.hid_num,args.drop_prob, args.repeat)) \n",
        "np.savetxt(args.env_name.split('-')[0]+'/Rwds/rwds_new_{0}hidden_{1}drop_prob_{2}repeat'.format(args.hid_num,args.drop_prob, args.repeat),rwds)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Finished episode: 0 Reward: -1273.5315 total_loss = 94.1264 = -0.0118 + 0.5 * 188.2829 + 0.01 * -0.3250\n",
            "-----------------\n",
            "Finished episode: 1 Reward: -1238.6181 total_loss = 84.9119 = -0.0165 + 0.5 * 169.8634 + 0.01 * -0.3302\n",
            "-----------------\n",
            "Finished episode: 2 Reward: -1147.4002 total_loss = 72.3506 = -0.0324 + 0.5 * 144.7723 + 0.01 * -0.3178\n",
            "-----------------\n",
            "Finished episode: 3 Reward: -1085.0436 total_loss = 66.1620 = -0.0064 + 0.5 * 132.3435 + 0.01 * -0.3316\n",
            "-----------------\n",
            "Finished episode: 4 Reward: -1186.3921 total_loss = 76.7583 = 0.0474 + 0.5 * 153.4285 + 0.01 * -0.3279\n",
            "-----------------\n",
            "Finished episode: 5 Reward: -1120.6595 total_loss = 73.5934 = 0.0769 + 0.5 * 147.0394 + 0.01 * -0.3231\n",
            "-----------------\n",
            "Finished episode: 6 Reward: -1102.4201 total_loss = 64.8228 = 0.0246 + 0.5 * 129.6029 + 0.01 * -0.3302\n",
            "-----------------\n",
            "Finished episode: 7 Reward: -1111.8241 total_loss = 72.1350 = 0.0751 + 0.5 * 144.1264 + 0.01 * -0.3276\n",
            "-----------------\n",
            "Finished episode: 8 Reward: -1111.6846 total_loss = 64.1159 = -0.0128 + 0.5 * 128.2641 + 0.01 * -0.3328\n",
            "-----------------\n",
            "Finished episode: 9 Reward: -1163.0620 total_loss = 73.0329 = 0.0980 + 0.5 * 145.8766 + 0.01 * -0.3311\n",
            "-----------------\n",
            "Finished episode: 10 Reward: -1096.3202 total_loss = 67.0217 = 0.0624 + 0.5 * 133.9252 + 0.01 * -0.3286\n",
            "-----------------\n",
            "Finished episode: 11 Reward: -1053.3648 total_loss = 59.3408 = -0.0006 + 0.5 * 118.6894 + 0.01 * -0.3311\n",
            "-----------------\n",
            "Finished episode: 12 Reward: -1092.2973 total_loss = 64.6712 = 0.0799 + 0.5 * 129.1891 + 0.01 * -0.3278\n",
            "-----------------\n",
            "Finished episode: 13 Reward: -1122.9955 total_loss = 61.8881 = 0.0545 + 0.5 * 123.6739 + 0.01 * -0.3337\n",
            "-----------------\n",
            "Finished episode: 14 Reward: -1076.1153 total_loss = 58.2797 = 0.0129 + 0.5 * 116.5403 + 0.01 * -0.3315\n",
            "-----------------\n",
            "Finished episode: 15 Reward: -1177.5811 total_loss = 61.9707 = -0.0260 + 0.5 * 124.0000 + 0.01 * -0.3339\n",
            "-----------------\n",
            "Finished episode: 16 Reward: -1156.0840 total_loss = 60.9786 = 0.0497 + 0.5 * 121.8644 + 0.01 * -0.3369\n",
            "-----------------\n",
            "Finished episode: 17 Reward: -1235.8924 total_loss = 60.2629 = 0.0181 + 0.5 * 120.4962 + 0.01 * -0.3296\n",
            "-----------------\n",
            "Finished episode: 18 Reward: -1104.0867 total_loss = 56.7403 = 0.0197 + 0.5 * 113.4478 + 0.01 * -0.3314\n",
            "-----------------\n",
            "Finished episode: 19 Reward: -1166.0920 total_loss = 60.2853 = 0.0350 + 0.5 * 120.5072 + 0.01 * -0.3358\n",
            "-----------------\n",
            "Finished episode: 20 Reward: -1053.1970 total_loss = 46.8536 = -0.0463 + 0.5 * 93.8064 + 0.01 * -0.3354\n",
            "-----------------\n",
            "Finished episode: 21 Reward: -1085.4326 total_loss = 51.6108 = -0.0288 + 0.5 * 103.2859 + 0.01 * -0.3343\n",
            "-----------------\n",
            "Finished episode: 22 Reward: -1135.0928 total_loss = 55.2673 = 0.0177 + 0.5 * 110.5058 + 0.01 * -0.3293\n",
            "-----------------\n",
            "Finished episode: 23 Reward: -1192.1409 total_loss = 57.0618 = 0.0228 + 0.5 * 114.0846 + 0.01 * -0.3349\n",
            "-----------------\n",
            "Finished episode: 24 Reward: -1149.2373 total_loss = 53.3723 = 0.0117 + 0.5 * 106.7277 + 0.01 * -0.3286\n",
            "-----------------\n",
            "Finished episode: 25 Reward: -1071.1775 total_loss = 47.8220 = -0.0115 + 0.5 * 95.6735 + 0.01 * -0.3315\n",
            "-----------------\n",
            "Finished episode: 26 Reward: -1070.1307 total_loss = 45.5447 = 0.0100 + 0.5 * 91.0762 + 0.01 * -0.3382\n",
            "-----------------\n",
            "Finished episode: 27 Reward: -1097.7669 total_loss = 47.6020 = -0.0019 + 0.5 * 95.2145 + 0.01 * -0.3296\n",
            "-----------------\n",
            "Finished episode: 28 Reward: -1052.7973 total_loss = 41.0920 = -0.0170 + 0.5 * 82.2246 + 0.01 * -0.3385\n",
            "-----------------\n",
            "Finished episode: 29 Reward: -1075.8538 total_loss = 43.7078 = 0.0300 + 0.5 * 87.3624 + 0.01 * -0.3369\n",
            "-----------------\n",
            "Finished episode: 30 Reward: -1023.0887 total_loss = 39.3829 = 0.0085 + 0.5 * 78.7556 + 0.01 * -0.3391\n",
            "-----------------\n",
            "Finished episode: 31 Reward: -1078.8948 total_loss = 42.5141 = 0.0040 + 0.5 * 85.0269 + 0.01 * -0.3359\n",
            "-----------------\n",
            "Finished episode: 32 Reward: -1019.7267 total_loss = 35.7914 = -0.1168 + 0.5 * 71.8230 + 0.01 * -0.3327\n",
            "-----------------\n",
            "Finished episode: 33 Reward: -1044.7270 total_loss = 38.9919 = -0.0225 + 0.5 * 78.0354 + 0.01 * -0.3319\n",
            "-----------------\n",
            "Finished episode: 34 Reward: -1010.6746 total_loss = 36.5813 = -0.0238 + 0.5 * 73.2168 + 0.01 * -0.3289\n",
            "-----------------\n",
            "Finished episode: 35 Reward: -991.3499 total_loss = 35.7104 = 0.0007 + 0.5 * 71.4261 + 0.01 * -0.3353\n",
            "-----------------\n",
            "Finished episode: 36 Reward: -1030.9387 total_loss = 37.4040 = 0.0545 + 0.5 * 74.7056 + 0.01 * -0.3292\n",
            "-----------------\n",
            "Finished episode: 37 Reward: -1040.7149 total_loss = 33.0487 = -0.0427 + 0.5 * 66.1896 + 0.01 * -0.3350\n",
            "-----------------\n",
            "Finished episode: 38 Reward: -990.8077 total_loss = 34.5934 = 0.0017 + 0.5 * 69.1902 + 0.01 * -0.3367\n",
            "-----------------\n",
            "Finished episode: 39 Reward: -1005.8582 total_loss = 30.8611 = -0.0287 + 0.5 * 61.7863 + 0.01 * -0.3327\n",
            "-----------------\n",
            "Finished episode: 40 Reward: -1043.7481 total_loss = 34.4579 = -0.0004 + 0.5 * 68.9233 + 0.01 * -0.3294\n",
            "-----------------\n",
            "Finished episode: 41 Reward: -966.1482 total_loss = 30.0153 = -0.0397 + 0.5 * 60.1168 + 0.01 * -0.3338\n",
            "-----------------\n",
            "Finished episode: 42 Reward: -1022.1878 total_loss = 33.5729 = 0.0585 + 0.5 * 67.0354 + 0.01 * -0.3307\n",
            "-----------------\n",
            "Finished episode: 43 Reward: -971.0568 total_loss = 31.0848 = 0.0460 + 0.5 * 62.0843 + 0.01 * -0.3320\n",
            "-----------------\n",
            "Finished episode: 44 Reward: -969.3954 total_loss = 24.6385 = -0.0512 + 0.5 * 49.3859 + 0.01 * -0.3274\n",
            "-----------------\n",
            "Finished episode: 45 Reward: -1022.7834 total_loss = 30.7518 = 0.0115 + 0.5 * 61.4872 + 0.01 * -0.3300\n",
            "-----------------\n",
            "Finished episode: 46 Reward: -998.9507 total_loss = 32.3916 = 0.0860 + 0.5 * 64.6178 + 0.01 * -0.3339\n",
            "-----------------\n",
            "Finished episode: 47 Reward: -1033.4728 total_loss = 26.3495 = -0.1165 + 0.5 * 52.9386 + 0.01 * -0.3345\n",
            "-----------------\n",
            "Finished episode: 48 Reward: -1040.1023 total_loss = 26.8375 = -0.0885 + 0.5 * 53.8587 + 0.01 * -0.3358\n",
            "-----------------\n",
            "Finished episode: 49 Reward: -1042.2867 total_loss = 29.5525 = 0.0154 + 0.5 * 59.0809 + 0.01 * -0.3321\n",
            "-----------------\n",
            "Finished episode: 50 Reward: -1027.2879 total_loss = 28.4620 = -0.0011 + 0.5 * 56.9329 + 0.01 * -0.3316\n",
            "-----------------\n",
            "Finished episode: 51 Reward: -1019.8860 total_loss = 25.9789 = -0.0736 + 0.5 * 52.1118 + 0.01 * -0.3393\n",
            "-----------------\n",
            "Finished episode: 52 Reward: -981.5400 total_loss = 25.9109 = -0.0124 + 0.5 * 51.8532 + 0.01 * -0.3328\n",
            "-----------------\n",
            "Finished episode: 53 Reward: -1000.3765 total_loss = 23.2706 = -0.1010 + 0.5 * 46.7499 + 0.01 * -0.3397\n",
            "-----------------\n",
            "Finished episode: 54 Reward: -986.0054 total_loss = 21.1524 = -0.1565 + 0.5 * 42.6244 + 0.01 * -0.3306\n",
            "-----------------\n",
            "Finished episode: 55 Reward: -1007.8117 total_loss = 23.7173 = -0.0007 + 0.5 * 47.4427 + 0.01 * -0.3345\n",
            "-----------------\n",
            "Finished episode: 56 Reward: -969.7461 total_loss = 22.6625 = -0.0098 + 0.5 * 45.3513 + 0.01 * -0.3316\n",
            "-----------------\n",
            "Finished episode: 57 Reward: -1003.7544 total_loss = 21.5103 = -0.0410 + 0.5 * 43.1093 + 0.01 * -0.3317\n",
            "-----------------\n",
            "Finished episode: 58 Reward: -959.3579 total_loss = 20.6022 = -0.1301 + 0.5 * 41.4713 + 0.01 * -0.3330\n",
            "-----------------\n",
            "Finished episode: 59 Reward: -948.0853 total_loss = 20.1455 = -0.0180 + 0.5 * 40.3335 + 0.01 * -0.3262\n",
            "-----------------\n",
            "Finished episode: 60 Reward: -960.3289 total_loss = 22.6589 = 0.0370 + 0.5 * 45.2505 + 0.01 * -0.3290\n",
            "-----------------\n",
            "Finished episode: 61 Reward: -971.1828 total_loss = 21.2273 = 0.0097 + 0.5 * 42.4418 + 0.01 * -0.3322\n",
            "-----------------\n",
            "Finished episode: 62 Reward: -1015.9991 total_loss = 25.0784 = 0.1010 + 0.5 * 49.9616 + 0.01 * -0.3384\n",
            "-----------------\n",
            "Finished episode: 63 Reward: -993.4685 total_loss = 21.4018 = 0.0192 + 0.5 * 42.7716 + 0.01 * -0.3275\n",
            "-----------------\n",
            "Finished episode: 64 Reward: -1008.3784 total_loss = 20.9625 = -0.0525 + 0.5 * 42.0367 + 0.01 * -0.3353\n",
            "-----------------\n",
            "Finished episode: 65 Reward: -986.2921 total_loss = 19.7790 = -0.0201 + 0.5 * 39.6047 + 0.01 * -0.3276\n",
            "-----------------\n",
            "Finished episode: 66 Reward: -1033.3718 total_loss = 23.2898 = 0.0536 + 0.5 * 46.4788 + 0.01 * -0.3175\n",
            "-----------------\n",
            "Finished episode: 67 Reward: -987.9531 total_loss = 20.4483 = 0.0312 + 0.5 * 40.8408 + 0.01 * -0.3332\n",
            "-----------------\n",
            "Finished episode: 68 Reward: -1018.2399 total_loss = 21.2428 = -0.0411 + 0.5 * 42.5744 + 0.01 * -0.3310\n",
            "-----------------\n",
            "Finished episode: 69 Reward: -969.6541 total_loss = 19.8973 = 0.0104 + 0.5 * 39.7803 + 0.01 * -0.3268\n",
            "-----------------\n",
            "Finished episode: 70 Reward: -1002.2170 total_loss = 18.9395 = -0.0551 + 0.5 * 37.9958 + 0.01 * -0.3330\n",
            "-----------------\n",
            "Finished episode: 71 Reward: -1027.9036 total_loss = 19.3929 = -0.0223 + 0.5 * 38.8371 + 0.01 * -0.3304\n",
            "-----------------\n",
            "Finished episode: 72 Reward: -1013.4656 total_loss = 20.9272 = 0.0312 + 0.5 * 41.7984 + 0.01 * -0.3277\n",
            "-----------------\n",
            "Finished episode: 73 Reward: -968.0166 total_loss = 18.6660 = 0.0199 + 0.5 * 37.2989 + 0.01 * -0.3335\n",
            "-----------------\n",
            "Finished episode: 74 Reward: -989.8899 total_loss = 19.7222 = 0.0714 + 0.5 * 39.3081 + 0.01 * -0.3261\n",
            "-----------------\n",
            "Finished episode: 75 Reward: -985.3284 total_loss = 19.4874 = 0.0411 + 0.5 * 38.8994 + 0.01 * -0.3320\n",
            "-----------------\n",
            "Finished episode: 76 Reward: -1010.0855 total_loss = 19.2207 = 0.0026 + 0.5 * 38.4427 + 0.01 * -0.3236\n",
            "-----------------\n",
            "Finished episode: 77 Reward: -1052.8097 total_loss = 20.5474 = 0.0283 + 0.5 * 41.0448 + 0.01 * -0.3254\n",
            "-----------------\n",
            "Finished episode: 78 Reward: -1090.7609 total_loss = 21.1909 = 0.0108 + 0.5 * 42.3667 + 0.01 * -0.3297\n",
            "-----------------\n",
            "Finished episode: 79 Reward: -1027.3522 total_loss = 17.9931 = -0.0526 + 0.5 * 36.0978 + 0.01 * -0.3297\n",
            "-----------------\n",
            "Finished episode: 80 Reward: -1020.9220 total_loss = 20.2935 = 0.0854 + 0.5 * 40.4228 + 0.01 * -0.3318\n",
            "-----------------\n",
            "Finished episode: 81 Reward: -1012.6596 total_loss = 17.7125 = -0.0324 + 0.5 * 35.4964 + 0.01 * -0.3264\n",
            "-----------------\n",
            "Finished episode: 82 Reward: -1014.9351 total_loss = 16.9213 = -0.0524 + 0.5 * 33.9539 + 0.01 * -0.3256\n",
            "-----------------\n",
            "Finished episode: 83 Reward: -996.5938 total_loss = 15.9412 = -0.0434 + 0.5 * 31.9756 + 0.01 * -0.3278\n",
            "-----------------\n",
            "Finished episode: 84 Reward: -995.8215 total_loss = 17.6527 = 0.0361 + 0.5 * 35.2398 + 0.01 * -0.3314\n",
            "-----------------\n",
            "Finished episode: 85 Reward: -1004.5041 total_loss = 17.6900 = 0.0677 + 0.5 * 35.2513 + 0.01 * -0.3321\n",
            "-----------------\n",
            "Finished episode: 86 Reward: -1016.5967 total_loss = 16.6783 = 0.0906 + 0.5 * 33.1817 + 0.01 * -0.3203\n",
            "-----------------\n",
            "Finished episode: 87 Reward: -1028.1173 total_loss = 16.8128 = -0.0628 + 0.5 * 33.7577 + 0.01 * -0.3223\n",
            "-----------------\n",
            "Finished episode: 88 Reward: -1032.0671 total_loss = 16.9390 = 0.0618 + 0.5 * 33.7608 + 0.01 * -0.3203\n",
            "-----------------\n",
            "Finished episode: 89 Reward: -980.9992 total_loss = 16.1271 = 0.0356 + 0.5 * 32.1891 + 0.01 * -0.3160\n",
            "-----------------\n",
            "Finished episode: 90 Reward: -992.6280 total_loss = 15.2520 = -0.0069 + 0.5 * 30.5242 + 0.01 * -0.3204\n",
            "-----------------\n",
            "Finished episode: 91 Reward: -1000.8053 total_loss = 14.9802 = 0.0541 + 0.5 * 29.8588 + 0.01 * -0.3251\n",
            "-----------------\n",
            "Finished episode: 92 Reward: -992.7888 total_loss = 16.2355 = 0.0050 + 0.5 * 32.4676 + 0.01 * -0.3291\n",
            "-----------------\n",
            "Finished episode: 93 Reward: -1005.7740 total_loss = 16.6890 = -0.0233 + 0.5 * 33.4311 + 0.01 * -0.3214\n",
            "-----------------\n",
            "Finished episode: 94 Reward: -974.0621 total_loss = 14.2081 = 0.1058 + 0.5 * 28.2111 + 0.01 * -0.3252\n",
            "-----------------\n",
            "Finished episode: 95 Reward: -985.6362 total_loss = 15.7798 = -0.0130 + 0.5 * 31.5922 + 0.01 * -0.3244\n",
            "-----------------\n",
            "Finished episode: 96 Reward: -1013.8838 total_loss = 16.1554 = -0.0154 + 0.5 * 32.3478 + 0.01 * -0.3167\n",
            "-----------------\n",
            "Finished episode: 97 Reward: -986.8207 total_loss = 14.4495 = -0.0825 + 0.5 * 29.0703 + 0.01 * -0.3167\n",
            "-----------------\n",
            "Finished episode: 98 Reward: -1058.4498 total_loss = 16.1598 = -0.1006 + 0.5 * 32.5272 + 0.01 * -0.3210\n",
            "-----------------\n",
            "Finished episode: 99 Reward: -989.2035 total_loss = 14.5231 = -0.0628 + 0.5 * 29.1782 + 0.01 * -0.3210\n",
            "-----------------\n",
            "Finished episode: 100 Reward: -1004.2446 total_loss = 15.2609 = 0.0536 + 0.5 * 30.4211 + 0.01 * -0.3240\n",
            "-----------------\n",
            "Finished episode: 101 Reward: -1016.6374 total_loss = 13.7417 = 0.0656 + 0.5 * 27.3584 + 0.01 * -0.3126\n",
            "-----------------\n",
            "Finished episode: 102 Reward: -1017.4513 total_loss = 15.6723 = -0.0155 + 0.5 * 31.3821 + 0.01 * -0.3216\n",
            "-----------------\n",
            "Finished episode: 103 Reward: -1036.5564 total_loss = 15.6857 = 0.0438 + 0.5 * 31.2903 + 0.01 * -0.3180\n",
            "-----------------\n",
            "Finished episode: 104 Reward: -1030.6226 total_loss = 14.8715 = 0.0984 + 0.5 * 29.5525 + 0.01 * -0.3175\n",
            "-----------------\n",
            "Finished episode: 105 Reward: -1022.1479 total_loss = 14.6412 = 0.0454 + 0.5 * 29.1978 + 0.01 * -0.3145\n",
            "-----------------\n",
            "Finished episode: 106 Reward: -1023.9346 total_loss = 15.4753 = 0.0116 + 0.5 * 30.9336 + 0.01 * -0.3102\n",
            "-----------------\n",
            "Finished episode: 107 Reward: -964.7729 total_loss = 14.4559 = -0.0222 + 0.5 * 28.9624 + 0.01 * -0.3176\n",
            "-----------------\n",
            "Finished episode: 108 Reward: -1014.3473 total_loss = 15.5185 = 0.1416 + 0.5 * 30.7601 + 0.01 * -0.3151\n",
            "-----------------\n",
            "Finished episode: 109 Reward: -959.0162 total_loss = 12.7978 = 0.0917 + 0.5 * 25.4185 + 0.01 * -0.3099\n",
            "-----------------\n",
            "Finished episode: 110 Reward: -1032.3827 total_loss = 13.4144 = 0.0199 + 0.5 * 26.7953 + 0.01 * -0.3160\n",
            "-----------------\n",
            "Finished episode: 111 Reward: -991.8051 total_loss = 13.8176 = -0.0172 + 0.5 * 27.6759 + 0.01 * -0.3090\n",
            "-----------------\n",
            "Finished episode: 112 Reward: -994.8301 total_loss = 14.3685 = -0.0818 + 0.5 * 28.9068 + 0.01 * -0.3137\n",
            "-----------------\n",
            "Finished episode: 113 Reward: -974.7830 total_loss = 13.7316 = 0.0677 + 0.5 * 27.3340 + 0.01 * -0.3067\n",
            "-----------------\n",
            "Finished episode: 114 Reward: -973.3147 total_loss = 12.0167 = 0.0379 + 0.5 * 23.9641 + 0.01 * -0.3220\n",
            "-----------------\n",
            "Finished episode: 115 Reward: -981.2694 total_loss = 13.7926 = 0.0006 + 0.5 * 27.5905 + 0.01 * -0.3215\n",
            "-----------------\n",
            "Finished episode: 116 Reward: -987.8008 total_loss = 13.5126 = -0.0052 + 0.5 * 27.0420 + 0.01 * -0.3164\n",
            "-----------------\n",
            "Finished episode: 117 Reward: -986.7490 total_loss = 12.4472 = 0.1017 + 0.5 * 24.6974 + 0.01 * -0.3182\n",
            "-----------------\n",
            "Finished episode: 118 Reward: -952.9888 total_loss = 13.7288 = 0.1392 + 0.5 * 27.1854 + 0.01 * -0.3103\n",
            "-----------------\n",
            "Finished episode: 119 Reward: -1003.2193 total_loss = 11.5621 = 0.0419 + 0.5 * 23.0468 + 0.01 * -0.3210\n",
            "-----------------\n",
            "Finished episode: 120 Reward: -897.0392 total_loss = 10.8695 = -0.1227 + 0.5 * 21.9905 + 0.01 * -0.3115\n",
            "-----------------\n",
            "Finished episode: 121 Reward: -942.5125 total_loss = 10.3389 = -0.0269 + 0.5 * 20.7379 + 0.01 * -0.3143\n",
            "-----------------\n",
            "Finished episode: 122 Reward: -975.7481 total_loss = 11.4542 = -0.1197 + 0.5 * 23.1540 + 0.01 * -0.3122\n",
            "-----------------\n",
            "Finished episode: 123 Reward: -966.4943 total_loss = 12.3831 = 0.0038 + 0.5 * 24.7648 + 0.01 * -0.3181\n",
            "-----------------\n",
            "Finished episode: 124 Reward: -1034.6737 total_loss = 12.3594 = -0.0133 + 0.5 * 24.7518 + 0.01 * -0.3143\n",
            "-----------------\n",
            "Finished episode: 125 Reward: -1030.7577 total_loss = 13.0085 = -0.0238 + 0.5 * 26.0708 + 0.01 * -0.3102\n",
            "-----------------\n",
            "Finished episode: 126 Reward: -996.7331 total_loss = 12.3744 = 0.0241 + 0.5 * 24.7068 + 0.01 * -0.3065\n",
            "-----------------\n",
            "Finished episode: 127 Reward: -961.0899 total_loss = 11.3195 = -0.0591 + 0.5 * 22.7634 + 0.01 * -0.3109\n",
            "-----------------\n",
            "Finished episode: 128 Reward: -959.4143 total_loss = 10.9211 = -0.0339 + 0.5 * 21.9162 + 0.01 * -0.3108\n",
            "-----------------\n",
            "Finished episode: 129 Reward: -1002.6434 total_loss = 11.6632 = 0.0000 + 0.5 * 23.3326 + 0.01 * -0.3138\n",
            "-----------------\n",
            "Finished episode: 130 Reward: -1015.0035 total_loss = 12.7044 = 0.0142 + 0.5 * 25.3865 + 0.01 * -0.3066\n",
            "-----------------\n",
            "Finished episode: 131 Reward: -991.5521 total_loss = 12.8005 = 0.0115 + 0.5 * 25.5843 + 0.01 * -0.3071\n",
            "-----------------\n",
            "Finished episode: 132 Reward: -1004.3029 total_loss = 10.4712 = -0.0425 + 0.5 * 21.0335 + 0.01 * -0.3089\n",
            "-----------------\n",
            "Finished episode: 133 Reward: -1022.0606 total_loss = 11.3304 = -0.0457 + 0.5 * 22.7584 + 0.01 * -0.3077\n",
            "-----------------\n",
            "Finished episode: 134 Reward: -993.7962 total_loss = 12.1604 = 0.0177 + 0.5 * 24.2916 + 0.01 * -0.3057\n",
            "-----------------\n",
            "Finished episode: 135 Reward: -1039.4698 total_loss = 12.6294 = -0.0474 + 0.5 * 25.3598 + 0.01 * -0.3079\n",
            "-----------------\n",
            "Finished episode: 136 Reward: -1070.0886 total_loss = 11.9527 = 0.0392 + 0.5 * 23.8329 + 0.01 * -0.3056\n",
            "-----------------\n",
            "Finished episode: 137 Reward: -1072.1611 total_loss = 12.5733 = -0.0136 + 0.5 * 25.1800 + 0.01 * -0.3086\n",
            "-----------------\n",
            "Finished episode: 138 Reward: -1081.7900 total_loss = 14.1230 = 0.0730 + 0.5 * 28.1062 + 0.01 * -0.3090\n",
            "-----------------\n",
            "Finished episode: 139 Reward: -1061.9447 total_loss = 11.9106 = 0.0944 + 0.5 * 23.6386 + 0.01 * -0.3081\n",
            "-----------------\n",
            "Finished episode: 140 Reward: -1072.1164 total_loss = 11.6042 = 0.0071 + 0.5 * 23.2000 + 0.01 * -0.3016\n",
            "-----------------\n",
            "Finished episode: 141 Reward: -1107.4157 total_loss = 11.0571 = -0.0453 + 0.5 * 22.2108 + 0.01 * -0.3054\n",
            "-----------------\n",
            "Finished episode: 142 Reward: -1095.9404 total_loss = 10.4401 = -0.0006 + 0.5 * 20.8873 + 0.01 * -0.2951\n",
            "-----------------\n",
            "Finished episode: 143 Reward: -1125.9079 total_loss = 13.1326 = -0.0268 + 0.5 * 26.3246 + 0.01 * -0.2989\n",
            "-----------------\n",
            "Finished episode: 144 Reward: -1103.3725 total_loss = 10.9711 = 0.1264 + 0.5 * 21.6954 + 0.01 * -0.3065\n",
            "-----------------\n",
            "Finished episode: 145 Reward: -1083.9883 total_loss = 12.6225 = -0.0851 + 0.5 * 25.4209 + 0.01 * -0.2918\n",
            "-----------------\n",
            "Finished episode: 146 Reward: -1122.0164 total_loss = 12.5979 = 0.0648 + 0.5 * 25.0721 + 0.01 * -0.2966\n",
            "-----------------\n",
            "Finished episode: 147 Reward: -1135.0699 total_loss = 13.1028 = 0.0136 + 0.5 * 26.1844 + 0.01 * -0.2946\n",
            "-----------------\n",
            "Finished episode: 148 Reward: -1156.6074 total_loss = 13.3350 = 0.0827 + 0.5 * 26.5106 + 0.01 * -0.2923\n",
            "-----------------\n",
            "Finished episode: 149 Reward: -1148.5513 total_loss = 13.7222 = 0.0389 + 0.5 * 27.3726 + 0.01 * -0.3011\n",
            "-----------------\n",
            "Finished episode: 150 Reward: -1146.1294 total_loss = 14.2076 = 0.0098 + 0.5 * 28.4014 + 0.01 * -0.2894\n",
            "-----------------\n",
            "Finished episode: 151 Reward: -1124.0147 total_loss = 11.3821 = 0.0563 + 0.5 * 22.6575 + 0.01 * -0.2889\n",
            "-----------------\n",
            "Finished episode: 152 Reward: -1172.1142 total_loss = 13.5439 = -0.0914 + 0.5 * 27.2764 + 0.01 * -0.2901\n",
            "-----------------\n",
            "Finished episode: 153 Reward: -1167.8306 total_loss = 13.2209 = 0.0410 + 0.5 * 26.3659 + 0.01 * -0.3000\n",
            "-----------------\n",
            "Finished episode: 154 Reward: -1151.7728 total_loss = 12.7859 = -0.0223 + 0.5 * 25.6224 + 0.01 * -0.2936\n",
            "-----------------\n",
            "Finished episode: 155 Reward: -1162.0394 total_loss = 11.7819 = -0.0699 + 0.5 * 23.7096 + 0.01 * -0.2964\n",
            "-----------------\n",
            "Finished episode: 156 Reward: -1175.0427 total_loss = 12.9610 = 0.0889 + 0.5 * 25.7503 + 0.01 * -0.2989\n",
            "-----------------\n",
            "Finished episode: 157 Reward: -1154.6393 total_loss = 12.7259 = -0.0148 + 0.5 * 25.4873 + 0.01 * -0.2925\n",
            "-----------------\n",
            "Finished episode: 158 Reward: -1128.6699 total_loss = 9.8445 = -0.0004 + 0.5 * 19.6957 + 0.01 * -0.2947\n",
            "-----------------\n",
            "Finished episode: 159 Reward: -1177.3454 total_loss = 12.4420 = -0.0309 + 0.5 * 24.9515 + 0.01 * -0.2883\n",
            "-----------------\n",
            "Finished episode: 160 Reward: -1166.0523 total_loss = 12.7344 = 0.0791 + 0.5 * 25.3163 + 0.01 * -0.2932\n",
            "-----------------\n",
            "Finished episode: 161 Reward: -1167.5814 total_loss = 11.8113 = 0.1077 + 0.5 * 23.4130 + 0.01 * -0.2875\n",
            "-----------------\n",
            "Finished episode: 162 Reward: -1170.9242 total_loss = 10.7024 = -0.0006 + 0.5 * 21.4120 + 0.01 * -0.2917\n",
            "-----------------\n",
            "Finished episode: 163 Reward: -1168.9733 total_loss = 13.2995 = 0.0394 + 0.5 * 26.5258 + 0.01 * -0.2837\n",
            "-----------------\n",
            "Finished episode: 164 Reward: -1168.0938 total_loss = 11.3531 = 0.0303 + 0.5 * 22.6513 + 0.01 * -0.2873\n",
            "-----------------\n",
            "Finished episode: 165 Reward: -1187.2702 total_loss = 13.5662 = -0.0277 + 0.5 * 27.1938 + 0.01 * -0.2919\n",
            "-----------------\n",
            "Finished episode: 166 Reward: -1179.4955 total_loss = 13.4044 = -0.1077 + 0.5 * 27.0298 + 0.01 * -0.2863\n",
            "-----------------\n",
            "Finished episode: 167 Reward: -1191.1687 total_loss = 10.3376 = 0.0314 + 0.5 * 20.6181 + 0.01 * -0.2911\n",
            "-----------------\n",
            "Finished episode: 168 Reward: -1176.5157 total_loss = 10.9662 = -0.0785 + 0.5 * 22.0951 + 0.01 * -0.2856\n",
            "-----------------\n",
            "Finished episode: 169 Reward: -1182.1729 total_loss = 12.5981 = -0.0073 + 0.5 * 25.2165 + 0.01 * -0.2842\n",
            "-----------------\n",
            "Finished episode: 170 Reward: -1185.6721 total_loss = 13.3500 = -0.0408 + 0.5 * 26.7872 + 0.01 * -0.2841\n",
            "-----------------\n",
            "Finished episode: 171 Reward: -1189.2942 total_loss = 13.3262 = 0.0733 + 0.5 * 26.5113 + 0.01 * -0.2731\n",
            "-----------------\n",
            "Finished episode: 172 Reward: -1191.1370 total_loss = 11.4541 = -0.0217 + 0.5 * 22.9570 + 0.01 * -0.2769\n",
            "-----------------\n",
            "Finished episode: 173 Reward: -1188.3140 total_loss = 11.3431 = -0.0915 + 0.5 * 22.8748 + 0.01 * -0.2812\n",
            "-----------------\n",
            "Finished episode: 174 Reward: -1188.6788 total_loss = 13.1415 = -0.0191 + 0.5 * 26.3268 + 0.01 * -0.2772\n",
            "-----------------\n",
            "Finished episode: 175 Reward: -1178.6609 total_loss = 12.0136 = 0.0483 + 0.5 * 23.9363 + 0.01 * -0.2925\n",
            "-----------------\n",
            "Finished episode: 176 Reward: -1192.1121 total_loss = 11.8821 = -0.0025 + 0.5 * 23.7748 + 0.01 * -0.2824\n",
            "-----------------\n",
            "Finished episode: 177 Reward: -1208.0803 total_loss = 13.6123 = 0.0153 + 0.5 * 27.1995 + 0.01 * -0.2837\n",
            "-----------------\n",
            "Finished episode: 178 Reward: -1180.3131 total_loss = 10.8959 = -0.1121 + 0.5 * 22.0217 + 0.01 * -0.2865\n",
            "-----------------\n",
            "Finished episode: 179 Reward: -1195.9254 total_loss = 13.7621 = 0.0271 + 0.5 * 27.4758 + 0.01 * -0.2849\n",
            "-----------------\n",
            "Finished episode: 180 Reward: -1190.3056 total_loss = 13.4366 = 0.0178 + 0.5 * 26.8431 + 0.01 * -0.2803\n",
            "-----------------\n",
            "Finished episode: 181 Reward: -1192.5525 total_loss = 12.8379 = -0.0850 + 0.5 * 25.8515 + 0.01 * -0.2847\n",
            "-----------------\n",
            "Finished episode: 182 Reward: -1180.2648 total_loss = 11.4855 = 0.0195 + 0.5 * 22.9376 + 0.01 * -0.2823\n",
            "-----------------\n",
            "Finished episode: 183 Reward: -1200.6719 total_loss = 10.3349 = -0.0407 + 0.5 * 20.7567 + 0.01 * -0.2745\n",
            "-----------------\n",
            "Finished episode: 184 Reward: -1208.3932 total_loss = 12.5065 = 0.0490 + 0.5 * 24.9207 + 0.01 * -0.2833\n",
            "-----------------\n",
            "Finished episode: 185 Reward: -1198.2144 total_loss = 13.6635 = -0.0566 + 0.5 * 27.4459 + 0.01 * -0.2805\n",
            "-----------------\n",
            "Finished episode: 186 Reward: -1205.4503 total_loss = 11.6497 = -0.0649 + 0.5 * 23.4348 + 0.01 * -0.2764\n",
            "-----------------\n",
            "Finished episode: 187 Reward: -1230.5419 total_loss = 11.7501 = 0.0039 + 0.5 * 23.4980 + 0.01 * -0.2713\n",
            "-----------------\n",
            "Finished episode: 188 Reward: -1216.6845 total_loss = 12.2045 = -0.0357 + 0.5 * 24.4861 + 0.01 * -0.2806\n",
            "-----------------\n",
            "Finished episode: 189 Reward: -1254.0936 total_loss = 14.3843 = 0.0724 + 0.5 * 28.6292 + 0.01 * -0.2648\n",
            "-----------------\n",
            "Finished episode: 190 Reward: -1244.7080 total_loss = 12.8656 = 0.0616 + 0.5 * 25.6134 + 0.01 * -0.2699\n",
            "-----------------\n",
            "Finished episode: 191 Reward: -1247.8365 total_loss = 13.3978 = 0.1149 + 0.5 * 26.5711 + 0.01 * -0.2709\n",
            "-----------------\n",
            "Finished episode: 192 Reward: -1233.8059 total_loss = 13.3742 = 0.0573 + 0.5 * 26.6391 + 0.01 * -0.2678\n",
            "-----------------\n",
            "Finished episode: 193 Reward: -1240.6156 total_loss = 11.0040 = 0.0070 + 0.5 * 21.9995 + 0.01 * -0.2757\n",
            "-----------------\n",
            "Finished episode: 194 Reward: -1263.1981 total_loss = 13.8406 = 0.0118 + 0.5 * 27.6632 + 0.01 * -0.2793\n",
            "-----------------\n",
            "Finished episode: 195 Reward: -1248.0780 total_loss = 13.2529 = 0.0418 + 0.5 * 26.4276 + 0.01 * -0.2697\n",
            "-----------------\n",
            "Finished episode: 196 Reward: -1267.4848 total_loss = 15.5242 = -0.0627 + 0.5 * 31.1791 + 0.01 * -0.2732\n",
            "-----------------\n",
            "Finished episode: 197 Reward: -1276.5723 total_loss = 14.2305 = 0.0323 + 0.5 * 28.4020 + 0.01 * -0.2793\n",
            "-----------------\n",
            "Finished episode: 198 Reward: -1269.2785 total_loss = 13.4741 = -0.0218 + 0.5 * 26.9973 + 0.01 * -0.2700\n",
            "-----------------\n",
            "Finished episode: 199 Reward: -1257.3950 total_loss = 14.4894 = -0.1935 + 0.5 * 29.3712 + 0.01 * -0.2708\n",
            "-----------------\n",
            "Finished episode: 200 Reward: -1274.0104 total_loss = 13.9309 = 0.1031 + 0.5 * 27.6612 + 0.01 * -0.2739\n",
            "-----------------\n",
            "Finished episode: 201 Reward: -1261.4738 total_loss = 11.3224 = 0.0758 + 0.5 * 22.4984 + 0.01 * -0.2608\n",
            "-----------------\n",
            "Finished episode: 202 Reward: -1277.6906 total_loss = 14.8055 = 0.0785 + 0.5 * 29.4593 + 0.01 * -0.2714\n",
            "-----------------\n",
            "Finished episode: 203 Reward: -1257.7031 total_loss = 12.4485 = -0.0608 + 0.5 * 25.0239 + 0.01 * -0.2646\n",
            "-----------------\n",
            "Finished episode: 204 Reward: -1288.2549 total_loss = 12.5214 = 0.0875 + 0.5 * 24.8731 + 0.01 * -0.2640\n",
            "-----------------\n",
            "Finished episode: 205 Reward: -1263.4189 total_loss = 11.2828 = 0.0003 + 0.5 * 22.5703 + 0.01 * -0.2683\n",
            "-----------------\n",
            "Finished episode: 206 Reward: -1275.6132 total_loss = 13.0149 = 0.0175 + 0.5 * 26.0003 + 0.01 * -0.2684\n",
            "-----------------\n",
            "Finished episode: 207 Reward: -1279.2341 total_loss = 14.1193 = -0.0390 + 0.5 * 28.3220 + 0.01 * -0.2672\n",
            "-----------------\n",
            "Finished episode: 208 Reward: -1276.0347 total_loss = 14.4133 = 0.0654 + 0.5 * 28.7010 + 0.01 * -0.2599\n",
            "-----------------\n",
            "Finished episode: 209 Reward: -1280.4131 total_loss = 12.7600 = -0.0198 + 0.5 * 25.5648 + 0.01 * -0.2622\n",
            "-----------------\n",
            "Finished episode: 210 Reward: -1281.4894 total_loss = 12.9915 = 0.0138 + 0.5 * 25.9607 + 0.01 * -0.2615\n",
            "-----------------\n",
            "Finished episode: 211 Reward: -1281.1827 total_loss = 14.2842 = 0.0543 + 0.5 * 28.4651 + 0.01 * -0.2731\n",
            "-----------------\n",
            "Finished episode: 212 Reward: -1280.0604 total_loss = 11.1941 = -0.0098 + 0.5 * 22.4132 + 0.01 * -0.2651\n",
            "-----------------\n",
            "Finished episode: 213 Reward: -1290.4488 total_loss = 11.8378 = -0.0393 + 0.5 * 23.7596 + 0.01 * -0.2705\n",
            "-----------------\n",
            "Finished episode: 214 Reward: -1292.0499 total_loss = 12.8021 = -0.0636 + 0.5 * 25.7366 + 0.01 * -0.2558\n",
            "-----------------\n",
            "Finished episode: 215 Reward: -1282.4970 total_loss = 11.7049 = -0.0900 + 0.5 * 23.5950 + 0.01 * -0.2527\n",
            "-----------------\n",
            "Finished episode: 216 Reward: -1285.1436 total_loss = 12.0365 = 0.0389 + 0.5 * 24.0003 + 0.01 * -0.2548\n",
            "-----------------\n",
            "Finished episode: 217 Reward: -1297.6235 total_loss = 13.2151 = -0.0795 + 0.5 * 26.5942 + 0.01 * -0.2535\n",
            "-----------------\n",
            "Finished episode: 218 Reward: -1300.6142 total_loss = 14.4651 = -0.0171 + 0.5 * 28.9694 + 0.01 * -0.2491\n",
            "-----------------\n",
            "Finished episode: 219 Reward: -1296.7314 total_loss = 16.1149 = -0.0940 + 0.5 * 32.4230 + 0.01 * -0.2536\n",
            "-----------------\n",
            "Finished episode: 220 Reward: -1304.0036 total_loss = 13.4135 = 0.0123 + 0.5 * 26.8076 + 0.01 * -0.2570\n",
            "-----------------\n",
            "Finished episode: 221 Reward: -1296.0811 total_loss = 14.3270 = -0.0339 + 0.5 * 28.7268 + 0.01 * -0.2536\n",
            "-----------------\n",
            "Finished episode: 222 Reward: -1303.0929 total_loss = 14.2535 = -0.0012 + 0.5 * 28.5142 + 0.01 * -0.2334\n",
            "-----------------\n",
            "Finished episode: 223 Reward: -1297.2285 total_loss = 12.4609 = -0.0101 + 0.5 * 24.9468 + 0.01 * -0.2456\n",
            "-----------------\n",
            "Finished episode: 224 Reward: -1285.6862 total_loss = 12.9191 = -0.0376 + 0.5 * 25.9184 + 0.01 * -0.2537\n",
            "-----------------\n",
            "Finished episode: 225 Reward: -1301.3938 total_loss = 12.4966 = -0.0298 + 0.5 * 25.0579 + 0.01 * -0.2552\n",
            "-----------------\n",
            "Finished episode: 226 Reward: -1297.5769 total_loss = 12.1162 = -0.0054 + 0.5 * 24.2485 + 0.01 * -0.2605\n",
            "-----------------\n",
            "Finished episode: 227 Reward: -1290.3490 total_loss = 13.9440 = 0.0174 + 0.5 * 27.8583 + 0.01 * -0.2602\n",
            "-----------------\n",
            "Finished episode: 228 Reward: -1308.9079 total_loss = 15.0401 = 0.0398 + 0.5 * 30.0054 + 0.01 * -0.2449\n",
            "-----------------\n",
            "Finished episode: 229 Reward: -1317.8905 total_loss = 13.0488 = 0.0774 + 0.5 * 25.9477 + 0.01 * -0.2502\n",
            "-----------------\n",
            "Finished episode: 230 Reward: -1313.7259 total_loss = 14.2104 = 0.0174 + 0.5 * 28.3910 + 0.01 * -0.2500\n",
            "-----------------\n",
            "Finished episode: 231 Reward: -1323.9784 total_loss = 14.2842 = -0.0172 + 0.5 * 28.6078 + 0.01 * -0.2599\n",
            "-----------------\n",
            "Finished episode: 232 Reward: -1316.8207 total_loss = 15.4749 = 0.0699 + 0.5 * 30.8152 + 0.01 * -0.2560\n",
            "-----------------\n",
            "Finished episode: 233 Reward: -1319.8597 total_loss = 12.9665 = -0.0550 + 0.5 * 26.0483 + 0.01 * -0.2577\n",
            "-----------------\n",
            "Finished episode: 234 Reward: -1315.2270 total_loss = 13.8648 = -0.1016 + 0.5 * 27.9378 + 0.01 * -0.2474\n",
            "-----------------\n",
            "Finished episode: 235 Reward: -1305.6444 total_loss = 12.8793 = 0.1022 + 0.5 * 25.5594 + 0.01 * -0.2620\n",
            "-----------------\n",
            "Finished episode: 236 Reward: -1316.9743 total_loss = 13.1674 = -0.0202 + 0.5 * 26.3804 + 0.01 * -0.2595\n",
            "-----------------\n",
            "Finished episode: 237 Reward: -1315.6965 total_loss = 12.5168 = -0.0026 + 0.5 * 25.0441 + 0.01 * -0.2608\n",
            "-----------------\n",
            "Finished episode: 238 Reward: -1323.8458 total_loss = 13.7698 = 0.0396 + 0.5 * 27.4657 + 0.01 * -0.2701\n",
            "-----------------\n",
            "Finished episode: 239 Reward: -1310.3823 total_loss = 12.3802 = 0.0550 + 0.5 * 24.6558 + 0.01 * -0.2627\n",
            "-----------------\n",
            "Finished episode: 240 Reward: -1309.6032 total_loss = 12.6385 = 0.0691 + 0.5 * 25.1440 + 0.01 * -0.2627\n",
            "-----------------\n",
            "Finished episode: 241 Reward: -1315.6279 total_loss = 16.2918 = 0.0493 + 0.5 * 32.4902 + 0.01 * -0.2628\n",
            "-----------------\n",
            "Finished episode: 242 Reward: -1316.7491 total_loss = 14.5004 = 0.0594 + 0.5 * 28.8874 + 0.01 * -0.2655\n",
            "-----------------\n",
            "Finished episode: 243 Reward: -1318.1145 total_loss = 12.6971 = -0.0823 + 0.5 * 25.5640 + 0.01 * -0.2544\n",
            "-----------------\n",
            "Finished episode: 244 Reward: -1315.1254 total_loss = 14.3408 = -0.0251 + 0.5 * 28.7370 + 0.01 * -0.2564\n",
            "-----------------\n",
            "Finished episode: 245 Reward: -1314.3397 total_loss = 13.3876 = 0.0109 + 0.5 * 26.7584 + 0.01 * -0.2500\n",
            "-----------------\n",
            "Finished episode: 246 Reward: -1308.9789 total_loss = 14.7540 = -0.0625 + 0.5 * 29.6381 + 0.01 * -0.2568\n",
            "-----------------\n",
            "Finished episode: 247 Reward: -1316.9772 total_loss = 15.0711 = 0.0429 + 0.5 * 30.0615 + 0.01 * -0.2522\n",
            "-----------------\n",
            "Finished episode: 248 Reward: -1314.5161 total_loss = 12.4129 = -0.1083 + 0.5 * 25.0473 + 0.01 * -0.2477\n",
            "-----------------\n",
            "Finished episode: 249 Reward: -1317.0539 total_loss = 13.3522 = 0.0493 + 0.5 * 26.6106 + 0.01 * -0.2403\n",
            "-----------------\n",
            "Finished episode: 250 Reward: -1314.7744 total_loss = 13.8979 = 0.0684 + 0.5 * 27.6639 + 0.01 * -0.2391\n",
            "-----------------\n",
            "Finished episode: 251 Reward: -1322.4361 total_loss = 13.4197 = 0.0102 + 0.5 * 26.8236 + 0.01 * -0.2329\n",
            "-----------------\n",
            "Finished episode: 252 Reward: -1328.1366 total_loss = 15.2009 = -0.0571 + 0.5 * 30.5207 + 0.01 * -0.2403\n",
            "-----------------\n",
            "Finished episode: 253 Reward: -1323.8384 total_loss = 15.4105 = -0.0238 + 0.5 * 30.8733 + 0.01 * -0.2385\n",
            "-----------------\n",
            "Finished episode: 254 Reward: -1325.4889 total_loss = 14.7585 = 0.0198 + 0.5 * 29.4820 + 0.01 * -0.2358\n",
            "-----------------\n",
            "Finished episode: 255 Reward: -1328.7112 total_loss = 14.2355 = 0.0224 + 0.5 * 28.4309 + 0.01 * -0.2366\n",
            "-----------------\n",
            "Finished episode: 256 Reward: -1315.7479 total_loss = 14.3963 = 0.0925 + 0.5 * 28.6125 + 0.01 * -0.2352\n",
            "-----------------\n",
            "Finished episode: 257 Reward: -1316.1885 total_loss = 13.4699 = -0.0214 + 0.5 * 26.9875 + 0.01 * -0.2445\n",
            "-----------------\n",
            "Finished episode: 258 Reward: -1315.1360 total_loss = 12.0978 = -0.0118 + 0.5 * 24.2240 + 0.01 * -0.2417\n",
            "-----------------\n",
            "Finished episode: 259 Reward: -1321.1631 total_loss = 16.0561 = -0.0717 + 0.5 * 32.2605 + 0.01 * -0.2377\n",
            "-----------------\n",
            "Finished episode: 260 Reward: -1318.3438 total_loss = 15.4405 = 0.0516 + 0.5 * 30.7825 + 0.01 * -0.2388\n",
            "-----------------\n",
            "Finished episode: 261 Reward: -1313.6198 total_loss = 15.3610 = -0.1025 + 0.5 * 30.9323 + 0.01 * -0.2569\n",
            "-----------------\n",
            "Finished episode: 262 Reward: -1326.3515 total_loss = 14.8764 = -0.0313 + 0.5 * 29.8202 + 0.01 * -0.2475\n",
            "-----------------\n",
            "Finished episode: 263 Reward: -1328.2392 total_loss = 14.3512 = 0.0123 + 0.5 * 28.6830 + 0.01 * -0.2601\n",
            "-----------------\n",
            "Finished episode: 264 Reward: -1327.4947 total_loss = 13.2894 = -0.0039 + 0.5 * 26.5917 + 0.01 * -0.2528\n",
            "-----------------\n",
            "Finished episode: 265 Reward: -1309.2600 total_loss = 16.4277 = 0.0803 + 0.5 * 32.6999 + 0.01 * -0.2553\n",
            "-----------------\n",
            "Finished episode: 266 Reward: -1324.4042 total_loss = 14.6159 = 0.0463 + 0.5 * 29.1442 + 0.01 * -0.2459\n",
            "-----------------\n",
            "Finished episode: 267 Reward: -1320.7071 total_loss = 14.1793 = 0.0059 + 0.5 * 28.3517 + 0.01 * -0.2479\n",
            "-----------------\n",
            "Finished episode: 268 Reward: -1320.1353 total_loss = 11.8900 = -0.0817 + 0.5 * 23.9482 + 0.01 * -0.2393\n",
            "-----------------\n",
            "Finished episode: 269 Reward: -1312.6194 total_loss = 13.7553 = -0.0093 + 0.5 * 27.5339 + 0.01 * -0.2426\n",
            "-----------------\n",
            "Finished episode: 270 Reward: -1321.8660 total_loss = 14.4830 = 0.1912 + 0.5 * 28.5887 + 0.01 * -0.2558\n",
            "-----------------\n",
            "Finished episode: 271 Reward: -1325.0060 total_loss = 13.7601 = 0.0830 + 0.5 * 27.3590 + 0.01 * -0.2391\n",
            "-----------------\n",
            "Finished episode: 272 Reward: -1325.3987 total_loss = 12.9546 = 0.1356 + 0.5 * 25.6428 + 0.01 * -0.2439\n",
            "-----------------\n",
            "Finished episode: 273 Reward: -1306.0536 total_loss = 13.2875 = -0.0898 + 0.5 * 26.7593 + 0.01 * -0.2386\n",
            "-----------------\n",
            "Finished episode: 274 Reward: -1319.7341 total_loss = 13.3424 = 0.0131 + 0.5 * 26.6633 + 0.01 * -0.2342\n",
            "-----------------\n",
            "Finished episode: 275 Reward: -1328.5081 total_loss = 14.5138 = 0.0089 + 0.5 * 29.0149 + 0.01 * -0.2470\n",
            "-----------------\n",
            "Finished episode: 276 Reward: -1314.3201 total_loss = 13.2726 = -0.0202 + 0.5 * 26.5905 + 0.01 * -0.2399\n",
            "-----------------\n",
            "Finished episode: 277 Reward: -1328.9747 total_loss = 13.1075 = -0.0193 + 0.5 * 26.2583 + 0.01 * -0.2351\n",
            "-----------------\n",
            "Finished episode: 278 Reward: -1330.6762 total_loss = 13.1348 = 0.0094 + 0.5 * 26.2555 + 0.01 * -0.2413\n",
            "-----------------\n",
            "Finished episode: 279 Reward: -1307.4969 total_loss = 14.0982 = -0.1930 + 0.5 * 28.5870 + 0.01 * -0.2324\n",
            "-----------------\n",
            "Finished episode: 280 Reward: -1323.3912 total_loss = 13.3896 = 0.0068 + 0.5 * 26.7704 + 0.01 * -0.2390\n",
            "-----------------\n",
            "Finished episode: 281 Reward: -1327.9469 total_loss = 15.6049 = 0.0036 + 0.5 * 31.2077 + 0.01 * -0.2521\n",
            "-----------------\n",
            "Finished episode: 282 Reward: -1334.0089 total_loss = 12.7904 = -0.0707 + 0.5 * 25.7266 + 0.01 * -0.2223\n",
            "-----------------\n",
            "Finished episode: 283 Reward: -1315.4399 total_loss = 13.9598 = 0.0642 + 0.5 * 27.7961 + 0.01 * -0.2445\n",
            "-----------------\n",
            "Finished episode: 284 Reward: -1303.7028 total_loss = 13.7710 = -0.1137 + 0.5 * 27.7741 + 0.01 * -0.2392\n",
            "-----------------\n",
            "Finished episode: 285 Reward: -1332.1808 total_loss = 13.9438 = 0.0770 + 0.5 * 27.7384 + 0.01 * -0.2462\n",
            "-----------------\n",
            "Finished episode: 286 Reward: -1323.2372 total_loss = 13.4476 = -0.0567 + 0.5 * 27.0136 + 0.01 * -0.2512\n",
            "-----------------\n",
            "Finished episode: 287 Reward: -1311.1874 total_loss = 13.6498 = -0.0381 + 0.5 * 27.3808 + 0.01 * -0.2502\n",
            "-----------------\n",
            "Finished episode: 288 Reward: -1312.8704 total_loss = 14.7485 = 0.0634 + 0.5 * 29.3753 + 0.01 * -0.2561\n",
            "-----------------\n",
            "Finished episode: 289 Reward: -1319.6364 total_loss = 13.5666 = -0.1177 + 0.5 * 27.3736 + 0.01 * -0.2467\n",
            "-----------------\n",
            "Finished episode: 290 Reward: -1316.1843 total_loss = 14.4080 = 0.0933 + 0.5 * 28.6347 + 0.01 * -0.2686\n",
            "-----------------\n",
            "Finished episode: 291 Reward: -1312.1746 total_loss = 12.8191 = 0.0053 + 0.5 * 25.6327 + 0.01 * -0.2585\n",
            "-----------------\n",
            "Finished episode: 292 Reward: -1328.0772 total_loss = 14.2958 = 0.0030 + 0.5 * 28.5907 + 0.01 * -0.2499\n",
            "-----------------\n",
            "Finished episode: 293 Reward: -1333.3176 total_loss = 15.1461 = 0.0652 + 0.5 * 30.1667 + 0.01 * -0.2517\n",
            "-----------------\n",
            "Finished episode: 294 Reward: -1328.8033 total_loss = 14.7408 = -0.1335 + 0.5 * 29.7540 + 0.01 * -0.2696\n",
            "-----------------\n",
            "Finished episode: 295 Reward: -1327.8406 total_loss = 13.9347 = 0.0898 + 0.5 * 27.6951 + 0.01 * -0.2638\n",
            "-----------------\n",
            "Finished episode: 296 Reward: -1324.2939 total_loss = 15.4667 = -0.0825 + 0.5 * 31.1035 + 0.01 * -0.2553\n",
            "-----------------\n",
            "Finished episode: 297 Reward: -1329.0215 total_loss = 13.6369 = 0.1238 + 0.5 * 27.0313 + 0.01 * -0.2590\n",
            "-----------------\n",
            "Finished episode: 298 Reward: -1321.3000 total_loss = 13.6421 = 0.0942 + 0.5 * 27.1011 + 0.01 * -0.2639\n",
            "-----------------\n",
            "Finished episode: 299 Reward: -1309.2425 total_loss = 13.6838 = -0.0389 + 0.5 * 27.4506 + 0.01 * -0.2654\n",
            "-----------------\n",
            "Finished episode: 300 Reward: -1326.0561 total_loss = 13.3363 = -0.1535 + 0.5 * 26.9850 + 0.01 * -0.2625\n",
            "-----------------\n",
            "Finished episode: 301 Reward: -1328.9921 total_loss = 14.5197 = -0.0970 + 0.5 * 29.2386 + 0.01 * -0.2573\n",
            "-----------------\n",
            "Finished episode: 302 Reward: -1330.1836 total_loss = 14.9918 = -0.0098 + 0.5 * 30.0083 + 0.01 * -0.2608\n",
            "-----------------\n",
            "Finished episode: 303 Reward: -1316.0414 total_loss = 13.8672 = 0.0482 + 0.5 * 27.6435 + 0.01 * -0.2731\n",
            "-----------------\n",
            "Finished episode: 304 Reward: -1325.6813 total_loss = 14.2377 = -0.0530 + 0.5 * 28.5866 + 0.01 * -0.2594\n",
            "-----------------\n",
            "Finished episode: 305 Reward: -1332.4376 total_loss = 13.3298 = 0.0777 + 0.5 * 26.5098 + 0.01 * -0.2755\n",
            "-----------------\n",
            "Finished episode: 306 Reward: -1340.5082 total_loss = 14.6043 = -0.0285 + 0.5 * 29.2712 + 0.01 * -0.2776\n",
            "-----------------\n",
            "Finished episode: 307 Reward: -1327.7246 total_loss = 14.2988 = -0.0210 + 0.5 * 28.6449 + 0.01 * -0.2657\n",
            "-----------------\n",
            "Finished episode: 308 Reward: -1327.4014 total_loss = 13.8570 = 0.0303 + 0.5 * 27.6589 + 0.01 * -0.2712\n",
            "-----------------\n",
            "Finished episode: 309 Reward: -1323.0722 total_loss = 15.1464 = 0.0633 + 0.5 * 30.1716 + 0.01 * -0.2723\n",
            "-----------------\n",
            "Finished episode: 310 Reward: -1337.0950 total_loss = 16.4479 = -0.0845 + 0.5 * 33.0701 + 0.01 * -0.2640\n",
            "-----------------\n",
            "Finished episode: 311 Reward: -1329.8583 total_loss = 12.7482 = 0.0157 + 0.5 * 25.4703 + 0.01 * -0.2670\n",
            "-----------------\n",
            "Finished episode: 312 Reward: -1319.1133 total_loss = 13.8266 = -0.0442 + 0.5 * 27.7469 + 0.01 * -0.2615\n",
            "-----------------\n",
            "Finished episode: 313 Reward: -1326.2641 total_loss = 14.0253 = -0.0011 + 0.5 * 28.0579 + 0.01 * -0.2596\n",
            "-----------------\n",
            "Finished episode: 314 Reward: -1319.5513 total_loss = 11.4549 = -0.0311 + 0.5 * 22.9773 + 0.01 * -0.2693\n",
            "-----------------\n",
            "Finished episode: 315 Reward: -1320.6782 total_loss = 12.5116 = -0.1109 + 0.5 * 25.2505 + 0.01 * -0.2682\n",
            "-----------------\n",
            "Finished episode: 316 Reward: -1317.0899 total_loss = 12.9070 = 0.0690 + 0.5 * 25.6814 + 0.01 * -0.2648\n",
            "-----------------\n",
            "Finished episode: 317 Reward: -1311.4568 total_loss = 15.0013 = -0.0394 + 0.5 * 30.0867 + 0.01 * -0.2645\n",
            "-----------------\n",
            "Finished episode: 318 Reward: -1334.8345 total_loss = 15.5202 = 0.0548 + 0.5 * 30.9362 + 0.01 * -0.2623\n",
            "-----------------\n",
            "Finished episode: 319 Reward: -1321.8048 total_loss = 13.2531 = 0.0120 + 0.5 * 26.4874 + 0.01 * -0.2644\n",
            "-----------------\n",
            "Finished episode: 320 Reward: -1306.9329 total_loss = 12.8403 = -0.0415 + 0.5 * 25.7688 + 0.01 * -0.2566\n",
            "-----------------\n",
            "Finished episode: 321 Reward: -1329.7490 total_loss = 12.5610 = -0.0319 + 0.5 * 25.1912 + 0.01 * -0.2684\n",
            "-----------------\n",
            "Finished episode: 322 Reward: -1321.3277 total_loss = 13.2643 = 0.0410 + 0.5 * 26.4519 + 0.01 * -0.2664\n",
            "-----------------\n",
            "Finished episode: 323 Reward: -1332.4793 total_loss = 15.7976 = -0.0423 + 0.5 * 31.6850 + 0.01 * -0.2647\n",
            "-----------------\n",
            "Finished episode: 324 Reward: -1331.3426 total_loss = 15.0859 = 0.0631 + 0.5 * 30.0509 + 0.01 * -0.2671\n",
            "-----------------\n",
            "Finished episode: 325 Reward: -1325.8972 total_loss = 14.2469 = 0.1756 + 0.5 * 28.1478 + 0.01 * -0.2663\n",
            "-----------------\n",
            "Finished episode: 326 Reward: -1310.2907 total_loss = 14.6487 = 0.0624 + 0.5 * 29.1781 + 0.01 * -0.2771\n",
            "-----------------\n",
            "Finished episode: 327 Reward: -1333.1442 total_loss = 15.4187 = 0.0557 + 0.5 * 30.7313 + 0.01 * -0.2727\n",
            "-----------------\n",
            "Finished episode: 328 Reward: -1333.4114 total_loss = 13.3520 = 0.0202 + 0.5 * 26.6691 + 0.01 * -0.2718\n",
            "-----------------\n",
            "Finished episode: 329 Reward: -1323.9580 total_loss = 16.0212 = 0.0585 + 0.5 * 31.9311 + 0.01 * -0.2786\n",
            "-----------------\n",
            "Finished episode: 330 Reward: -1320.2947 total_loss = 14.6984 = 0.0873 + 0.5 * 29.2276 + 0.01 * -0.2726\n",
            "-----------------\n",
            "Finished episode: 331 Reward: -1320.1664 total_loss = 12.7771 = -0.0242 + 0.5 * 25.6080 + 0.01 * -0.2689\n",
            "-----------------\n",
            "Finished episode: 332 Reward: -1320.8262 total_loss = 14.0129 = 0.0323 + 0.5 * 27.9668 + 0.01 * -0.2730\n",
            "-----------------\n",
            "Finished episode: 333 Reward: -1308.3427 total_loss = 14.2030 = 0.0281 + 0.5 * 28.3552 + 0.01 * -0.2672\n",
            "-----------------\n",
            "Finished episode: 334 Reward: -1313.5816 total_loss = 12.7217 = 0.0472 + 0.5 * 25.3544 + 0.01 * -0.2705\n",
            "-----------------\n",
            "Finished episode: 335 Reward: -1314.7614 total_loss = 13.0535 = 0.0597 + 0.5 * 25.9930 + 0.01 * -0.2716\n",
            "-----------------\n",
            "Finished episode: 336 Reward: -1319.7141 total_loss = 15.1502 = -0.0097 + 0.5 * 30.3251 + 0.01 * -0.2612\n",
            "-----------------\n",
            "Finished episode: 337 Reward: -1322.3551 total_loss = 14.7724 = 0.0420 + 0.5 * 29.4659 + 0.01 * -0.2613\n",
            "-----------------\n",
            "Finished episode: 338 Reward: -1316.5139 total_loss = 13.1046 = 0.0032 + 0.5 * 26.2081 + 0.01 * -0.2640\n",
            "-----------------\n",
            "Finished episode: 339 Reward: -1329.7204 total_loss = 14.4897 = 0.0379 + 0.5 * 28.9091 + 0.01 * -0.2740\n",
            "-----------------\n",
            "Finished episode: 340 Reward: -1324.2432 total_loss = 15.1451 = -0.0376 + 0.5 * 30.3709 + 0.01 * -0.2747\n",
            "-----------------\n",
            "Finished episode: 341 Reward: -1294.8323 total_loss = 13.9806 = -0.0152 + 0.5 * 27.9970 + 0.01 * -0.2630\n",
            "-----------------\n",
            "Finished episode: 342 Reward: -1328.8061 total_loss = 14.0369 = 0.0805 + 0.5 * 27.9182 + 0.01 * -0.2653\n",
            "-----------------\n",
            "Finished episode: 343 Reward: -1332.5776 total_loss = 14.5533 = 0.0190 + 0.5 * 29.0739 + 0.01 * -0.2698\n",
            "-----------------\n",
            "Finished episode: 344 Reward: -1326.7614 total_loss = 13.7625 = -0.0901 + 0.5 * 27.7106 + 0.01 * -0.2725\n",
            "-----------------\n",
            "Finished episode: 345 Reward: -1317.3977 total_loss = 13.7487 = 0.0183 + 0.5 * 27.4660 + 0.01 * -0.2654\n",
            "-----------------\n",
            "Finished episode: 346 Reward: -1321.6218 total_loss = 13.1367 = 0.0485 + 0.5 * 26.1817 + 0.01 * -0.2618\n",
            "-----------------\n",
            "Finished episode: 347 Reward: -1315.1432 total_loss = 11.2672 = 0.0279 + 0.5 * 22.4839 + 0.01 * -0.2646\n",
            "-----------------\n",
            "Finished episode: 348 Reward: -1322.0174 total_loss = 13.7315 = 0.0740 + 0.5 * 27.3204 + 0.01 * -0.2695\n",
            "-----------------\n",
            "Finished episode: 349 Reward: -1315.6191 total_loss = 13.8140 = 0.0336 + 0.5 * 27.5664 + 0.01 * -0.2797\n",
            "-----------------\n",
            "Finished episode: 350 Reward: -1310.7451 total_loss = 13.4552 = -0.0407 + 0.5 * 26.9969 + 0.01 * -0.2638\n",
            "-----------------\n",
            "Finished episode: 351 Reward: -1332.1400 total_loss = 15.1601 = 0.0476 + 0.5 * 30.2305 + 0.01 * -0.2733\n",
            "-----------------\n",
            "Finished episode: 352 Reward: -1311.7704 total_loss = 14.1912 = 0.1070 + 0.5 * 28.1736 + 0.01 * -0.2615\n",
            "-----------------\n",
            "Finished episode: 353 Reward: -1317.8903 total_loss = 13.9294 = 0.0128 + 0.5 * 27.8387 + 0.01 * -0.2670\n",
            "-----------------\n",
            "Finished episode: 354 Reward: -1330.2595 total_loss = 15.9503 = 0.0009 + 0.5 * 31.9040 + 0.01 * -0.2610\n",
            "-----------------\n",
            "Finished episode: 355 Reward: -1326.4511 total_loss = 16.5881 = -0.0297 + 0.5 * 33.2411 + 0.01 * -0.2697\n",
            "-----------------\n",
            "Finished episode: 356 Reward: -1321.7057 total_loss = 14.4027 = -0.0245 + 0.5 * 28.8595 + 0.01 * -0.2618\n",
            "-----------------\n",
            "Finished episode: 357 Reward: -1329.5744 total_loss = 14.2055 = -0.1002 + 0.5 * 28.6167 + 0.01 * -0.2700\n",
            "-----------------\n",
            "Finished episode: 358 Reward: -1313.0786 total_loss = 12.0869 = 0.0545 + 0.5 * 24.0703 + 0.01 * -0.2731\n",
            "-----------------\n",
            "Finished episode: 359 Reward: -1328.2895 total_loss = 14.6044 = 0.0017 + 0.5 * 29.2107 + 0.01 * -0.2686\n",
            "-----------------\n",
            "Finished episode: 360 Reward: -1325.8607 total_loss = 14.8013 = -0.1421 + 0.5 * 29.8920 + 0.01 * -0.2661\n",
            "-----------------\n",
            "Finished episode: 361 Reward: -1325.2844 total_loss = 12.1075 = -0.0630 + 0.5 * 24.3464 + 0.01 * -0.2678\n",
            "-----------------\n",
            "Finished episode: 362 Reward: -1325.3870 total_loss = 13.1702 = 0.0042 + 0.5 * 26.3373 + 0.01 * -0.2661\n",
            "-----------------\n",
            "Finished episode: 363 Reward: -1327.2676 total_loss = 16.1395 = -0.0550 + 0.5 * 32.3943 + 0.01 * -0.2635\n",
            "-----------------\n",
            "Finished episode: 364 Reward: -1325.0059 total_loss = 14.3141 = -0.0264 + 0.5 * 28.6862 + 0.01 * -0.2622\n",
            "-----------------\n",
            "Finished episode: 365 Reward: -1320.1687 total_loss = 13.3304 = -0.0142 + 0.5 * 26.6943 + 0.01 * -0.2612\n",
            "-----------------\n",
            "Finished episode: 366 Reward: -1325.9166 total_loss = 15.7007 = 0.0417 + 0.5 * 31.3233 + 0.01 * -0.2590\n",
            "-----------------\n",
            "Finished episode: 367 Reward: -1309.3990 total_loss = 12.1037 = 0.0196 + 0.5 * 24.1732 + 0.01 * -0.2511\n",
            "-----------------\n",
            "Finished episode: 368 Reward: -1316.1963 total_loss = 13.6122 = -0.0122 + 0.5 * 27.2539 + 0.01 * -0.2648\n",
            "-----------------\n",
            "Finished episode: 369 Reward: -1317.9304 total_loss = 12.8320 = -0.0020 + 0.5 * 25.6731 + 0.01 * -0.2591\n",
            "-----------------\n",
            "Finished episode: 370 Reward: -1327.3722 total_loss = 12.6394 = -0.1400 + 0.5 * 25.5639 + 0.01 * -0.2625\n",
            "-----------------\n",
            "Finished episode: 371 Reward: -1315.5307 total_loss = 14.3358 = -0.0405 + 0.5 * 28.7577 + 0.01 * -0.2513\n",
            "-----------------\n",
            "Finished episode: 372 Reward: -1326.7710 total_loss = 13.1308 = -0.0081 + 0.5 * 26.2830 + 0.01 * -0.2575\n",
            "-----------------\n",
            "Finished episode: 373 Reward: -1322.1670 total_loss = 14.3423 = 0.1087 + 0.5 * 28.4722 + 0.01 * -0.2505\n",
            "-----------------\n",
            "Finished episode: 374 Reward: -1325.0884 total_loss = 14.1238 = -0.0328 + 0.5 * 28.3182 + 0.01 * -0.2470\n",
            "-----------------\n",
            "Finished episode: 375 Reward: -1314.6359 total_loss = 12.6500 = 0.1273 + 0.5 * 25.0504 + 0.01 * -0.2507\n",
            "-----------------\n",
            "Finished episode: 376 Reward: -1313.3475 total_loss = 13.9533 = -0.0185 + 0.5 * 27.9485 + 0.01 * -0.2504\n",
            "-----------------\n",
            "Finished episode: 377 Reward: -1311.6719 total_loss = 13.9958 = 0.1613 + 0.5 * 27.6736 + 0.01 * -0.2386\n",
            "-----------------\n",
            "Finished episode: 378 Reward: -1303.3151 total_loss = 13.4596 = 0.0932 + 0.5 * 26.7376 + 0.01 * -0.2454\n",
            "-----------------\n",
            "Finished episode: 379 Reward: -1316.6014 total_loss = 13.7229 = -0.0014 + 0.5 * 27.4532 + 0.01 * -0.2363\n",
            "-----------------\n",
            "Finished episode: 380 Reward: -1318.4545 total_loss = 13.9365 = 0.0288 + 0.5 * 27.8202 + 0.01 * -0.2329\n",
            "-----------------\n",
            "Finished episode: 381 Reward: -1334.7645 total_loss = 14.4308 = -0.0627 + 0.5 * 28.9919 + 0.01 * -0.2427\n",
            "-----------------\n",
            "Finished episode: 382 Reward: -1326.5849 total_loss = 12.6687 = 0.0439 + 0.5 * 25.2540 + 0.01 * -0.2237\n",
            "-----------------\n",
            "Finished episode: 383 Reward: -1320.9825 total_loss = 14.2019 = 0.0562 + 0.5 * 28.2962 + 0.01 * -0.2353\n",
            "-----------------\n",
            "Finished episode: 384 Reward: -1329.9756 total_loss = 15.6508 = 0.1431 + 0.5 * 31.0201 + 0.01 * -0.2421\n",
            "-----------------\n",
            "Finished episode: 385 Reward: -1321.8045 total_loss = 14.0600 = 0.0723 + 0.5 * 27.9800 + 0.01 * -0.2306\n",
            "-----------------\n",
            "Finished episode: 386 Reward: -1327.5163 total_loss = 13.6795 = -0.0621 + 0.5 * 27.4878 + 0.01 * -0.2347\n",
            "-----------------\n",
            "Finished episode: 387 Reward: -1329.6432 total_loss = 14.2298 = -0.0004 + 0.5 * 28.4651 + 0.01 * -0.2422\n",
            "-----------------\n",
            "Finished episode: 388 Reward: -1328.6828 total_loss = 14.0973 = 0.0266 + 0.5 * 28.1461 + 0.01 * -0.2340\n",
            "-----------------\n",
            "Finished episode: 389 Reward: -1327.5311 total_loss = 14.1198 = -0.0170 + 0.5 * 28.2785 + 0.01 * -0.2421\n",
            "-----------------\n",
            "Finished episode: 390 Reward: -1321.7830 total_loss = 13.4805 = -0.0397 + 0.5 * 27.0452 + 0.01 * -0.2393\n",
            "-----------------\n",
            "Finished episode: 391 Reward: -1317.6674 total_loss = 13.6476 = 0.0072 + 0.5 * 27.2855 + 0.01 * -0.2349\n",
            "-----------------\n",
            "Finished episode: 392 Reward: -1319.8253 total_loss = 11.9126 = 0.0894 + 0.5 * 23.6510 + 0.01 * -0.2325\n",
            "-----------------\n",
            "Finished episode: 393 Reward: -1319.6800 total_loss = 14.4054 = -0.0766 + 0.5 * 28.9686 + 0.01 * -0.2267\n",
            "-----------------\n",
            "Finished episode: 394 Reward: -1319.0406 total_loss = 13.7235 = -0.0432 + 0.5 * 27.5382 + 0.01 * -0.2414\n",
            "-----------------\n",
            "Finished episode: 395 Reward: -1307.2350 total_loss = 13.4309 = -0.0351 + 0.5 * 26.9369 + 0.01 * -0.2379\n",
            "-----------------\n",
            "Finished episode: 396 Reward: -1321.4484 total_loss = 13.6511 = -0.1164 + 0.5 * 27.5397 + 0.01 * -0.2390\n",
            "-----------------\n",
            "Finished episode: 397 Reward: -1323.2992 total_loss = 12.7286 = 0.0814 + 0.5 * 25.2994 + 0.01 * -0.2451\n",
            "-----------------\n",
            "Finished episode: 398 Reward: -1312.0333 total_loss = 14.5452 = -0.0529 + 0.5 * 29.2008 + 0.01 * -0.2284\n",
            "-----------------\n",
            "Finished episode: 399 Reward: -1325.7440 total_loss = 13.5901 = 0.0114 + 0.5 * 27.1621 + 0.01 * -0.2383\n",
            "-----------------\n",
            "Finished episode: 400 Reward: -1326.2678 total_loss = 14.4712 = 0.0298 + 0.5 * 28.8875 + 0.01 * -0.2342\n",
            "-----------------\n",
            "Finished episode: 401 Reward: -1322.8623 total_loss = 14.7237 = 0.0104 + 0.5 * 29.4315 + 0.01 * -0.2396\n",
            "-----------------\n",
            "Finished episode: 402 Reward: -1324.1455 total_loss = 13.4455 = -0.0689 + 0.5 * 27.0337 + 0.01 * -0.2480\n",
            "-----------------\n",
            "Finished episode: 403 Reward: -1326.7813 total_loss = 13.8391 = -0.0327 + 0.5 * 27.7482 + 0.01 * -0.2292\n",
            "-----------------\n",
            "Finished episode: 404 Reward: -1323.2109 total_loss = 12.4696 = 0.0389 + 0.5 * 24.8660 + 0.01 * -0.2317\n",
            "-----------------\n",
            "Finished episode: 405 Reward: -1324.5528 total_loss = 16.7495 = -0.0185 + 0.5 * 33.5406 + 0.01 * -0.2318\n",
            "-----------------\n",
            "Finished episode: 406 Reward: -1326.3586 total_loss = 14.6272 = -0.0405 + 0.5 * 29.3402 + 0.01 * -0.2362\n",
            "-----------------\n",
            "Finished episode: 407 Reward: -1323.0343 total_loss = 12.4718 = -0.0800 + 0.5 * 25.1083 + 0.01 * -0.2323\n",
            "-----------------\n",
            "Finished episode: 408 Reward: -1317.7826 total_loss = 13.6246 = 0.0279 + 0.5 * 27.1983 + 0.01 * -0.2391\n",
            "-----------------\n",
            "Finished episode: 409 Reward: -1303.0541 total_loss = 10.9127 = -0.0506 + 0.5 * 21.9313 + 0.01 * -0.2344\n",
            "-----------------\n",
            "Finished episode: 410 Reward: -1324.0668 total_loss = 13.1137 = 0.0306 + 0.5 * 26.1711 + 0.01 * -0.2455\n",
            "-----------------\n",
            "Finished episode: 411 Reward: -1317.2145 total_loss = 12.3318 = 0.1253 + 0.5 * 24.4179 + 0.01 * -0.2421\n",
            "-----------------\n",
            "Finished episode: 412 Reward: -1324.1097 total_loss = 14.0123 = -0.0236 + 0.5 * 28.0763 + 0.01 * -0.2346\n",
            "-----------------\n",
            "Finished episode: 413 Reward: -1311.6735 total_loss = 15.5361 = 0.0597 + 0.5 * 30.9574 + 0.01 * -0.2330\n",
            "-----------------\n",
            "Finished episode: 414 Reward: -1313.9172 total_loss = 12.7932 = 0.0111 + 0.5 * 25.5689 + 0.01 * -0.2320\n",
            "-----------------\n",
            "Finished episode: 415 Reward: -1310.4212 total_loss = 13.0562 = -0.1054 + 0.5 * 26.3280 + 0.01 * -0.2332\n",
            "-----------------\n",
            "Finished episode: 416 Reward: -1322.9778 total_loss = 12.5624 = 0.0998 + 0.5 * 24.9300 + 0.01 * -0.2346\n",
            "-----------------\n",
            "Finished episode: 417 Reward: -1328.9756 total_loss = 13.3841 = -0.0618 + 0.5 * 26.8965 + 0.01 * -0.2283\n",
            "-----------------\n",
            "Finished episode: 418 Reward: -1327.0543 total_loss = 14.4542 = -0.0352 + 0.5 * 28.9835 + 0.01 * -0.2334\n",
            "-----------------\n",
            "Finished episode: 419 Reward: -1330.0075 total_loss = 14.4466 = -0.1182 + 0.5 * 29.1343 + 0.01 * -0.2355\n",
            "-----------------\n",
            "Finished episode: 420 Reward: -1307.4667 total_loss = 13.1469 = -0.0301 + 0.5 * 26.3586 + 0.01 * -0.2286\n",
            "-----------------\n",
            "Finished episode: 421 Reward: -1328.4613 total_loss = 14.4041 = 0.0138 + 0.5 * 28.7850 + 0.01 * -0.2174\n",
            "-----------------\n",
            "Finished episode: 422 Reward: -1325.3595 total_loss = 13.4744 = -0.1081 + 0.5 * 27.1695 + 0.01 * -0.2223\n",
            "-----------------\n",
            "Finished episode: 423 Reward: -1317.0044 total_loss = 13.2969 = -0.0162 + 0.5 * 26.6309 + 0.01 * -0.2305\n",
            "-----------------\n",
            "Finished episode: 424 Reward: -1317.8280 total_loss = 12.2208 = 0.0411 + 0.5 * 24.3640 + 0.01 * -0.2364\n",
            "-----------------\n",
            "Finished episode: 425 Reward: -1326.1648 total_loss = 13.7254 = 0.0427 + 0.5 * 27.3700 + 0.01 * -0.2228\n",
            "-----------------\n",
            "Finished episode: 426 Reward: -1319.3573 total_loss = 13.4214 = 0.0343 + 0.5 * 26.7785 + 0.01 * -0.2216\n",
            "-----------------\n",
            "Finished episode: 427 Reward: -1333.7300 total_loss = 13.8582 = 0.1166 + 0.5 * 27.4879 + 0.01 * -0.2348\n",
            "-----------------\n",
            "Finished episode: 428 Reward: -1331.3826 total_loss = 14.4353 = -0.0300 + 0.5 * 28.9351 + 0.01 * -0.2260\n",
            "-----------------\n",
            "Finished episode: 429 Reward: -1322.1112 total_loss = 14.1379 = -0.0795 + 0.5 * 28.4393 + 0.01 * -0.2229\n",
            "-----------------\n",
            "Finished episode: 430 Reward: -1321.3460 total_loss = 13.4493 = 0.0302 + 0.5 * 26.8427 + 0.01 * -0.2316\n",
            "-----------------\n",
            "Finished episode: 431 Reward: -1327.9425 total_loss = 14.4385 = -0.0031 + 0.5 * 28.8878 + 0.01 * -0.2343\n",
            "-----------------\n",
            "Finished episode: 432 Reward: -1338.7972 total_loss = 15.3257 = -0.0281 + 0.5 * 30.7124 + 0.01 * -0.2365\n",
            "-----------------\n",
            "Finished episode: 433 Reward: -1332.7420 total_loss = 13.4339 = 0.0465 + 0.5 * 26.7794 + 0.01 * -0.2306\n",
            "-----------------\n",
            "Finished episode: 434 Reward: -1318.4416 total_loss = 14.6541 = -0.1215 + 0.5 * 29.5560 + 0.01 * -0.2395\n",
            "-----------------\n",
            "Finished episode: 435 Reward: -1323.0587 total_loss = 15.0540 = -0.0328 + 0.5 * 30.1784 + 0.01 * -0.2353\n",
            "-----------------\n",
            "Finished episode: 436 Reward: -1323.2094 total_loss = 14.0546 = -0.0099 + 0.5 * 28.1334 + 0.01 * -0.2226\n",
            "-----------------\n",
            "Finished episode: 437 Reward: -1324.1485 total_loss = 14.7465 = 0.0163 + 0.5 * 29.4650 + 0.01 * -0.2257\n",
            "-----------------\n",
            "Finished episode: 438 Reward: -1329.1738 total_loss = 12.9019 = 0.0576 + 0.5 * 25.6932 + 0.01 * -0.2290\n",
            "-----------------\n",
            "Finished episode: 439 Reward: -1330.4962 total_loss = 13.3564 = -0.0588 + 0.5 * 26.8351 + 0.01 * -0.2353\n",
            "-----------------\n",
            "Finished episode: 440 Reward: -1327.9873 total_loss = 14.9597 = 0.0093 + 0.5 * 29.9057 + 0.01 * -0.2456\n",
            "-----------------\n",
            "Finished episode: 441 Reward: -1312.7613 total_loss = 12.6320 = 0.0777 + 0.5 * 25.1131 + 0.01 * -0.2264\n",
            "-----------------\n",
            "Finished episode: 442 Reward: -1328.6162 total_loss = 13.8293 = -0.0062 + 0.5 * 27.6757 + 0.01 * -0.2349\n",
            "-----------------\n",
            "Finished episode: 443 Reward: -1304.0546 total_loss = 13.3611 = 0.0570 + 0.5 * 26.6128 + 0.01 * -0.2349\n",
            "-----------------\n",
            "Finished episode: 444 Reward: -1336.2328 total_loss = 13.7445 = 0.0485 + 0.5 * 27.3968 + 0.01 * -0.2446\n",
            "-----------------\n",
            "Finished episode: 445 Reward: -1332.5714 total_loss = 15.9881 = 0.0191 + 0.5 * 31.9426 + 0.01 * -0.2354\n",
            "-----------------\n",
            "Finished episode: 446 Reward: -1332.4823 total_loss = 12.2593 = -0.0167 + 0.5 * 24.5564 + 0.01 * -0.2299\n",
            "-----------------\n",
            "Finished episode: 447 Reward: -1325.3147 total_loss = 15.0307 = -0.0584 + 0.5 * 30.1827 + 0.01 * -0.2312\n",
            "-----------------\n",
            "Finished episode: 448 Reward: -1318.7497 total_loss = 13.6631 = -0.0891 + 0.5 * 27.5091 + 0.01 * -0.2401\n",
            "-----------------\n",
            "Finished episode: 449 Reward: -1326.0139 total_loss = 14.0194 = -0.0462 + 0.5 * 28.1360 + 0.01 * -0.2416\n",
            "-----------------\n",
            "Finished episode: 450 Reward: -1316.6336 total_loss = 11.5378 = 0.0445 + 0.5 * 22.9912 + 0.01 * -0.2285\n",
            "-----------------\n",
            "Finished episode: 451 Reward: -1319.8491 total_loss = 12.8166 = -0.0201 + 0.5 * 25.6779 + 0.01 * -0.2305\n",
            "-----------------\n",
            "Finished episode: 452 Reward: -1326.0470 total_loss = 13.5678 = 0.0740 + 0.5 * 26.9920 + 0.01 * -0.2196\n",
            "-----------------\n",
            "Finished episode: 453 Reward: -1326.7394 total_loss = 14.1586 = 0.0073 + 0.5 * 28.3072 + 0.01 * -0.2259\n",
            "-----------------\n",
            "Finished episode: 454 Reward: -1322.8308 total_loss = 14.0946 = 0.0496 + 0.5 * 28.0946 + 0.01 * -0.2325\n",
            "-----------------\n",
            "Finished episode: 455 Reward: -1330.6556 total_loss = 13.0648 = -0.0266 + 0.5 * 26.1875 + 0.01 * -0.2351\n",
            "-----------------\n",
            "Finished episode: 456 Reward: -1334.5680 total_loss = 14.9976 = 0.0741 + 0.5 * 29.8517 + 0.01 * -0.2337\n",
            "-----------------\n",
            "Finished episode: 457 Reward: -1330.1013 total_loss = 13.3825 = 0.1913 + 0.5 * 26.3872 + 0.01 * -0.2375\n",
            "-----------------\n",
            "Finished episode: 458 Reward: -1320.2051 total_loss = 13.8372 = -0.0037 + 0.5 * 27.6866 + 0.01 * -0.2412\n",
            "-----------------\n",
            "Finished episode: 459 Reward: -1325.0106 total_loss = 13.1177 = -0.0215 + 0.5 * 26.2831 + 0.01 * -0.2408\n",
            "-----------------\n",
            "Finished episode: 460 Reward: -1319.4780 total_loss = 15.0017 = 0.0947 + 0.5 * 29.8191 + 0.01 * -0.2470\n",
            "-----------------\n",
            "Finished episode: 461 Reward: -1336.5762 total_loss = 15.1572 = -0.1175 + 0.5 * 30.5543 + 0.01 * -0.2415\n",
            "-----------------\n",
            "Finished episode: 462 Reward: -1331.1497 total_loss = 13.8207 = 0.0035 + 0.5 * 27.6391 + 0.01 * -0.2303\n",
            "-----------------\n",
            "Finished episode: 463 Reward: -1322.8874 total_loss = 14.8683 = 0.0420 + 0.5 * 29.6573 + 0.01 * -0.2354\n",
            "-----------------\n",
            "Finished episode: 464 Reward: -1331.6142 total_loss = 14.0541 = -0.0077 + 0.5 * 28.1281 + 0.01 * -0.2314\n",
            "-----------------\n",
            "Finished episode: 465 Reward: -1327.9123 total_loss = 14.3912 = -0.1027 + 0.5 * 28.9924 + 0.01 * -0.2315\n",
            "-----------------\n",
            "Finished episode: 466 Reward: -1325.3406 total_loss = 12.7000 = -0.0154 + 0.5 * 25.4354 + 0.01 * -0.2272\n",
            "-----------------\n",
            "Finished episode: 467 Reward: -1331.7034 total_loss = 14.8721 = -0.0372 + 0.5 * 29.8231 + 0.01 * -0.2299\n",
            "-----------------\n",
            "Finished episode: 468 Reward: -1314.6881 total_loss = 13.4348 = -0.1275 + 0.5 * 27.1292 + 0.01 * -0.2246\n",
            "-----------------\n",
            "Finished episode: 469 Reward: -1337.0384 total_loss = 16.0515 = 0.1006 + 0.5 * 31.9063 + 0.01 * -0.2212\n",
            "-----------------\n",
            "Finished episode: 470 Reward: -1313.6511 total_loss = 12.8489 = 0.0232 + 0.5 * 25.6560 + 0.01 * -0.2228\n",
            "-----------------\n",
            "Finished episode: 471 Reward: -1310.8880 total_loss = 14.1748 = -0.0742 + 0.5 * 28.5025 + 0.01 * -0.2230\n",
            "-----------------\n",
            "Finished episode: 472 Reward: -1328.3017 total_loss = 13.2480 = -0.0094 + 0.5 * 26.5193 + 0.01 * -0.2257\n",
            "-----------------\n",
            "Finished episode: 473 Reward: -1322.4617 total_loss = 15.2052 = -0.0048 + 0.5 * 30.4249 + 0.01 * -0.2351\n",
            "-----------------\n",
            "Finished episode: 474 Reward: -1335.6784 total_loss = 13.4007 = -0.0542 + 0.5 * 26.9142 + 0.01 * -0.2276\n",
            "-----------------\n",
            "Finished episode: 475 Reward: -1327.6871 total_loss = 15.6742 = 0.0539 + 0.5 * 31.2450 + 0.01 * -0.2225\n",
            "-----------------\n",
            "Finished episode: 476 Reward: -1329.9070 total_loss = 13.5918 = -0.0182 + 0.5 * 27.2245 + 0.01 * -0.2187\n",
            "-----------------\n",
            "Finished episode: 477 Reward: -1326.3277 total_loss = 13.4577 = 0.0324 + 0.5 * 26.8552 + 0.01 * -0.2304\n",
            "-----------------\n",
            "Finished episode: 478 Reward: -1331.1494 total_loss = 13.7933 = 0.0224 + 0.5 * 27.5461 + 0.01 * -0.2223\n",
            "-----------------\n",
            "Finished episode: 479 Reward: -1337.8730 total_loss = 12.8238 = 0.0249 + 0.5 * 25.6023 + 0.01 * -0.2243\n",
            "-----------------\n",
            "Finished episode: 480 Reward: -1322.4682 total_loss = 13.1803 = -0.0278 + 0.5 * 26.4207 + 0.01 * -0.2211\n",
            "-----------------\n",
            "Finished episode: 481 Reward: -1336.9684 total_loss = 13.9135 = 0.0293 + 0.5 * 27.7730 + 0.01 * -0.2307\n",
            "-----------------\n",
            "Finished episode: 482 Reward: -1333.8934 total_loss = 13.9450 = -0.0459 + 0.5 * 27.9864 + 0.01 * -0.2282\n",
            "-----------------\n",
            "Finished episode: 483 Reward: -1319.7698 total_loss = 13.1549 = -0.0141 + 0.5 * 26.3425 + 0.01 * -0.2320\n",
            "-----------------\n",
            "Finished episode: 484 Reward: -1330.2481 total_loss = 14.0727 = 0.1510 + 0.5 * 27.8481 + 0.01 * -0.2267\n",
            "-----------------\n",
            "Finished episode: 485 Reward: -1322.7663 total_loss = 13.8544 = -0.1137 + 0.5 * 27.9407 + 0.01 * -0.2273\n",
            "-----------------\n",
            "Finished episode: 486 Reward: -1324.7661 total_loss = 11.7173 = -0.0752 + 0.5 * 23.5898 + 0.01 * -0.2379\n",
            "-----------------\n",
            "Finished episode: 487 Reward: -1330.8131 total_loss = 14.8243 = 0.0866 + 0.5 * 29.4802 + 0.01 * -0.2354\n",
            "-----------------\n",
            "Finished episode: 488 Reward: -1334.2214 total_loss = 16.0426 = -0.0956 + 0.5 * 32.2812 + 0.01 * -0.2399\n",
            "-----------------\n",
            "Finished episode: 489 Reward: -1317.3163 total_loss = 13.7648 = 0.0033 + 0.5 * 27.5277 + 0.01 * -0.2424\n",
            "-----------------\n",
            "Finished episode: 490 Reward: -1332.5029 total_loss = 15.3295 = 0.0907 + 0.5 * 30.4823 + 0.01 * -0.2334\n",
            "-----------------\n",
            "Finished episode: 491 Reward: -1329.1912 total_loss = 12.6266 = 0.0430 + 0.5 * 25.1720 + 0.01 * -0.2387\n",
            "-----------------\n",
            "Finished episode: 492 Reward: -1322.8025 total_loss = 13.2984 = -0.0404 + 0.5 * 26.6826 + 0.01 * -0.2453\n",
            "-----------------\n",
            "Finished episode: 493 Reward: -1327.7891 total_loss = 12.6570 = -0.1060 + 0.5 * 25.5309 + 0.01 * -0.2487\n",
            "-----------------\n",
            "Finished episode: 494 Reward: -1324.5202 total_loss = 13.5199 = 0.0588 + 0.5 * 26.9270 + 0.01 * -0.2428\n",
            "-----------------\n",
            "Finished episode: 495 Reward: -1340.8942 total_loss = 14.9975 = 0.0123 + 0.5 * 29.9753 + 0.01 * -0.2427\n",
            "-----------------\n",
            "Finished episode: 496 Reward: -1334.8806 total_loss = 14.1105 = 0.0027 + 0.5 * 28.2204 + 0.01 * -0.2398\n",
            "-----------------\n",
            "Finished episode: 497 Reward: -1333.9178 total_loss = 16.3053 = -0.0097 + 0.5 * 32.6349 + 0.01 * -0.2431\n",
            "-----------------\n",
            "Finished episode: 498 Reward: -1329.3795 total_loss = 13.7075 = 0.0409 + 0.5 * 27.3381 + 0.01 * -0.2482\n",
            "-----------------\n",
            "Finished episode: 499 Reward: -1333.1682 total_loss = 16.5030 = 0.0777 + 0.5 * 32.8556 + 0.01 * -0.2490\n",
            "-----------------\n",
            "Finished episode: 500 Reward: -1332.5585 total_loss = 14.3978 = -0.0312 + 0.5 * 28.8628 + 0.01 * -0.2406\n",
            "-----------------\n",
            "Finished episode: 501 Reward: -1330.4112 total_loss = 15.5150 = 0.0164 + 0.5 * 31.0023 + 0.01 * -0.2527\n",
            "-----------------\n",
            "Finished episode: 502 Reward: -1332.8301 total_loss = 14.1497 = 0.0624 + 0.5 * 28.1794 + 0.01 * -0.2418\n",
            "-----------------\n",
            "Finished episode: 503 Reward: -1335.3225 total_loss = 15.7383 = -0.0224 + 0.5 * 31.5263 + 0.01 * -0.2509\n",
            "-----------------\n",
            "Finished episode: 504 Reward: -1331.0249 total_loss = 12.9852 = -0.0155 + 0.5 * 26.0064 + 0.01 * -0.2468\n",
            "-----------------\n",
            "Finished episode: 505 Reward: -1323.4874 total_loss = 13.5342 = 0.0118 + 0.5 * 27.0497 + 0.01 * -0.2493\n",
            "-----------------\n",
            "Finished episode: 506 Reward: -1338.2287 total_loss = 15.6240 = -0.1530 + 0.5 * 31.5589 + 0.01 * -0.2462\n",
            "-----------------\n",
            "Finished episode: 507 Reward: -1326.3406 total_loss = 13.6920 = 0.0033 + 0.5 * 27.3823 + 0.01 * -0.2424\n",
            "-----------------\n",
            "Finished episode: 508 Reward: -1330.7175 total_loss = 13.9366 = -0.0118 + 0.5 * 27.9016 + 0.01 * -0.2384\n",
            "-----------------\n",
            "Finished episode: 509 Reward: -1336.4556 total_loss = 15.1283 = 0.0386 + 0.5 * 30.1843 + 0.01 * -0.2478\n",
            "-----------------\n",
            "Finished episode: 510 Reward: -1340.8357 total_loss = 14.6806 = -0.0026 + 0.5 * 29.3711 + 0.01 * -0.2423\n",
            "-----------------\n",
            "Finished episode: 511 Reward: -1333.0789 total_loss = 14.4562 = -0.1018 + 0.5 * 29.1210 + 0.01 * -0.2430\n",
            "-----------------\n",
            "Finished episode: 512 Reward: -1333.0405 total_loss = 13.5093 = 0.0160 + 0.5 * 26.9914 + 0.01 * -0.2343\n",
            "-----------------\n",
            "Finished episode: 513 Reward: -1329.9019 total_loss = 14.3245 = -0.0371 + 0.5 * 28.7281 + 0.01 * -0.2424\n",
            "-----------------\n",
            "Finished episode: 514 Reward: -1332.2261 total_loss = 13.9692 = -0.0169 + 0.5 * 27.9771 + 0.01 * -0.2459\n",
            "-----------------\n",
            "Finished episode: 515 Reward: -1328.6208 total_loss = 15.5976 = 0.1000 + 0.5 * 31.0001 + 0.01 * -0.2373\n",
            "-----------------\n",
            "Finished episode: 516 Reward: -1331.7031 total_loss = 12.9429 = -0.0565 + 0.5 * 26.0036 + 0.01 * -0.2433\n",
            "-----------------\n",
            "Finished episode: 517 Reward: -1324.0168 total_loss = 15.3489 = -0.1030 + 0.5 * 30.9085 + 0.01 * -0.2345\n",
            "-----------------\n",
            "Finished episode: 518 Reward: -1334.4955 total_loss = 14.8705 = -0.0417 + 0.5 * 29.8291 + 0.01 * -0.2386\n",
            "-----------------\n",
            "Finished episode: 519 Reward: -1333.9160 total_loss = 14.7614 = 0.0453 + 0.5 * 29.4372 + 0.01 * -0.2429\n",
            "-----------------\n",
            "Finished episode: 520 Reward: -1326.6905 total_loss = 12.1922 = -0.0142 + 0.5 * 24.4178 + 0.01 * -0.2515\n",
            "-----------------\n",
            "Finished episode: 521 Reward: -1328.0232 total_loss = 13.9376 = -0.0579 + 0.5 * 27.9959 + 0.01 * -0.2466\n",
            "-----------------\n",
            "Finished episode: 522 Reward: -1335.6557 total_loss = 17.0138 = 0.0890 + 0.5 * 33.8547 + 0.01 * -0.2543\n",
            "-----------------\n",
            "Finished episode: 523 Reward: -1335.4891 total_loss = 15.1908 = -0.0995 + 0.5 * 30.5857 + 0.01 * -0.2547\n",
            "-----------------\n",
            "Finished episode: 524 Reward: -1318.0468 total_loss = 13.2191 = 0.0148 + 0.5 * 26.4136 + 0.01 * -0.2538\n",
            "-----------------\n",
            "Finished episode: 525 Reward: -1325.2933 total_loss = 14.3671 = 0.0553 + 0.5 * 28.6288 + 0.01 * -0.2577\n",
            "-----------------\n",
            "Finished episode: 526 Reward: -1315.6731 total_loss = 12.8982 = 0.0357 + 0.5 * 25.7301 + 0.01 * -0.2593\n",
            "-----------------\n",
            "Finished episode: 527 Reward: -1335.2568 total_loss = 14.5849 = -0.0382 + 0.5 * 29.2514 + 0.01 * -0.2600\n",
            "-----------------\n",
            "Finished episode: 528 Reward: -1344.5377 total_loss = 15.8282 = 0.0805 + 0.5 * 31.5007 + 0.01 * -0.2568\n",
            "-----------------\n",
            "Finished episode: 529 Reward: -1319.3572 total_loss = 15.2293 = 0.0697 + 0.5 * 30.3243 + 0.01 * -0.2554\n",
            "-----------------\n",
            "Finished episode: 530 Reward: -1322.4220 total_loss = 14.4663 = -0.0045 + 0.5 * 28.9469 + 0.01 * -0.2696\n",
            "-----------------\n",
            "Finished episode: 531 Reward: -1324.3472 total_loss = 13.0851 = -0.0011 + 0.5 * 26.1777 + 0.01 * -0.2607\n",
            "-----------------\n",
            "Finished episode: 532 Reward: -1320.1584 total_loss = 13.7186 = -0.0076 + 0.5 * 27.4576 + 0.01 * -0.2657\n",
            "-----------------\n",
            "Finished episode: 533 Reward: -1327.4692 total_loss = 14.9667 = -0.0293 + 0.5 * 29.9969 + 0.01 * -0.2459\n",
            "-----------------\n",
            "Finished episode: 534 Reward: -1333.0503 total_loss = 13.3551 = 0.0367 + 0.5 * 26.6418 + 0.01 * -0.2472\n",
            "-----------------\n",
            "Finished episode: 535 Reward: -1330.5085 total_loss = 13.6550 = -0.0750 + 0.5 * 27.4650 + 0.01 * -0.2509\n",
            "-----------------\n",
            "Finished episode: 536 Reward: -1325.9169 total_loss = 12.4595 = -0.0781 + 0.5 * 25.0805 + 0.01 * -0.2608\n",
            "-----------------\n",
            "Finished episode: 537 Reward: -1318.3390 total_loss = 14.0987 = 0.0104 + 0.5 * 28.1816 + 0.01 * -0.2558\n",
            "-----------------\n",
            "Finished episode: 538 Reward: -1323.6782 total_loss = 12.2256 = -0.0861 + 0.5 * 24.6284 + 0.01 * -0.2477\n",
            "-----------------\n",
            "Finished episode: 539 Reward: -1320.8351 total_loss = 14.3901 = 0.0296 + 0.5 * 28.7257 + 0.01 * -0.2426\n",
            "-----------------\n",
            "Finished episode: 540 Reward: -1325.4336 total_loss = 12.4105 = 0.0244 + 0.5 * 24.7772 + 0.01 * -0.2428\n",
            "-----------------\n",
            "Finished episode: 541 Reward: -1334.6351 total_loss = 13.7475 = -0.0642 + 0.5 * 27.6282 + 0.01 * -0.2441\n",
            "-----------------\n",
            "Finished episode: 542 Reward: -1325.8758 total_loss = 14.6211 = -0.0701 + 0.5 * 29.3871 + 0.01 * -0.2436\n",
            "-----------------\n",
            "Finished episode: 543 Reward: -1326.0460 total_loss = 13.3038 = 0.0134 + 0.5 * 26.5854 + 0.01 * -0.2356\n",
            "-----------------\n",
            "Finished episode: 544 Reward: -1329.5864 total_loss = 15.7976 = 0.0537 + 0.5 * 31.4928 + 0.01 * -0.2446\n",
            "-----------------\n",
            "Finished episode: 545 Reward: -1312.4254 total_loss = 13.9628 = 0.0316 + 0.5 * 27.8670 + 0.01 * -0.2318\n",
            "-----------------\n",
            "Finished episode: 546 Reward: -1343.9361 total_loss = 15.6583 = 0.0967 + 0.5 * 31.1280 + 0.01 * -0.2381\n",
            "-----------------\n",
            "Finished episode: 547 Reward: -1328.3366 total_loss = 14.5316 = 0.0118 + 0.5 * 29.0445 + 0.01 * -0.2445\n",
            "-----------------\n",
            "Finished episode: 548 Reward: -1344.2993 total_loss = 15.9738 = -0.0282 + 0.5 * 32.0088 + 0.01 * -0.2420\n",
            "-----------------\n",
            "Finished episode: 549 Reward: -1337.6718 total_loss = 15.5321 = -0.0481 + 0.5 * 31.1652 + 0.01 * -0.2428\n",
            "-----------------\n",
            "Finished episode: 550 Reward: -1323.3086 total_loss = 14.3667 = 0.0033 + 0.5 * 28.7316 + 0.01 * -0.2416\n",
            "-----------------\n",
            "Finished episode: 551 Reward: -1333.4241 total_loss = 13.9600 = -0.0806 + 0.5 * 28.0860 + 0.01 * -0.2410\n",
            "-----------------\n",
            "Finished episode: 552 Reward: -1331.3436 total_loss = 12.4387 = -0.0780 + 0.5 * 25.0383 + 0.01 * -0.2514\n",
            "-----------------\n",
            "Finished episode: 553 Reward: -1335.3140 total_loss = 15.5863 = 0.0553 + 0.5 * 31.0668 + 0.01 * -0.2434\n",
            "-----------------\n",
            "Finished episode: 554 Reward: -1331.7487 total_loss = 12.8497 = -0.0478 + 0.5 * 25.7999 + 0.01 * -0.2458\n",
            "-----------------\n",
            "Finished episode: 555 Reward: -1336.9883 total_loss = 14.7349 = -0.0263 + 0.5 * 29.5270 + 0.01 * -0.2370\n",
            "-----------------\n",
            "Finished episode: 556 Reward: -1329.4686 total_loss = 13.4606 = 0.0476 + 0.5 * 26.8309 + 0.01 * -0.2524\n",
            "-----------------\n",
            "Finished episode: 557 Reward: -1321.8458 total_loss = 13.2583 = -0.0179 + 0.5 * 26.5576 + 0.01 * -0.2524\n",
            "-----------------\n",
            "Finished episode: 558 Reward: -1322.0277 total_loss = 14.4216 = 0.0145 + 0.5 * 28.8189 + 0.01 * -0.2356\n",
            "-----------------\n",
            "Finished episode: 559 Reward: -1322.7575 total_loss = 15.8966 = -0.0606 + 0.5 * 31.9194 + 0.01 * -0.2459\n",
            "-----------------\n",
            "Finished episode: 560 Reward: -1336.5255 total_loss = 15.0810 = -0.0028 + 0.5 * 30.1725 + 0.01 * -0.2436\n",
            "-----------------\n",
            "Finished episode: 561 Reward: -1346.8767 total_loss = 14.9853 = 0.0703 + 0.5 * 29.8348 + 0.01 * -0.2366\n",
            "-----------------\n",
            "Finished episode: 562 Reward: -1334.4263 total_loss = 14.1237 = 0.0847 + 0.5 * 28.0827 + 0.01 * -0.2358\n",
            "-----------------\n",
            "Finished episode: 563 Reward: -1331.4873 total_loss = 15.4813 = 0.0160 + 0.5 * 30.9356 + 0.01 * -0.2536\n",
            "-----------------\n",
            "Finished episode: 564 Reward: -1331.4231 total_loss = 14.6767 = 0.0327 + 0.5 * 29.2927 + 0.01 * -0.2366\n",
            "-----------------\n",
            "Finished episode: 565 Reward: -1328.7419 total_loss = 13.1711 = 0.0338 + 0.5 * 26.2793 + 0.01 * -0.2295\n",
            "-----------------\n",
            "Finished episode: 566 Reward: -1336.6982 total_loss = 13.8267 = -0.0610 + 0.5 * 27.7801 + 0.01 * -0.2361\n",
            "-----------------\n",
            "Finished episode: 567 Reward: -1321.5266 total_loss = 12.4512 = -0.1344 + 0.5 * 25.1761 + 0.01 * -0.2433\n",
            "-----------------\n",
            "Finished episode: 568 Reward: -1330.5942 total_loss = 13.0548 = -0.0733 + 0.5 * 26.2610 + 0.01 * -0.2403\n",
            "-----------------\n",
            "Finished episode: 569 Reward: -1340.8039 total_loss = 15.5395 = -0.0395 + 0.5 * 31.1629 + 0.01 * -0.2420\n",
            "-----------------\n",
            "Finished episode: 570 Reward: -1338.6526 total_loss = 15.4800 = 0.0575 + 0.5 * 30.8499 + 0.01 * -0.2433\n",
            "-----------------\n",
            "Finished episode: 571 Reward: -1332.7867 total_loss = 13.2120 = 0.0342 + 0.5 * 26.3604 + 0.01 * -0.2416\n",
            "-----------------\n",
            "Finished episode: 572 Reward: -1315.8300 total_loss = 14.3665 = -0.1668 + 0.5 * 29.0715 + 0.01 * -0.2439\n",
            "-----------------\n",
            "Finished episode: 573 Reward: -1324.6634 total_loss = 15.9354 = -0.0646 + 0.5 * 32.0047 + 0.01 * -0.2413\n",
            "-----------------\n",
            "Finished episode: 574 Reward: -1322.7370 total_loss = 15.3225 = 0.0272 + 0.5 * 30.5955 + 0.01 * -0.2423\n",
            "-----------------\n",
            "Finished episode: 575 Reward: -1340.9895 total_loss = 13.9426 = 0.0579 + 0.5 * 27.7742 + 0.01 * -0.2421\n",
            "-----------------\n",
            "Finished episode: 576 Reward: -1335.5894 total_loss = 14.1220 = 0.0136 + 0.5 * 28.2211 + 0.01 * -0.2234\n",
            "-----------------\n",
            "Finished episode: 577 Reward: -1329.2657 total_loss = 13.6958 = 0.0539 + 0.5 * 27.2883 + 0.01 * -0.2279\n",
            "-----------------\n",
            "Finished episode: 578 Reward: -1325.8434 total_loss = 14.0340 = -0.0220 + 0.5 * 28.1167 + 0.01 * -0.2318\n",
            "-----------------\n",
            "Finished episode: 579 Reward: -1330.7644 total_loss = 14.7679 = -0.0489 + 0.5 * 29.6384 + 0.01 * -0.2416\n",
            "-----------------\n",
            "Finished episode: 580 Reward: -1319.2361 total_loss = 12.4296 = -0.0450 + 0.5 * 24.9540 + 0.01 * -0.2322\n",
            "-----------------\n",
            "Finished episode: 581 Reward: -1342.5651 total_loss = 13.8802 = -0.0349 + 0.5 * 27.8350 + 0.01 * -0.2407\n",
            "-----------------\n",
            "Finished episode: 582 Reward: -1334.7774 total_loss = 14.7428 = -0.0422 + 0.5 * 29.5750 + 0.01 * -0.2460\n",
            "-----------------\n",
            "Finished episode: 583 Reward: -1330.6121 total_loss = 13.7259 = 0.0684 + 0.5 * 27.3197 + 0.01 * -0.2376\n",
            "-----------------\n",
            "Finished episode: 584 Reward: -1342.1395 total_loss = 15.3814 = -0.0655 + 0.5 * 30.8987 + 0.01 * -0.2476\n",
            "-----------------\n",
            "Finished episode: 585 Reward: -1330.1655 total_loss = 15.5467 = -0.0834 + 0.5 * 31.2650 + 0.01 * -0.2400\n",
            "-----------------\n",
            "Finished episode: 586 Reward: -1329.0469 total_loss = 13.0264 = 0.0163 + 0.5 * 26.0248 + 0.01 * -0.2381\n",
            "-----------------\n",
            "Finished episode: 587 Reward: -1317.3957 total_loss = 12.8909 = -0.0752 + 0.5 * 25.9368 + 0.01 * -0.2323\n",
            "-----------------\n",
            "Finished episode: 588 Reward: -1321.0942 total_loss = 13.9722 = -0.0304 + 0.5 * 28.0101 + 0.01 * -0.2517\n",
            "-----------------\n",
            "Finished episode: 589 Reward: -1318.1728 total_loss = 14.9084 = 0.0603 + 0.5 * 29.7010 + 0.01 * -0.2411\n",
            "-----------------\n",
            "Finished episode: 590 Reward: -1332.0939 total_loss = 14.4718 = 0.0579 + 0.5 * 28.8326 + 0.01 * -0.2458\n",
            "-----------------\n",
            "Finished episode: 591 Reward: -1324.0000 total_loss = 14.7247 = 0.0296 + 0.5 * 29.3950 + 0.01 * -0.2393\n",
            "-----------------\n",
            "Finished episode: 592 Reward: -1332.5605 total_loss = 15.0323 = -0.0590 + 0.5 * 30.1874 + 0.01 * -0.2426\n",
            "-----------------\n",
            "Finished episode: 593 Reward: -1329.3452 total_loss = 15.5151 = 0.0860 + 0.5 * 30.8634 + 0.01 * -0.2540\n",
            "-----------------\n",
            "Finished episode: 594 Reward: -1336.3891 total_loss = 15.0088 = -0.0348 + 0.5 * 30.0923 + 0.01 * -0.2487\n",
            "-----------------\n",
            "Finished episode: 595 Reward: -1338.6867 total_loss = 15.2927 = -0.0787 + 0.5 * 30.7481 + 0.01 * -0.2605\n",
            "-----------------\n",
            "Finished episode: 596 Reward: -1325.0622 total_loss = 14.7970 = 0.0423 + 0.5 * 29.5144 + 0.01 * -0.2531\n",
            "-----------------\n",
            "Finished episode: 597 Reward: -1336.0158 total_loss = 13.3443 = 0.0148 + 0.5 * 26.6642 + 0.01 * -0.2588\n",
            "-----------------\n",
            "Finished episode: 598 Reward: -1331.0600 total_loss = 13.5023 = -0.0581 + 0.5 * 27.1259 + 0.01 * -0.2578\n",
            "-----------------\n",
            "Finished episode: 599 Reward: -1333.1403 total_loss = 14.8272 = 0.0890 + 0.5 * 29.4815 + 0.01 * -0.2568\n",
            "-----------------\n",
            "Finished episode: 600 Reward: -1333.8477 total_loss = 16.4308 = -0.1158 + 0.5 * 33.0984 + 0.01 * -0.2583\n",
            "-----------------\n",
            "Finished episode: 601 Reward: -1323.6269 total_loss = 12.2360 = -0.0756 + 0.5 * 24.6283 + 0.01 * -0.2538\n",
            "-----------------\n",
            "Finished episode: 602 Reward: -1328.1343 total_loss = 14.4363 = -0.0135 + 0.5 * 28.9047 + 0.01 * -0.2514\n",
            "-----------------\n",
            "Finished episode: 603 Reward: -1314.4029 total_loss = 11.8724 = 0.0172 + 0.5 * 23.7154 + 0.01 * -0.2564\n",
            "-----------------\n",
            "Finished episode: 604 Reward: -1335.4494 total_loss = 16.6654 = -0.0007 + 0.5 * 33.3373 + 0.01 * -0.2525\n",
            "-----------------\n",
            "Finished episode: 605 Reward: -1321.2869 total_loss = 13.2242 = 0.0609 + 0.5 * 26.3315 + 0.01 * -0.2462\n",
            "-----------------\n",
            "Finished episode: 606 Reward: -1322.9921 total_loss = 12.1583 = -0.0296 + 0.5 * 24.3807 + 0.01 * -0.2441\n",
            "-----------------\n",
            "Finished episode: 607 Reward: -1330.2411 total_loss = 13.3753 = -0.0259 + 0.5 * 26.8074 + 0.01 * -0.2490\n",
            "-----------------\n",
            "Finished episode: 608 Reward: -1327.4225 total_loss = 16.0482 = -0.0584 + 0.5 * 32.2181 + 0.01 * -0.2500\n",
            "-----------------\n",
            "Finished episode: 609 Reward: -1330.6285 total_loss = 13.6691 = -0.1270 + 0.5 * 27.5974 + 0.01 * -0.2544\n",
            "-----------------\n",
            "Finished episode: 610 Reward: -1325.0108 total_loss = 12.3500 = -0.0299 + 0.5 * 24.7650 + 0.01 * -0.2584\n",
            "-----------------\n",
            "Finished episode: 611 Reward: -1336.5223 total_loss = 14.3013 = 0.0722 + 0.5 * 28.4633 + 0.01 * -0.2616\n",
            "-----------------\n",
            "Finished episode: 612 Reward: -1314.8606 total_loss = 14.0536 = 0.0740 + 0.5 * 27.9644 + 0.01 * -0.2568\n",
            "-----------------\n",
            "Finished episode: 613 Reward: -1338.0874 total_loss = 16.2032 = 0.0005 + 0.5 * 32.4105 + 0.01 * -0.2542\n",
            "-----------------\n",
            "Finished episode: 614 Reward: -1335.3530 total_loss = 14.5597 = -0.0699 + 0.5 * 29.2644 + 0.01 * -0.2644\n",
            "-----------------\n",
            "Finished episode: 615 Reward: -1343.6935 total_loss = 16.3761 = -0.0350 + 0.5 * 32.8275 + 0.01 * -0.2587\n",
            "-----------------\n",
            "Finished episode: 616 Reward: -1340.1444 total_loss = 13.4103 = 0.0338 + 0.5 * 26.7583 + 0.01 * -0.2647\n",
            "-----------------\n",
            "Finished episode: 617 Reward: -1327.2896 total_loss = 12.9758 = 0.0170 + 0.5 * 25.9228 + 0.01 * -0.2589\n",
            "-----------------\n",
            "Finished episode: 618 Reward: -1334.3540 total_loss = 13.7844 = -0.0405 + 0.5 * 27.6549 + 0.01 * -0.2543\n",
            "-----------------\n",
            "Finished episode: 620 Reward: -1323.6767 total_loss = 14.5501 = 0.0532 + 0.5 * 28.9986 + 0.01 * -0.2409\n",
            "-----------------\n",
            "Finished episode: 621 Reward: -1322.5822 total_loss = 14.2942 = 0.0704 + 0.5 * 28.4526 + 0.01 * -0.2527\n",
            "-----------------\n",
            "Finished episode: 622 Reward: -1324.8690 total_loss = 14.1612 = -0.0110 + 0.5 * 28.3496 + 0.01 * -0.2650\n",
            "-----------------\n",
            "Finished episode: 623 Reward: -1328.1169 total_loss = 14.2541 = -0.1005 + 0.5 * 28.7142 + 0.01 * -0.2491\n",
            "-----------------\n",
            "Finished episode: 624 Reward: -1338.4094 total_loss = 13.5095 = 0.0450 + 0.5 * 26.9339 + 0.01 * -0.2454\n",
            "-----------------\n",
            "Finished episode: 625 Reward: -1333.6018 total_loss = 13.6234 = 0.0654 + 0.5 * 27.1208 + 0.01 * -0.2465\n",
            "-----------------\n",
            "Finished episode: 626 Reward: -1337.0725 total_loss = 12.7718 = 0.0532 + 0.5 * 25.4422 + 0.01 * -0.2500\n",
            "-----------------\n",
            "Finished episode: 627 Reward: -1341.6552 total_loss = 13.8815 = -0.0615 + 0.5 * 27.8910 + 0.01 * -0.2504\n",
            "-----------------\n",
            "Finished episode: 628 Reward: -1316.9854 total_loss = 14.8487 = 0.0916 + 0.5 * 29.5192 + 0.01 * -0.2493\n",
            "-----------------\n",
            "Finished episode: 629 Reward: -1344.6751 total_loss = 15.6954 = 0.0313 + 0.5 * 31.3334 + 0.01 * -0.2520\n",
            "-----------------\n",
            "Finished episode: 630 Reward: -1336.5186 total_loss = 14.1296 = -0.0464 + 0.5 * 28.3568 + 0.01 * -0.2457\n",
            "-----------------\n",
            "Finished episode: 631 Reward: -1322.9561 total_loss = 14.5630 = -0.1154 + 0.5 * 29.3618 + 0.01 * -0.2452\n",
            "-----------------\n",
            "Finished episode: 632 Reward: -1329.9974 total_loss = 14.3828 = 0.0030 + 0.5 * 28.7645 + 0.01 * -0.2490\n",
            "-----------------\n",
            "Finished episode: 633 Reward: -1334.2842 total_loss = 13.0305 = -0.0709 + 0.5 * 26.2077 + 0.01 * -0.2465\n",
            "-----------------\n",
            "Finished episode: 634 Reward: -1321.7303 total_loss = 13.3225 = 0.0210 + 0.5 * 26.6080 + 0.01 * -0.2444\n",
            "-----------------\n",
            "Finished episode: 635 Reward: -1336.8672 total_loss = 14.2423 = 0.0286 + 0.5 * 28.4320 + 0.01 * -0.2295\n",
            "-----------------\n",
            "Finished episode: 636 Reward: -1341.1110 total_loss = 16.1671 = -0.0526 + 0.5 * 32.4440 + 0.01 * -0.2316\n",
            "-----------------\n",
            "Finished episode: 637 Reward: -1325.1766 total_loss = 14.0647 = -0.0328 + 0.5 * 28.1999 + 0.01 * -0.2502\n",
            "-----------------\n",
            "Finished episode: 638 Reward: -1303.1851 total_loss = 13.0891 = 0.0350 + 0.5 * 26.1130 + 0.01 * -0.2398\n",
            "-----------------\n",
            "Finished episode: 639 Reward: -1330.4010 total_loss = 12.7836 = -0.0249 + 0.5 * 25.6218 + 0.01 * -0.2443\n",
            "-----------------\n",
            "Finished episode: 640 Reward: -1326.3145 total_loss = 14.4809 = 0.0355 + 0.5 * 28.8955 + 0.01 * -0.2347\n",
            "-----------------\n",
            "Finished episode: 641 Reward: -1328.0694 total_loss = 13.4821 = -0.0289 + 0.5 * 27.0269 + 0.01 * -0.2428\n",
            "-----------------\n",
            "Finished episode: 642 Reward: -1328.3378 total_loss = 13.2933 = 0.0901 + 0.5 * 26.4110 + 0.01 * -0.2351\n",
            "-----------------\n",
            "Finished episode: 643 Reward: -1322.9369 total_loss = 14.0182 = -0.0437 + 0.5 * 28.1286 + 0.01 * -0.2419\n",
            "-----------------\n",
            "Finished episode: 644 Reward: -1318.2777 total_loss = 14.0640 = -0.0991 + 0.5 * 28.3308 + 0.01 * -0.2283\n",
            "-----------------\n",
            "Finished episode: 645 Reward: -1333.9692 total_loss = 14.5870 = 0.0035 + 0.5 * 29.1718 + 0.01 * -0.2373\n",
            "-----------------\n",
            "Finished episode: 646 Reward: -1334.2188 total_loss = 13.7371 = 0.0530 + 0.5 * 27.3728 + 0.01 * -0.2368\n",
            "-----------------\n",
            "Finished episode: 647 Reward: -1328.2745 total_loss = 12.2521 = -0.0106 + 0.5 * 24.5296 + 0.01 * -0.2191\n",
            "-----------------\n",
            "Finished episode: 648 Reward: -1335.6370 total_loss = 13.4501 = 0.0439 + 0.5 * 26.8168 + 0.01 * -0.2243\n",
            "-----------------\n",
            "Finished episode: 649 Reward: -1342.2233 total_loss = 15.7289 = -0.0386 + 0.5 * 31.5395 + 0.01 * -0.2244\n",
            "-----------------\n",
            "Finished episode: 650 Reward: -1336.1101 total_loss = 14.3802 = -0.0439 + 0.5 * 28.8528 + 0.01 * -0.2280\n",
            "-----------------\n",
            "Finished episode: 651 Reward: -1323.5597 total_loss = 16.1087 = 0.0829 + 0.5 * 32.0562 + 0.01 * -0.2325\n",
            "-----------------\n",
            "Finished episode: 652 Reward: -1331.0229 total_loss = 14.5488 = -0.0249 + 0.5 * 29.1523 + 0.01 * -0.2411\n",
            "-----------------\n",
            "Finished episode: 653 Reward: -1325.1019 total_loss = 14.4860 = 0.0612 + 0.5 * 28.8542 + 0.01 * -0.2271\n",
            "-----------------\n",
            "Finished episode: 654 Reward: -1328.6095 total_loss = 14.9326 = -0.0069 + 0.5 * 29.8837 + 0.01 * -0.2387\n",
            "-----------------\n",
            "Finished episode: 655 Reward: -1327.1881 total_loss = 15.7301 = 0.0167 + 0.5 * 31.4315 + 0.01 * -0.2359\n",
            "-----------------\n",
            "Finished episode: 656 Reward: -1346.7969 total_loss = 16.0837 = 0.0501 + 0.5 * 32.0720 + 0.01 * -0.2422\n",
            "-----------------\n",
            "Finished episode: 657 Reward: -1324.9030 total_loss = 13.7435 = -0.0378 + 0.5 * 27.5672 + 0.01 * -0.2348\n",
            "-----------------\n",
            "Finished episode: 658 Reward: -1327.7177 total_loss = 13.3883 = 0.0013 + 0.5 * 26.7787 + 0.01 * -0.2268\n",
            "-----------------\n",
            "Finished episode: 659 Reward: -1320.3832 total_loss = 14.4170 = 0.0837 + 0.5 * 28.6714 + 0.01 * -0.2382\n",
            "-----------------\n",
            "Finished episode: 660 Reward: -1335.9125 total_loss = 14.2394 = -0.0168 + 0.5 * 28.5172 + 0.01 * -0.2449\n",
            "-----------------\n",
            "Finished episode: 661 Reward: -1321.3052 total_loss = 13.3109 = 0.0075 + 0.5 * 26.6114 + 0.01 * -0.2343\n",
            "-----------------\n",
            "Finished episode: 662 Reward: -1331.2468 total_loss = 15.0576 = 0.2038 + 0.5 * 29.7125 + 0.01 * -0.2432\n",
            "-----------------\n",
            "Finished episode: 663 Reward: -1333.4567 total_loss = 13.2754 = 0.0153 + 0.5 * 26.5248 + 0.01 * -0.2320\n",
            "-----------------\n",
            "Finished episode: 664 Reward: -1333.8912 total_loss = 13.6157 = 0.0209 + 0.5 * 27.1945 + 0.01 * -0.2406\n",
            "-----------------\n",
            "Finished episode: 665 Reward: -1332.2177 total_loss = 13.9639 = 0.0017 + 0.5 * 27.9292 + 0.01 * -0.2373\n",
            "-----------------\n",
            "Finished episode: 666 Reward: -1338.1829 total_loss = 13.8443 = 0.0504 + 0.5 * 27.5928 + 0.01 * -0.2406\n",
            "-----------------\n",
            "Finished episode: 667 Reward: -1346.7069 total_loss = 16.9568 = -0.0472 + 0.5 * 34.0127 + 0.01 * -0.2324\n",
            "-----------------\n",
            "Finished episode: 668 Reward: -1333.0975 total_loss = 16.1284 = 0.1053 + 0.5 * 32.0509 + 0.01 * -0.2402\n",
            "-----------------\n",
            "Finished episode: 669 Reward: -1298.2249 total_loss = 16.5169 = 0.1304 + 0.5 * 32.7775 + 0.01 * -0.2343\n",
            "-----------------\n",
            "Finished episode: 670 Reward: -1332.2196 total_loss = 12.2365 = 0.0147 + 0.5 * 24.4482 + 0.01 * -0.2283\n",
            "-----------------\n",
            "Finished episode: 671 Reward: -1329.8311 total_loss = 14.7352 = -0.0368 + 0.5 * 29.5487 + 0.01 * -0.2296\n",
            "-----------------\n",
            "Finished episode: 672 Reward: -1319.6428 total_loss = 12.0718 = 0.0199 + 0.5 * 24.1088 + 0.01 * -0.2475\n",
            "-----------------\n",
            "Finished episode: 673 Reward: -1325.1674 total_loss = 13.7545 = -0.1268 + 0.5 * 27.7673 + 0.01 * -0.2331\n",
            "-----------------\n",
            "Finished episode: 674 Reward: -1324.3513 total_loss = 13.7915 = 0.0585 + 0.5 * 27.4706 + 0.01 * -0.2336\n",
            "-----------------\n",
            "Finished episode: 675 Reward: -1333.7928 total_loss = 15.5568 = 0.0197 + 0.5 * 31.0791 + 0.01 * -0.2385\n",
            "-----------------\n",
            "Finished episode: 676 Reward: -1323.1619 total_loss = 14.6186 = 0.0382 + 0.5 * 29.1657 + 0.01 * -0.2398\n",
            "-----------------\n",
            "Finished episode: 677 Reward: -1329.7747 total_loss = 14.6973 = -0.0276 + 0.5 * 29.4545 + 0.01 * -0.2346\n",
            "-----------------\n",
            "Finished episode: 678 Reward: -1349.9115 total_loss = 16.4848 = 0.0172 + 0.5 * 32.9397 + 0.01 * -0.2247\n",
            "-----------------\n",
            "Finished episode: 679 Reward: -1336.2830 total_loss = 14.0722 = -0.0005 + 0.5 * 28.1500 + 0.01 * -0.2334\n",
            "-----------------\n",
            "Finished episode: 680 Reward: -1335.4288 total_loss = 15.1457 = -0.0067 + 0.5 * 30.3093 + 0.01 * -0.2340\n",
            "-----------------\n",
            "Finished episode: 681 Reward: -1335.8906 total_loss = 14.4985 = -0.0121 + 0.5 * 29.0259 + 0.01 * -0.2319\n",
            "-----------------\n",
            "Finished episode: 682 Reward: -1337.0769 total_loss = 14.1056 = -0.0287 + 0.5 * 28.2733 + 0.01 * -0.2306\n",
            "-----------------\n",
            "Finished episode: 683 Reward: -1341.8227 total_loss = 14.4664 = -0.0593 + 0.5 * 29.0562 + 0.01 * -0.2455\n",
            "-----------------\n",
            "Finished episode: 684 Reward: -1349.9293 total_loss = 14.0651 = -0.0029 + 0.5 * 28.1409 + 0.01 * -0.2448\n",
            "-----------------\n",
            "Finished episode: 685 Reward: -1337.4385 total_loss = 13.9203 = 0.0253 + 0.5 * 27.7947 + 0.01 * -0.2380\n",
            "-----------------\n",
            "Finished episode: 686 Reward: -1342.5985 total_loss = 15.4013 = 0.0082 + 0.5 * 30.7911 + 0.01 * -0.2433\n",
            "-----------------\n",
            "Finished episode: 687 Reward: -1340.1947 total_loss = 14.7615 = 0.0061 + 0.5 * 29.5157 + 0.01 * -0.2478\n",
            "-----------------\n",
            "Finished episode: 688 Reward: -1343.2913 total_loss = 13.8251 = 0.0040 + 0.5 * 27.6469 + 0.01 * -0.2389\n",
            "-----------------\n",
            "Finished episode: 689 Reward: -1316.3263 total_loss = 14.4520 = -0.1160 + 0.5 * 29.1410 + 0.01 * -0.2453\n",
            "-----------------\n",
            "Finished episode: 690 Reward: -1333.0754 total_loss = 15.0864 = -0.0338 + 0.5 * 30.2455 + 0.01 * -0.2481\n",
            "-----------------\n",
            "Finished episode: 691 Reward: -1344.3654 total_loss = 14.5492 = 0.0037 + 0.5 * 29.0961 + 0.01 * -0.2503\n",
            "-----------------\n",
            "Finished episode: 692 Reward: -1332.6349 total_loss = 16.0124 = 0.0094 + 0.5 * 32.0108 + 0.01 * -0.2410\n",
            "-----------------\n",
            "Finished episode: 693 Reward: -1333.5069 total_loss = 14.7720 = 0.0752 + 0.5 * 29.3985 + 0.01 * -0.2471\n",
            "-----------------\n",
            "Finished episode: 694 Reward: -1337.8345 total_loss = 16.2098 = -0.0090 + 0.5 * 32.4425 + 0.01 * -0.2419\n",
            "-----------------\n",
            "Finished episode: 695 Reward: -1318.2289 total_loss = 13.3128 = 0.0237 + 0.5 * 26.5831 + 0.01 * -0.2481\n",
            "-----------------\n",
            "Finished episode: 696 Reward: -1334.2087 total_loss = 16.0417 = 0.1356 + 0.5 * 31.8171 + 0.01 * -0.2457\n",
            "-----------------\n",
            "Finished episode: 697 Reward: -1336.2040 total_loss = 16.0192 = -0.0200 + 0.5 * 32.0833 + 0.01 * -0.2496\n",
            "-----------------\n",
            "Finished episode: 698 Reward: -1333.8018 total_loss = 14.9201 = 0.0376 + 0.5 * 29.7701 + 0.01 * -0.2494\n",
            "-----------------\n",
            "Finished episode: 699 Reward: -1343.3427 total_loss = 13.7068 = -0.0650 + 0.5 * 27.5486 + 0.01 * -0.2481\n",
            "-----------------\n",
            "Finished episode: 700 Reward: -1327.5096 total_loss = 14.1382 = 0.0563 + 0.5 * 28.1687 + 0.01 * -0.2398\n",
            "-----------------\n",
            "Finished episode: 701 Reward: -1342.9657 total_loss = 14.1406 = 0.0064 + 0.5 * 28.2732 + 0.01 * -0.2451\n",
            "-----------------\n",
            "Finished episode: 702 Reward: -1330.6693 total_loss = 14.6929 = -0.0384 + 0.5 * 29.4677 + 0.01 * -0.2543\n",
            "-----------------\n",
            "Finished episode: 703 Reward: -1337.1236 total_loss = 15.5597 = -0.0523 + 0.5 * 31.2289 + 0.01 * -0.2558\n",
            "-----------------\n",
            "Finished episode: 704 Reward: -1342.8963 total_loss = 13.5432 = -0.0732 + 0.5 * 27.2378 + 0.01 * -0.2450\n",
            "-----------------\n",
            "Finished episode: 705 Reward: -1339.4130 total_loss = 14.5627 = -0.0421 + 0.5 * 29.2146 + 0.01 * -0.2539\n",
            "-----------------\n",
            "Finished episode: 706 Reward: -1326.2585 total_loss = 11.4677 = 0.0027 + 0.5 * 22.9352 + 0.01 * -0.2587\n",
            "-----------------\n",
            "Finished episode: 707 Reward: -1340.6816 total_loss = 15.4963 = -0.0596 + 0.5 * 31.1165 + 0.01 * -0.2423\n",
            "-----------------\n",
            "Finished episode: 708 Reward: -1321.6664 total_loss = 14.4121 = 0.1315 + 0.5 * 28.5662 + 0.01 * -0.2465\n",
            "-----------------\n",
            "Finished episode: 709 Reward: -1340.5387 total_loss = 13.9855 = -0.0077 + 0.5 * 27.9913 + 0.01 * -0.2468\n",
            "-----------------\n",
            "Finished episode: 710 Reward: -1324.4899 total_loss = 12.6905 = -0.0272 + 0.5 * 25.4405 + 0.01 * -0.2514\n",
            "-----------------\n",
            "Finished episode: 711 Reward: -1334.6601 total_loss = 14.9328 = -0.0253 + 0.5 * 29.9211 + 0.01 * -0.2426\n",
            "-----------------\n",
            "Finished episode: 712 Reward: -1341.1273 total_loss = 14.0717 = 0.0944 + 0.5 * 27.9596 + 0.01 * -0.2463\n",
            "-----------------\n",
            "Finished episode: 713 Reward: -1330.9872 total_loss = 12.8902 = 0.0203 + 0.5 * 25.7448 + 0.01 * -0.2490\n",
            "-----------------\n",
            "Finished episode: 714 Reward: -1336.7236 total_loss = 14.7344 = -0.0268 + 0.5 * 29.5274 + 0.01 * -0.2528\n",
            "-----------------\n",
            "Finished episode: 715 Reward: -1323.3548 total_loss = 12.7346 = -0.0336 + 0.5 * 25.5412 + 0.01 * -0.2426\n",
            "-----------------\n",
            "Finished episode: 716 Reward: -1312.8462 total_loss = 13.5719 = 0.0135 + 0.5 * 27.1217 + 0.01 * -0.2374\n",
            "-----------------\n",
            "Finished episode: 717 Reward: -1331.0317 total_loss = 15.2158 = -0.0930 + 0.5 * 30.6227 + 0.01 * -0.2473\n",
            "-----------------\n",
            "Finished episode: 718 Reward: -1330.0146 total_loss = 13.5674 = -0.0172 + 0.5 * 27.1740 + 0.01 * -0.2411\n",
            "-----------------\n",
            "Finished episode: 719 Reward: -1330.9321 total_loss = 14.3837 = -0.0436 + 0.5 * 28.8594 + 0.01 * -0.2479\n",
            "-----------------\n",
            "Finished episode: 720 Reward: -1331.6440 total_loss = 15.6208 = 0.0119 + 0.5 * 31.2228 + 0.01 * -0.2482\n",
            "-----------------\n",
            "Finished episode: 721 Reward: -1338.7724 total_loss = 16.0621 = 0.0149 + 0.5 * 32.0994 + 0.01 * -0.2490\n",
            "-----------------\n",
            "Finished episode: 722 Reward: -1340.3375 total_loss = 15.2174 = -0.0557 + 0.5 * 30.5509 + 0.01 * -0.2356\n",
            "-----------------\n",
            "Finished episode: 723 Reward: -1334.8301 total_loss = 15.4857 = 0.1368 + 0.5 * 30.7025 + 0.01 * -0.2357\n",
            "-----------------\n",
            "Finished episode: 724 Reward: -1329.9603 total_loss = 14.2128 = 0.0298 + 0.5 * 28.3709 + 0.01 * -0.2424\n",
            "-----------------\n",
            "Finished episode: 725 Reward: -1335.9426 total_loss = 13.7621 = 0.0405 + 0.5 * 27.4481 + 0.01 * -0.2460\n",
            "-----------------\n",
            "Finished episode: 726 Reward: -1332.6629 total_loss = 14.5442 = 0.0064 + 0.5 * 29.0804 + 0.01 * -0.2451\n",
            "-----------------\n",
            "Finished episode: 727 Reward: -1333.5164 total_loss = 11.7062 = 0.0067 + 0.5 * 23.4038 + 0.01 * -0.2442\n",
            "-----------------\n",
            "Finished episode: 728 Reward: -1343.3431 total_loss = 14.3488 = -0.0940 + 0.5 * 28.8906 + 0.01 * -0.2424\n",
            "-----------------\n",
            "Finished episode: 729 Reward: -1335.5430 total_loss = 13.8779 = -0.0119 + 0.5 * 27.7847 + 0.01 * -0.2534\n",
            "-----------------\n",
            "Finished episode: 730 Reward: -1338.7622 total_loss = 13.8782 = 0.0361 + 0.5 * 27.6892 + 0.01 * -0.2496\n",
            "-----------------\n",
            "Finished episode: 731 Reward: -1342.0369 total_loss = 15.0998 = -0.0896 + 0.5 * 30.3836 + 0.01 * -0.2432\n",
            "-----------------\n",
            "Finished episode: 732 Reward: -1338.3600 total_loss = 13.6947 = -0.0168 + 0.5 * 27.4280 + 0.01 * -0.2474\n",
            "-----------------\n",
            "Finished episode: 733 Reward: -1334.6807 total_loss = 14.3787 = 0.0310 + 0.5 * 28.7005 + 0.01 * -0.2474\n",
            "-----------------\n",
            "Finished episode: 734 Reward: -1329.7983 total_loss = 15.4781 = 0.0930 + 0.5 * 30.7751 + 0.01 * -0.2441\n",
            "-----------------\n",
            "Finished episode: 735 Reward: -1335.1520 total_loss = 14.0043 = -0.0002 + 0.5 * 28.0142 + 0.01 * -0.2560\n",
            "-----------------\n",
            "Finished episode: 736 Reward: -1323.1683 total_loss = 15.8676 = -0.0953 + 0.5 * 31.9307 + 0.01 * -0.2425\n",
            "-----------------\n",
            "Finished episode: 737 Reward: -1332.0362 total_loss = 15.2033 = -0.0561 + 0.5 * 30.5237 + 0.01 * -0.2509\n",
            "-----------------\n",
            "Finished episode: 738 Reward: -1330.8250 total_loss = 14.4016 = 0.0415 + 0.5 * 28.7250 + 0.01 * -0.2494\n",
            "-----------------\n",
            "Finished episode: 739 Reward: -1337.9149 total_loss = 13.2352 = -0.0111 + 0.5 * 26.4976 + 0.01 * -0.2429\n",
            "-----------------\n",
            "Finished episode: 740 Reward: -1335.8796 total_loss = 13.0797 = -0.0842 + 0.5 * 26.3323 + 0.01 * -0.2330\n",
            "-----------------\n",
            "Finished episode: 741 Reward: -1339.6606 total_loss = 14.5956 = -0.0576 + 0.5 * 29.3112 + 0.01 * -0.2399\n",
            "-----------------\n",
            "Finished episode: 742 Reward: -1339.1587 total_loss = 13.3412 = -0.0521 + 0.5 * 26.7914 + 0.01 * -0.2456\n",
            "-----------------\n",
            "Finished episode: 743 Reward: -1332.8019 total_loss = 16.3214 = 0.0030 + 0.5 * 32.6416 + 0.01 * -0.2441\n",
            "-----------------\n",
            "Finished episode: 744 Reward: -1335.1125 total_loss = 14.7447 = -0.0467 + 0.5 * 29.5877 + 0.01 * -0.2386\n",
            "-----------------\n",
            "Finished episode: 745 Reward: -1329.7744 total_loss = 15.2462 = -0.0701 + 0.5 * 30.6373 + 0.01 * -0.2351\n",
            "-----------------\n",
            "Finished episode: 746 Reward: -1332.6059 total_loss = 16.0720 = 0.0005 + 0.5 * 32.1477 + 0.01 * -0.2332\n",
            "-----------------\n",
            "Finished episode: 747 Reward: -1323.4228 total_loss = 14.7885 = 0.0763 + 0.5 * 29.4293 + 0.01 * -0.2430\n",
            "-----------------\n",
            "Finished episode: 748 Reward: -1331.2202 total_loss = 15.4552 = 0.0333 + 0.5 * 30.8485 + 0.01 * -0.2372\n",
            "-----------------\n",
            "Finished episode: 749 Reward: -1341.1617 total_loss = 15.9745 = -0.0199 + 0.5 * 31.9937 + 0.01 * -0.2408\n",
            "-----------------\n",
            "Finished episode: 750 Reward: -1334.4185 total_loss = 14.9405 = 0.0744 + 0.5 * 29.7369 + 0.01 * -0.2371\n",
            "-----------------\n",
            "Finished episode: 751 Reward: -1327.2350 total_loss = 13.0963 = 0.0744 + 0.5 * 26.0487 + 0.01 * -0.2449\n",
            "-----------------\n",
            "Finished episode: 752 Reward: -1340.6676 total_loss = 14.8867 = 0.0392 + 0.5 * 29.6999 + 0.01 * -0.2418\n",
            "-----------------\n",
            "Finished episode: 753 Reward: -1329.8073 total_loss = 14.7713 = 0.0209 + 0.5 * 29.5056 + 0.01 * -0.2409\n",
            "-----------------\n",
            "Finished episode: 754 Reward: -1330.9824 total_loss = 12.8125 = -0.0907 + 0.5 * 25.8114 + 0.01 * -0.2503\n",
            "-----------------\n",
            "Finished episode: 755 Reward: -1345.5360 total_loss = 14.3074 = -0.0139 + 0.5 * 28.6477 + 0.01 * -0.2532\n",
            "-----------------\n",
            "Finished episode: 756 Reward: -1336.0481 total_loss = 14.9888 = 0.0439 + 0.5 * 29.8947 + 0.01 * -0.2443\n",
            "-----------------\n",
            "Finished episode: 757 Reward: -1335.1413 total_loss = 16.2540 = 0.0627 + 0.5 * 32.3874 + 0.01 * -0.2434\n",
            "-----------------\n",
            "Finished episode: 758 Reward: -1341.8988 total_loss = 14.8837 = -0.0490 + 0.5 * 29.8702 + 0.01 * -0.2463\n",
            "-----------------\n",
            "Finished episode: 759 Reward: -1320.0886 total_loss = 14.4951 = -0.0504 + 0.5 * 29.0961 + 0.01 * -0.2449\n",
            "-----------------\n",
            "Finished episode: 760 Reward: -1331.8647 total_loss = 14.7405 = 0.0863 + 0.5 * 29.3132 + 0.01 * -0.2467\n",
            "-----------------\n",
            "Finished episode: 761 Reward: -1335.0578 total_loss = 14.8165 = 0.0929 + 0.5 * 29.4521 + 0.01 * -0.2489\n",
            "-----------------\n",
            "Finished episode: 762 Reward: -1341.0063 total_loss = 15.7508 = -0.0428 + 0.5 * 31.5922 + 0.01 * -0.2489\n",
            "-----------------\n",
            "Finished episode: 763 Reward: -1336.9811 total_loss = 13.6940 = -0.0787 + 0.5 * 27.5503 + 0.01 * -0.2469\n",
            "-----------------\n",
            "Finished episode: 764 Reward: -1313.5537 total_loss = 13.6396 = 0.0035 + 0.5 * 27.2772 + 0.01 * -0.2471\n",
            "-----------------\n",
            "Finished episode: 765 Reward: -1329.8709 total_loss = 13.9146 = -0.0667 + 0.5 * 27.9674 + 0.01 * -0.2394\n",
            "-----------------\n",
            "Finished episode: 766 Reward: -1324.7156 total_loss = 13.0262 = 0.0415 + 0.5 * 25.9746 + 0.01 * -0.2550\n",
            "-----------------\n",
            "Finished episode: 767 Reward: -1342.7195 total_loss = 14.4449 = 0.0744 + 0.5 * 28.7460 + 0.01 * -0.2495\n",
            "-----------------\n",
            "Finished episode: 768 Reward: -1327.3884 total_loss = 16.6476 = -0.0882 + 0.5 * 33.4764 + 0.01 * -0.2470\n",
            "-----------------\n",
            "Finished episode: 769 Reward: -1336.3363 total_loss = 13.2868 = 0.0285 + 0.5 * 26.5215 + 0.01 * -0.2469\n",
            "-----------------\n",
            "Finished episode: 770 Reward: -1323.6060 total_loss = 13.3795 = -0.1576 + 0.5 * 27.0793 + 0.01 * -0.2523\n",
            "-----------------\n",
            "Finished episode: 771 Reward: -1332.3485 total_loss = 13.9464 = -0.0244 + 0.5 * 27.9466 + 0.01 * -0.2453\n",
            "-----------------\n",
            "Finished episode: 772 Reward: -1336.4513 total_loss = 14.8524 = -0.0335 + 0.5 * 29.7768 + 0.01 * -0.2549\n",
            "-----------------\n",
            "Finished episode: 773 Reward: -1336.2842 total_loss = 12.5856 = -0.0015 + 0.5 * 25.1790 + 0.01 * -0.2382\n",
            "-----------------\n",
            "Finished episode: 774 Reward: -1337.2644 total_loss = 15.4995 = -0.0841 + 0.5 * 31.1721 + 0.01 * -0.2464\n",
            "-----------------\n",
            "Finished episode: 775 Reward: -1335.3791 total_loss = 13.5749 = -0.0611 + 0.5 * 27.2769 + 0.01 * -0.2520\n",
            "-----------------\n",
            "Finished episode: 776 Reward: -1330.8579 total_loss = 12.0361 = -0.0632 + 0.5 * 24.2037 + 0.01 * -0.2492\n",
            "-----------------\n",
            "Finished episode: 777 Reward: -1329.7742 total_loss = 13.3968 = -0.0348 + 0.5 * 26.8680 + 0.01 * -0.2317\n",
            "-----------------\n",
            "Finished episode: 778 Reward: -1318.6304 total_loss = 14.8264 = 0.0228 + 0.5 * 29.6122 + 0.01 * -0.2485\n",
            "-----------------\n",
            "Finished episode: 779 Reward: -1320.1298 total_loss = 13.3589 = -0.0741 + 0.5 * 26.8709 + 0.01 * -0.2471\n",
            "-----------------\n",
            "Finished episode: 780 Reward: -1340.5231 total_loss = 14.9752 = 0.0381 + 0.5 * 29.8789 + 0.01 * -0.2375\n",
            "-----------------\n",
            "Finished episode: 781 Reward: -1333.3044 total_loss = 16.0509 = -0.0642 + 0.5 * 32.2351 + 0.01 * -0.2407\n",
            "-----------------\n",
            "Finished episode: 782 Reward: -1336.3401 total_loss = 15.1939 = 0.0194 + 0.5 * 30.3540 + 0.01 * -0.2438\n",
            "-----------------\n",
            "Finished episode: 783 Reward: -1328.1186 total_loss = 13.4011 = -0.0007 + 0.5 * 26.8082 + 0.01 * -0.2329\n",
            "-----------------\n",
            "Finished episode: 784 Reward: -1340.2975 total_loss = 13.3997 = -0.0062 + 0.5 * 26.8165 + 0.01 * -0.2351\n",
            "-----------------\n",
            "Finished episode: 785 Reward: -1337.0146 total_loss = 14.3428 = 0.0392 + 0.5 * 28.6121 + 0.01 * -0.2476\n",
            "-----------------\n",
            "Finished episode: 786 Reward: -1342.5231 total_loss = 16.2953 = 0.0580 + 0.5 * 32.4795 + 0.01 * -0.2413\n",
            "-----------------\n",
            "Finished episode: 787 Reward: -1339.5023 total_loss = 14.5012 = 0.0144 + 0.5 * 28.9786 + 0.01 * -0.2483\n",
            "-----------------\n",
            "Finished episode: 788 Reward: -1332.0472 total_loss = 14.1561 = 0.1389 + 0.5 * 28.0394 + 0.01 * -0.2484\n",
            "-----------------\n",
            "Finished episode: 789 Reward: -1335.5592 total_loss = 15.4298 = -0.0750 + 0.5 * 31.0149 + 0.01 * -0.2569\n",
            "-----------------\n",
            "Finished episode: 790 Reward: -1320.6232 total_loss = 12.3224 = -0.0274 + 0.5 * 24.7046 + 0.01 * -0.2535\n",
            "-----------------\n",
            "Finished episode: 791 Reward: -1334.2082 total_loss = 14.1706 = 0.0814 + 0.5 * 28.1833 + 0.01 * -0.2432\n",
            "-----------------\n",
            "Finished episode: 792 Reward: -1327.7258 total_loss = 15.1154 = 0.1538 + 0.5 * 29.9281 + 0.01 * -0.2437\n",
            "-----------------\n",
            "Finished episode: 793 Reward: -1329.4361 total_loss = 13.0987 = 0.0098 + 0.5 * 26.1828 + 0.01 * -0.2471\n",
            "-----------------\n",
            "Finished episode: 794 Reward: -1327.5608 total_loss = 13.5798 = -0.0086 + 0.5 * 27.1816 + 0.01 * -0.2398\n",
            "-----------------\n",
            "Finished episode: 795 Reward: -1331.5795 total_loss = 16.9873 = 0.0631 + 0.5 * 33.8535 + 0.01 * -0.2509\n",
            "-----------------\n",
            "Finished episode: 796 Reward: -1345.6137 total_loss = 15.2935 = 0.0207 + 0.5 * 30.5508 + 0.01 * -0.2514\n",
            "-----------------\n",
            "Finished episode: 797 Reward: -1334.3933 total_loss = 15.8672 = 0.0201 + 0.5 * 31.6992 + 0.01 * -0.2450\n",
            "-----------------\n",
            "Finished episode: 798 Reward: -1327.8728 total_loss = 13.0004 = 0.0126 + 0.5 * 25.9806 + 0.01 * -0.2511\n",
            "-----------------\n",
            "Finished episode: 799 Reward: -1332.5305 total_loss = 13.6160 = -0.0123 + 0.5 * 27.2615 + 0.01 * -0.2417\n",
            "-----------------\n",
            "Finished episode: 800 Reward: -1326.7484 total_loss = 13.9824 = -0.0161 + 0.5 * 28.0019 + 0.01 * -0.2459\n",
            "-----------------\n",
            "Finished episode: 801 Reward: -1336.3494 total_loss = 15.7608 = -0.0003 + 0.5 * 31.5270 + 0.01 * -0.2365\n",
            "-----------------\n",
            "Finished episode: 802 Reward: -1328.2489 total_loss = 14.9162 = 0.0019 + 0.5 * 29.8334 + 0.01 * -0.2402\n",
            "-----------------\n",
            "Finished episode: 803 Reward: -1346.6438 total_loss = 14.8007 = -0.0264 + 0.5 * 29.6591 + 0.01 * -0.2409\n",
            "-----------------\n",
            "Finished episode: 804 Reward: -1325.3847 total_loss = 14.4918 = -0.0560 + 0.5 * 29.1007 + 0.01 * -0.2521\n",
            "-----------------\n",
            "Finished episode: 805 Reward: -1334.8572 total_loss = 12.8966 = -0.0016 + 0.5 * 25.8012 + 0.01 * -0.2436\n",
            "-----------------\n",
            "Finished episode: 806 Reward: -1337.6659 total_loss = 14.8699 = 0.0381 + 0.5 * 29.6685 + 0.01 * -0.2483\n",
            "-----------------\n",
            "Finished episode: 807 Reward: -1337.4629 total_loss = 14.2669 = 0.0458 + 0.5 * 28.4474 + 0.01 * -0.2531\n",
            "-----------------\n",
            "Finished episode: 808 Reward: -1325.0096 total_loss = 15.7674 = 0.0052 + 0.5 * 31.5296 + 0.01 * -0.2533\n",
            "-----------------\n",
            "Finished episode: 809 Reward: -1333.1320 total_loss = 14.7287 = -0.0577 + 0.5 * 29.5778 + 0.01 * -0.2450\n",
            "-----------------\n",
            "Finished episode: 810 Reward: -1327.4596 total_loss = 15.0610 = 0.0206 + 0.5 * 30.0858 + 0.01 * -0.2461\n",
            "-----------------\n",
            "Finished episode: 811 Reward: -1326.8998 total_loss = 12.4701 = 0.0225 + 0.5 * 24.9003 + 0.01 * -0.2532\n",
            "-----------------\n",
            "Finished episode: 812 Reward: -1335.8302 total_loss = 13.2128 = 0.0147 + 0.5 * 26.4011 + 0.01 * -0.2456\n",
            "-----------------\n",
            "Finished episode: 813 Reward: -1335.6187 total_loss = 14.6983 = 0.0364 + 0.5 * 29.3289 + 0.01 * -0.2498\n",
            "-----------------\n",
            "Finished episode: 814 Reward: -1327.8390 total_loss = 12.6187 = -0.0239 + 0.5 * 25.2904 + 0.01 * -0.2588\n",
            "-----------------\n",
            "Finished episode: 815 Reward: -1346.9163 total_loss = 15.2522 = -0.0198 + 0.5 * 30.5492 + 0.01 * -0.2560\n",
            "-----------------\n",
            "Finished episode: 816 Reward: -1332.1035 total_loss = 13.2388 = 0.0874 + 0.5 * 26.3080 + 0.01 * -0.2585\n",
            "-----------------\n",
            "Finished episode: 817 Reward: -1320.2337 total_loss = 12.8181 = -0.0371 + 0.5 * 25.7157 + 0.01 * -0.2582\n",
            "-----------------\n",
            "Finished episode: 818 Reward: -1332.2291 total_loss = 13.8480 = 0.0337 + 0.5 * 27.6336 + 0.01 * -0.2456\n",
            "-----------------\n",
            "Finished episode: 819 Reward: -1324.6476 total_loss = 12.9643 = -0.0153 + 0.5 * 25.9642 + 0.01 * -0.2518\n",
            "-----------------\n",
            "Finished episode: 820 Reward: -1335.5379 total_loss = 17.0408 = -0.1284 + 0.5 * 34.3436 + 0.01 * -0.2557\n",
            "-----------------\n",
            "Finished episode: 821 Reward: -1332.5972 total_loss = 14.6440 = 0.0060 + 0.5 * 29.2809 + 0.01 * -0.2493\n",
            "-----------------\n",
            "Finished episode: 822 Reward: -1323.5372 total_loss = 13.4613 = -0.0545 + 0.5 * 27.0366 + 0.01 * -0.2514\n",
            "-----------------\n",
            "Finished episode: 823 Reward: -1339.6319 total_loss = 16.7510 = -0.1399 + 0.5 * 33.7868 + 0.01 * -0.2441\n",
            "-----------------\n",
            "Finished episode: 824 Reward: -1324.5997 total_loss = 14.2356 = -0.0142 + 0.5 * 28.5045 + 0.01 * -0.2469\n",
            "-----------------\n",
            "Finished episode: 825 Reward: -1333.5670 total_loss = 15.6926 = 0.1363 + 0.5 * 31.1178 + 0.01 * -0.2557\n",
            "-----------------\n",
            "Finished episode: 826 Reward: -1339.2547 total_loss = 14.1438 = 0.0176 + 0.5 * 28.2574 + 0.01 * -0.2563\n",
            "-----------------\n",
            "Finished episode: 827 Reward: -1335.6056 total_loss = 13.1101 = 0.0148 + 0.5 * 26.1954 + 0.01 * -0.2367\n",
            "-----------------\n",
            "Finished episode: 828 Reward: -1330.3141 total_loss = 14.7146 = -0.0358 + 0.5 * 29.5059 + 0.01 * -0.2566\n",
            "-----------------\n",
            "Finished episode: 829 Reward: -1326.2440 total_loss = 14.8204 = 0.1631 + 0.5 * 29.3197 + 0.01 * -0.2597\n",
            "-----------------\n",
            "Finished episode: 830 Reward: -1340.9864 total_loss = 14.8990 = -0.0530 + 0.5 * 29.9091 + 0.01 * -0.2615\n",
            "-----------------\n",
            "Finished episode: 831 Reward: -1330.0307 total_loss = 13.4710 = -0.0479 + 0.5 * 27.0430 + 0.01 * -0.2523\n",
            "-----------------\n",
            "Finished episode: 832 Reward: -1337.0120 total_loss = 14.3690 = -0.0078 + 0.5 * 28.7589 + 0.01 * -0.2591\n",
            "-----------------\n",
            "Finished episode: 833 Reward: -1336.2130 total_loss = 14.1044 = 0.0612 + 0.5 * 28.0914 + 0.01 * -0.2509\n",
            "-----------------\n",
            "Finished episode: 834 Reward: -1334.9979 total_loss = 14.7851 = 0.0043 + 0.5 * 29.5670 + 0.01 * -0.2621\n",
            "-----------------\n",
            "Finished episode: 835 Reward: -1328.9005 total_loss = 13.3049 = -0.0003 + 0.5 * 26.6157 + 0.01 * -0.2616\n",
            "-----------------\n",
            "Finished episode: 836 Reward: -1334.0290 total_loss = 15.8155 = 0.0057 + 0.5 * 31.6247 + 0.01 * -0.2505\n",
            "-----------------\n",
            "Finished episode: 837 Reward: -1344.1188 total_loss = 16.9391 = -0.0089 + 0.5 * 33.9012 + 0.01 * -0.2620\n",
            "-----------------\n",
            "Finished episode: 838 Reward: -1317.1499 total_loss = 12.8004 = -0.0153 + 0.5 * 25.6366 + 0.01 * -0.2615\n",
            "-----------------\n",
            "Finished episode: 839 Reward: -1335.3966 total_loss = 13.6255 = 0.1017 + 0.5 * 27.0526 + 0.01 * -0.2593\n",
            "-----------------\n",
            "Finished episode: 840 Reward: -1324.7665 total_loss = 13.6962 = 0.0322 + 0.5 * 27.3333 + 0.01 * -0.2631\n",
            "-----------------\n",
            "Finished episode: 841 Reward: -1333.5408 total_loss = 15.2999 = 0.0623 + 0.5 * 30.4803 + 0.01 * -0.2563\n",
            "-----------------\n",
            "Finished episode: 842 Reward: -1340.0812 total_loss = 13.3788 = -0.0747 + 0.5 * 26.9123 + 0.01 * -0.2598\n",
            "-----------------\n",
            "Finished episode: 843 Reward: -1334.0822 total_loss = 13.4129 = 0.0576 + 0.5 * 26.7158 + 0.01 * -0.2539\n",
            "-----------------\n",
            "Finished episode: 844 Reward: -1338.2036 total_loss = 16.4572 = 0.1184 + 0.5 * 32.6829 + 0.01 * -0.2586\n",
            "-----------------\n",
            "Finished episode: 845 Reward: -1335.4424 total_loss = 14.1612 = 0.0154 + 0.5 * 28.2966 + 0.01 * -0.2514\n",
            "-----------------\n",
            "Finished episode: 846 Reward: -1334.3353 total_loss = 13.1474 = 0.0249 + 0.5 * 26.2499 + 0.01 * -0.2441\n",
            "-----------------\n",
            "Finished episode: 847 Reward: -1341.0146 total_loss = 15.5779 = 0.0005 + 0.5 * 31.1598 + 0.01 * -0.2542\n",
            "-----------------\n",
            "Finished episode: 848 Reward: -1331.3893 total_loss = 12.9872 = 0.0439 + 0.5 * 25.8918 + 0.01 * -0.2601\n",
            "-----------------\n",
            "Finished episode: 849 Reward: -1323.6258 total_loss = 12.9436 = -0.0159 + 0.5 * 25.9240 + 0.01 * -0.2556\n",
            "-----------------\n",
            "Finished episode: 850 Reward: -1317.0117 total_loss = 15.2620 = -0.0251 + 0.5 * 30.5795 + 0.01 * -0.2617\n",
            "-----------------\n",
            "Finished episode: 851 Reward: -1339.7230 total_loss = 14.0450 = 0.0042 + 0.5 * 28.0869 + 0.01 * -0.2652\n",
            "-----------------\n",
            "Finished episode: 852 Reward: -1330.7086 total_loss = 13.0746 = -0.0310 + 0.5 * 26.2165 + 0.01 * -0.2596\n",
            "-----------------\n",
            "Finished episode: 853 Reward: -1331.6427 total_loss = 16.4008 = 0.0117 + 0.5 * 32.7833 + 0.01 * -0.2543\n",
            "-----------------\n",
            "Finished episode: 854 Reward: -1325.8353 total_loss = 13.4509 = 0.0167 + 0.5 * 26.8734 + 0.01 * -0.2514\n",
            "-----------------\n",
            "Finished episode: 855 Reward: -1333.5361 total_loss = 14.3814 = 0.0578 + 0.5 * 28.6523 + 0.01 * -0.2591\n",
            "-----------------\n",
            "Finished episode: 856 Reward: -1329.1183 total_loss = 12.9902 = 0.0969 + 0.5 * 25.7916 + 0.01 * -0.2522\n",
            "-----------------\n",
            "Finished episode: 857 Reward: -1324.3781 total_loss = 13.5963 = 0.0426 + 0.5 * 27.1127 + 0.01 * -0.2604\n",
            "-----------------\n",
            "Finished episode: 858 Reward: -1339.6761 total_loss = 15.1217 = 0.0679 + 0.5 * 30.1128 + 0.01 * -0.2581\n",
            "-----------------\n",
            "Finished episode: 859 Reward: -1344.8134 total_loss = 14.1749 = -0.0099 + 0.5 * 28.3746 + 0.01 * -0.2479\n",
            "-----------------\n",
            "Finished episode: 860 Reward: -1335.3322 total_loss = 13.9273 = -0.0384 + 0.5 * 27.9368 + 0.01 * -0.2642\n",
            "-----------------\n",
            "Finished episode: 861 Reward: -1333.7616 total_loss = 13.1668 = -0.0601 + 0.5 * 26.4589 + 0.01 * -0.2597\n",
            "-----------------\n",
            "Finished episode: 862 Reward: -1338.1890 total_loss = 14.5654 = 0.0099 + 0.5 * 29.1161 + 0.01 * -0.2542\n",
            "-----------------\n",
            "Finished episode: 863 Reward: -1334.9560 total_loss = 13.5286 = 0.0072 + 0.5 * 27.0481 + 0.01 * -0.2628\n",
            "-----------------\n",
            "Finished episode: 864 Reward: -1334.9737 total_loss = 13.8281 = 0.0179 + 0.5 * 27.6256 + 0.01 * -0.2502\n",
            "-----------------\n",
            "Finished episode: 865 Reward: -1326.5593 total_loss = 15.8932 = 0.1081 + 0.5 * 31.5754 + 0.01 * -0.2537\n",
            "-----------------\n",
            "Finished episode: 866 Reward: -1334.9239 total_loss = 16.1097 = 0.0792 + 0.5 * 32.0661 + 0.01 * -0.2613\n",
            "-----------------\n",
            "Finished episode: 867 Reward: -1338.1229 total_loss = 16.1611 = 0.0608 + 0.5 * 32.2057 + 0.01 * -0.2480\n",
            "-----------------\n",
            "Finished episode: 868 Reward: -1336.0103 total_loss = 13.2620 = -0.0607 + 0.5 * 26.6503 + 0.01 * -0.2495\n",
            "-----------------\n",
            "Finished episode: 869 Reward: -1333.2307 total_loss = 11.9555 = -0.0359 + 0.5 * 23.9881 + 0.01 * -0.2728\n",
            "-----------------\n",
            "Finished episode: 870 Reward: -1332.4137 total_loss = 14.6776 = 0.0194 + 0.5 * 29.3216 + 0.01 * -0.2571\n",
            "-----------------\n",
            "Finished episode: 871 Reward: -1345.6502 total_loss = 16.9745 = 0.0370 + 0.5 * 33.8805 + 0.01 * -0.2711\n",
            "-----------------\n",
            "Finished episode: 872 Reward: -1324.7458 total_loss = 14.0425 = 0.0548 + 0.5 * 27.9806 + 0.01 * -0.2611\n",
            "-----------------\n",
            "Finished episode: 873 Reward: -1340.5442 total_loss = 14.3164 = -0.0563 + 0.5 * 28.7507 + 0.01 * -0.2612\n",
            "-----------------\n",
            "Finished episode: 874 Reward: -1333.2153 total_loss = 14.4876 = -0.0831 + 0.5 * 29.1467 + 0.01 * -0.2741\n",
            "-----------------\n",
            "Finished episode: 875 Reward: -1337.0699 total_loss = 14.3129 = 0.0123 + 0.5 * 28.6065 + 0.01 * -0.2597\n",
            "-----------------\n",
            "Finished episode: 876 Reward: -1341.5852 total_loss = 13.2538 = 0.0759 + 0.5 * 26.3611 + 0.01 * -0.2690\n",
            "-----------------\n",
            "Finished episode: 877 Reward: -1338.1454 total_loss = 15.0743 = 0.0389 + 0.5 * 30.0760 + 0.01 * -0.2623\n",
            "-----------------\n",
            "Finished episode: 878 Reward: -1328.1416 total_loss = 14.8657 = 0.0459 + 0.5 * 29.6448 + 0.01 * -0.2606\n",
            "-----------------\n",
            "Finished episode: 879 Reward: -1324.5739 total_loss = 14.9024 = -0.0458 + 0.5 * 29.9017 + 0.01 * -0.2586\n",
            "-----------------\n",
            "Finished episode: 880 Reward: -1328.7248 total_loss = 13.7774 = -0.0395 + 0.5 * 27.6392 + 0.01 * -0.2661\n",
            "-----------------\n",
            "Finished episode: 881 Reward: -1336.5782 total_loss = 16.4052 = 0.0768 + 0.5 * 32.6622 + 0.01 * -0.2700\n",
            "-----------------\n",
            "Finished episode: 882 Reward: -1325.2936 total_loss = 13.3959 = -0.0039 + 0.5 * 26.8050 + 0.01 * -0.2595\n",
            "-----------------\n",
            "Finished episode: 883 Reward: -1345.4287 total_loss = 16.1303 = -0.0611 + 0.5 * 32.3882 + 0.01 * -0.2682\n",
            "-----------------\n",
            "Finished episode: 884 Reward: -1330.6304 total_loss = 13.7063 = -0.0130 + 0.5 * 27.4440 + 0.01 * -0.2682\n",
            "-----------------\n",
            "Finished episode: 885 Reward: -1331.4945 total_loss = 12.3945 = -0.0133 + 0.5 * 24.8210 + 0.01 * -0.2696\n",
            "-----------------\n",
            "Finished episode: 886 Reward: -1328.3736 total_loss = 13.2844 = 0.0009 + 0.5 * 26.5721 + 0.01 * -0.2606\n",
            "-----------------\n",
            "Finished episode: 887 Reward: -1329.8839 total_loss = 15.2964 = 0.0883 + 0.5 * 30.4217 + 0.01 * -0.2656\n",
            "-----------------\n",
            "Finished episode: 888 Reward: -1334.6311 total_loss = 14.6732 = 0.0254 + 0.5 * 29.3008 + 0.01 * -0.2616\n",
            "-----------------\n",
            "Finished episode: 889 Reward: -1338.2772 total_loss = 15.5882 = -0.0304 + 0.5 * 31.2424 + 0.01 * -0.2646\n",
            "-----------------\n",
            "Finished episode: 890 Reward: -1340.4213 total_loss = 13.4913 = -0.0611 + 0.5 * 27.1100 + 0.01 * -0.2589\n",
            "-----------------\n",
            "Finished episode: 891 Reward: -1334.5791 total_loss = 15.9322 = 0.0395 + 0.5 * 31.7907 + 0.01 * -0.2689\n",
            "-----------------\n",
            "Finished episode: 892 Reward: -1343.4009 total_loss = 14.5906 = 0.0091 + 0.5 * 29.1684 + 0.01 * -0.2696\n",
            "-----------------\n",
            "Finished episode: 893 Reward: -1332.8188 total_loss = 15.4282 = 0.0485 + 0.5 * 30.7650 + 0.01 * -0.2754\n",
            "-----------------\n",
            "Finished episode: 894 Reward: -1339.5373 total_loss = 13.8192 = 0.0027 + 0.5 * 27.6384 + 0.01 * -0.2649\n",
            "-----------------\n",
            "Finished episode: 895 Reward: -1336.6022 total_loss = 15.0492 = -0.0286 + 0.5 * 30.1608 + 0.01 * -0.2636\n",
            "-----------------\n",
            "Finished episode: 896 Reward: -1337.4777 total_loss = 15.4890 = -0.0108 + 0.5 * 31.0049 + 0.01 * -0.2666\n",
            "-----------------\n",
            "Finished episode: 897 Reward: -1335.5787 total_loss = 14.5335 = -0.1585 + 0.5 * 29.3892 + 0.01 * -0.2642\n",
            "-----------------\n",
            "Finished episode: 898 Reward: -1337.5930 total_loss = 15.6175 = 0.0454 + 0.5 * 31.1495 + 0.01 * -0.2622\n",
            "-----------------\n",
            "Finished episode: 899 Reward: -1337.4758 total_loss = 13.8215 = 0.0014 + 0.5 * 27.6453 + 0.01 * -0.2590\n",
            "-----------------\n",
            "Finished episode: 900 Reward: -1340.8493 total_loss = 15.8812 = 0.0268 + 0.5 * 31.7141 + 0.01 * -0.2679\n",
            "-----------------\n",
            "Finished episode: 901 Reward: -1333.8374 total_loss = 14.0369 = 0.0327 + 0.5 * 28.0139 + 0.01 * -0.2738\n",
            "-----------------\n",
            "Finished episode: 902 Reward: -1337.6136 total_loss = 16.0216 = 0.0178 + 0.5 * 32.0129 + 0.01 * -0.2699\n",
            "-----------------\n",
            "Finished episode: 903 Reward: -1337.0677 total_loss = 15.6064 = 0.1331 + 0.5 * 30.9520 + 0.01 * -0.2683\n",
            "-----------------\n",
            "Finished episode: 904 Reward: -1329.2665 total_loss = 13.1845 = -0.1264 + 0.5 * 26.6271 + 0.01 * -0.2633\n",
            "-----------------\n",
            "Finished episode: 905 Reward: -1336.1063 total_loss = 13.1655 = -0.0163 + 0.5 * 26.3689 + 0.01 * -0.2650\n",
            "-----------------\n",
            "Finished episode: 906 Reward: -1338.1101 total_loss = 15.0573 = 0.1267 + 0.5 * 29.8665 + 0.01 * -0.2633\n",
            "-----------------\n",
            "Finished episode: 907 Reward: -1342.2876 total_loss = 15.4609 = -0.0742 + 0.5 * 31.0755 + 0.01 * -0.2654\n",
            "-----------------\n",
            "Finished episode: 908 Reward: -1336.9790 total_loss = 14.8045 = -0.0191 + 0.5 * 29.6523 + 0.01 * -0.2639\n",
            "-----------------\n",
            "Finished episode: 909 Reward: -1335.0264 total_loss = 13.0120 = 0.0078 + 0.5 * 26.0137 + 0.01 * -0.2660\n",
            "-----------------\n",
            "Finished episode: 910 Reward: -1320.4237 total_loss = 14.5557 = -0.0273 + 0.5 * 29.1713 + 0.01 * -0.2703\n",
            "-----------------\n",
            "Finished episode: 911 Reward: -1332.8110 total_loss = 13.8899 = -0.0123 + 0.5 * 27.8098 + 0.01 * -0.2675\n",
            "-----------------\n",
            "Finished episode: 912 Reward: -1331.4024 total_loss = 13.4644 = 0.0504 + 0.5 * 26.8335 + 0.01 * -0.2748\n",
            "-----------------\n",
            "Finished episode: 913 Reward: -1324.5666 total_loss = 11.4390 = 0.0278 + 0.5 * 22.8276 + 0.01 * -0.2669\n",
            "-----------------\n",
            "Finished episode: 914 Reward: -1339.4417 total_loss = 13.1949 = -0.0458 + 0.5 * 26.4866 + 0.01 * -0.2679\n",
            "-----------------\n",
            "Finished episode: 915 Reward: -1333.6024 total_loss = 13.9321 = -0.0160 + 0.5 * 27.9017 + 0.01 * -0.2698\n",
            "-----------------\n",
            "Finished episode: 916 Reward: -1332.1174 total_loss = 14.0746 = -0.0864 + 0.5 * 28.3271 + 0.01 * -0.2623\n",
            "-----------------\n",
            "Finished episode: 917 Reward: -1331.3391 total_loss = 13.5473 = 0.0088 + 0.5 * 27.0823 + 0.01 * -0.2734\n",
            "-----------------\n",
            "Finished episode: 918 Reward: -1347.5842 total_loss = 15.9339 = -0.1070 + 0.5 * 32.0872 + 0.01 * -0.2692\n",
            "-----------------\n",
            "Finished episode: 919 Reward: -1325.7204 total_loss = 14.0680 = -0.0865 + 0.5 * 28.3145 + 0.01 * -0.2739\n",
            "-----------------\n",
            "Finished episode: 920 Reward: -1331.2385 total_loss = 13.5879 = -0.0605 + 0.5 * 27.3022 + 0.01 * -0.2662\n",
            "-----------------\n",
            "Finished episode: 921 Reward: -1332.5797 total_loss = 13.8504 = 0.0389 + 0.5 * 27.6282 + 0.01 * -0.2680\n",
            "-----------------\n",
            "Finished episode: 922 Reward: -1333.6060 total_loss = 15.9776 = -0.0584 + 0.5 * 32.0774 + 0.01 * -0.2712\n",
            "-----------------\n",
            "Finished episode: 923 Reward: -1320.0888 total_loss = 15.0769 = -0.0904 + 0.5 * 30.3400 + 0.01 * -0.2636\n",
            "-----------------\n",
            "Finished episode: 924 Reward: -1331.1963 total_loss = 12.6062 = 0.0075 + 0.5 * 25.2030 + 0.01 * -0.2762\n",
            "-----------------\n",
            "Finished episode: 925 Reward: -1343.7656 total_loss = 15.2660 = 0.0230 + 0.5 * 30.4910 + 0.01 * -0.2570\n",
            "-----------------\n",
            "Finished episode: 926 Reward: -1333.0772 total_loss = 14.9692 = -0.0324 + 0.5 * 30.0085 + 0.01 * -0.2630\n",
            "-----------------\n",
            "Finished episode: 927 Reward: -1342.2603 total_loss = 15.4509 = 0.0233 + 0.5 * 30.8606 + 0.01 * -0.2729\n",
            "-----------------\n",
            "Finished episode: 928 Reward: -1338.7890 total_loss = 13.7170 = 0.0319 + 0.5 * 27.3755 + 0.01 * -0.2677\n",
            "-----------------\n",
            "Finished episode: 929 Reward: -1330.6831 total_loss = 13.2928 = -0.0705 + 0.5 * 26.7320 + 0.01 * -0.2705\n",
            "-----------------\n",
            "Finished episode: 930 Reward: -1333.9066 total_loss = 14.7029 = 0.1154 + 0.5 * 29.1804 + 0.01 * -0.2693\n",
            "-----------------\n",
            "Finished episode: 931 Reward: -1336.0927 total_loss = 15.4991 = 0.0121 + 0.5 * 30.9794 + 0.01 * -0.2670\n",
            "-----------------\n",
            "Finished episode: 932 Reward: -1342.5113 total_loss = 15.9975 = -0.0194 + 0.5 * 32.0393 + 0.01 * -0.2750\n",
            "-----------------\n",
            "Finished episode: 933 Reward: -1335.4541 total_loss = 14.6274 = -0.0937 + 0.5 * 29.4473 + 0.01 * -0.2636\n",
            "-----------------\n",
            "Finished episode: 934 Reward: -1327.3181 total_loss = 13.0966 = 0.0097 + 0.5 * 26.1793 + 0.01 * -0.2719\n",
            "-----------------\n",
            "Finished episode: 935 Reward: -1339.0482 total_loss = 15.1981 = 0.0441 + 0.5 * 30.3133 + 0.01 * -0.2636\n",
            "-----------------\n",
            "Finished episode: 936 Reward: -1331.1655 total_loss = 13.3294 = 0.0666 + 0.5 * 26.5312 + 0.01 * -0.2764\n",
            "-----------------\n",
            "Finished episode: 937 Reward: -1328.7732 total_loss = 13.4868 = -0.0093 + 0.5 * 26.9977 + 0.01 * -0.2733\n",
            "-----------------\n",
            "Finished episode: 938 Reward: -1337.6066 total_loss = 13.7455 = 0.0233 + 0.5 * 27.4496 + 0.01 * -0.2632\n",
            "-----------------\n",
            "Finished episode: 939 Reward: -1347.2690 total_loss = 17.1032 = 0.0005 + 0.5 * 34.2110 + 0.01 * -0.2746\n",
            "-----------------\n",
            "Finished episode: 940 Reward: -1334.4869 total_loss = 11.6990 = 0.0603 + 0.5 * 23.2827 + 0.01 * -0.2688\n",
            "-----------------\n",
            "Finished episode: 941 Reward: -1347.7775 total_loss = 15.8587 = 0.0088 + 0.5 * 31.7053 + 0.01 * -0.2715\n",
            "-----------------\n",
            "Finished episode: 942 Reward: -1328.2321 total_loss = 14.1573 = 0.0142 + 0.5 * 28.2915 + 0.01 * -0.2718\n",
            "-----------------\n",
            "Finished episode: 943 Reward: -1332.9261 total_loss = 13.9414 = -0.0858 + 0.5 * 28.0598 + 0.01 * -0.2633\n",
            "-----------------\n",
            "Finished episode: 944 Reward: -1333.8899 total_loss = 14.9243 = 0.1258 + 0.5 * 29.6025 + 0.01 * -0.2681\n",
            "-----------------\n",
            "Finished episode: 945 Reward: -1327.1433 total_loss = 16.0600 = -0.0006 + 0.5 * 32.1265 + 0.01 * -0.2693\n",
            "-----------------\n",
            "Finished episode: 946 Reward: -1342.7580 total_loss = 14.8685 = 0.0771 + 0.5 * 29.5882 + 0.01 * -0.2680\n",
            "-----------------\n",
            "Finished episode: 947 Reward: -1338.8125 total_loss = 16.5580 = -0.0646 + 0.5 * 33.2506 + 0.01 * -0.2728\n",
            "-----------------\n",
            "Finished episode: 948 Reward: -1332.3237 total_loss = 13.9543 = -0.0266 + 0.5 * 27.9670 + 0.01 * -0.2662\n",
            "-----------------\n",
            "Finished episode: 949 Reward: -1331.8994 total_loss = 15.4364 = -0.0385 + 0.5 * 30.9551 + 0.01 * -0.2716\n",
            "-----------------\n",
            "Finished episode: 950 Reward: -1335.5302 total_loss = 14.0511 = -0.0719 + 0.5 * 28.2514 + 0.01 * -0.2727\n",
            "-----------------\n",
            "Finished episode: 951 Reward: -1332.6826 total_loss = 15.4345 = -0.0041 + 0.5 * 30.8825 + 0.01 * -0.2707\n",
            "-----------------\n",
            "Finished episode: 952 Reward: -1339.2111 total_loss = 14.7782 = -0.0329 + 0.5 * 29.6275 + 0.01 * -0.2725\n",
            "-----------------\n",
            "Finished episode: 953 Reward: -1341.7278 total_loss = 13.9393 = -0.0200 + 0.5 * 27.9239 + 0.01 * -0.2667\n",
            "-----------------\n",
            "Finished episode: 954 Reward: -1341.6941 total_loss = 16.8573 = 0.0219 + 0.5 * 33.6761 + 0.01 * -0.2645\n",
            "-----------------\n",
            "Finished episode: 955 Reward: -1333.4150 total_loss = 13.4245 = 0.1700 + 0.5 * 26.5145 + 0.01 * -0.2694\n",
            "-----------------\n",
            "Finished episode: 956 Reward: -1340.2133 total_loss = 15.4009 = -0.0923 + 0.5 * 30.9919 + 0.01 * -0.2775\n",
            "-----------------\n",
            "Finished episode: 957 Reward: -1344.3226 total_loss = 14.5251 = 0.0080 + 0.5 * 29.0396 + 0.01 * -0.2676\n",
            "-----------------\n",
            "Finished episode: 958 Reward: -1336.8300 total_loss = 15.0519 = 0.0625 + 0.5 * 29.9842 + 0.01 * -0.2714\n",
            "-----------------\n",
            "Finished episode: 959 Reward: -1337.9420 total_loss = 13.8155 = -0.0670 + 0.5 * 27.7704 + 0.01 * -0.2723\n",
            "-----------------\n",
            "Finished episode: 960 Reward: -1336.5222 total_loss = 14.7959 = -0.0163 + 0.5 * 29.6298 + 0.01 * -0.2692\n",
            "-----------------\n",
            "Finished episode: 961 Reward: -1327.1745 total_loss = 15.5312 = -0.0879 + 0.5 * 31.2436 + 0.01 * -0.2710\n",
            "-----------------\n",
            "Finished episode: 962 Reward: -1334.0979 total_loss = 13.7295 = 0.0801 + 0.5 * 27.3043 + 0.01 * -0.2700\n",
            "-----------------\n",
            "Finished episode: 963 Reward: -1331.3849 total_loss = 11.8536 = 0.0594 + 0.5 * 23.5938 + 0.01 * -0.2775\n",
            "-----------------\n",
            "Finished episode: 964 Reward: -1336.5989 total_loss = 13.8211 = 0.0434 + 0.5 * 27.5608 + 0.01 * -0.2766\n",
            "-----------------\n",
            "Finished episode: 965 Reward: -1334.6228 total_loss = 14.7551 = 0.0332 + 0.5 * 29.4493 + 0.01 * -0.2720\n",
            "-----------------\n",
            "Finished episode: 966 Reward: -1344.2119 total_loss = 15.0417 = 0.0222 + 0.5 * 30.0442 + 0.01 * -0.2658\n",
            "-----------------\n",
            "Finished episode: 967 Reward: -1328.8824 total_loss = 16.3640 = 0.0164 + 0.5 * 32.7006 + 0.01 * -0.2760\n",
            "-----------------\n",
            "Finished episode: 968 Reward: -1338.9340 total_loss = 14.7959 = 0.1347 + 0.5 * 29.3277 + 0.01 * -0.2731\n",
            "-----------------\n",
            "Finished episode: 969 Reward: -1315.3058 total_loss = 13.9485 = -0.0509 + 0.5 * 28.0041 + 0.01 * -0.2656\n",
            "-----------------\n",
            "Finished episode: 970 Reward: -1326.0778 total_loss = 14.2004 = -0.0381 + 0.5 * 28.4823 + 0.01 * -0.2704\n",
            "-----------------\n",
            "Finished episode: 971 Reward: -1335.5494 total_loss = 16.9755 = 0.0785 + 0.5 * 33.7993 + 0.01 * -0.2709\n",
            "-----------------\n",
            "Finished episode: 972 Reward: -1341.3081 total_loss = 14.8608 = -0.0467 + 0.5 * 29.8204 + 0.01 * -0.2686\n",
            "-----------------\n",
            "Finished episode: 973 Reward: -1330.5383 total_loss = 13.1824 = -0.1018 + 0.5 * 26.5738 + 0.01 * -0.2674\n",
            "-----------------\n",
            "Finished episode: 974 Reward: -1332.9282 total_loss = 15.0406 = 0.0004 + 0.5 * 30.0856 + 0.01 * -0.2644\n",
            "-----------------\n",
            "Finished episode: 975 Reward: -1333.0583 total_loss = 13.2053 = 0.0776 + 0.5 * 26.2608 + 0.01 * -0.2700\n",
            "-----------------\n",
            "Finished episode: 976 Reward: -1341.8508 total_loss = 16.0668 = -0.0207 + 0.5 * 32.1804 + 0.01 * -0.2700\n",
            "-----------------\n",
            "Finished episode: 977 Reward: -1333.1151 total_loss = 14.8685 = -0.0180 + 0.5 * 29.7785 + 0.01 * -0.2749\n",
            "-----------------\n",
            "Finished episode: 978 Reward: -1330.6899 total_loss = 13.6824 = -0.0529 + 0.5 * 27.4760 + 0.01 * -0.2653\n",
            "-----------------\n",
            "Finished episode: 979 Reward: -1331.2181 total_loss = 13.7475 = -0.0169 + 0.5 * 27.5342 + 0.01 * -0.2680\n",
            "-----------------\n",
            "Finished episode: 980 Reward: -1330.2860 total_loss = 14.5557 = -0.0411 + 0.5 * 29.1992 + 0.01 * -0.2735\n",
            "-----------------\n",
            "Finished episode: 981 Reward: -1333.2576 total_loss = 14.9589 = -0.0202 + 0.5 * 29.9635 + 0.01 * -0.2694\n",
            "-----------------\n",
            "Finished episode: 982 Reward: -1321.3919 total_loss = 13.8951 = -0.0122 + 0.5 * 27.8200 + 0.01 * -0.2642\n",
            "-----------------\n",
            "Finished episode: 983 Reward: -1337.5537 total_loss = 13.7873 = -0.0223 + 0.5 * 27.6245 + 0.01 * -0.2681\n",
            "-----------------\n",
            "Finished episode: 984 Reward: -1333.4202 total_loss = 15.1210 = -0.0264 + 0.5 * 30.3004 + 0.01 * -0.2725\n",
            "-----------------\n",
            "Finished episode: 985 Reward: -1340.8029 total_loss = 14.5129 = -0.0623 + 0.5 * 29.1558 + 0.01 * -0.2663\n",
            "-----------------\n",
            "Finished episode: 986 Reward: -1334.1136 total_loss = 14.1786 = -0.0384 + 0.5 * 28.4392 + 0.01 * -0.2670\n",
            "-----------------\n",
            "Finished episode: 987 Reward: -1343.5357 total_loss = 14.3102 = -0.0059 + 0.5 * 28.6373 + 0.01 * -0.2638\n",
            "-----------------\n",
            "Finished episode: 988 Reward: -1337.5065 total_loss = 15.6668 = 0.0527 + 0.5 * 31.2336 + 0.01 * -0.2703\n",
            "-----------------\n",
            "Finished episode: 989 Reward: -1334.9871 total_loss = 14.2757 = -0.0672 + 0.5 * 28.6913 + 0.01 * -0.2709\n",
            "-----------------\n",
            "Finished episode: 990 Reward: -1333.5073 total_loss = 14.2157 = -0.1015 + 0.5 * 28.6399 + 0.01 * -0.2734\n",
            "-----------------\n",
            "Finished episode: 991 Reward: -1326.7411 total_loss = 12.5335 = 0.1088 + 0.5 * 24.8547 + 0.01 * -0.2663\n",
            "-----------------\n",
            "Finished episode: 992 Reward: -1326.3931 total_loss = 14.8934 = -0.0596 + 0.5 * 29.9113 + 0.01 * -0.2666\n",
            "-----------------\n",
            "Finished episode: 993 Reward: -1342.5676 total_loss = 15.5645 = -0.0150 + 0.5 * 31.1646 + 0.01 * -0.2748\n",
            "-----------------\n",
            "Finished episode: 994 Reward: -1340.0066 total_loss = 16.6857 = 0.0309 + 0.5 * 33.3149 + 0.01 * -0.2672\n",
            "-----------------\n",
            "Finished episode: 995 Reward: -1327.8948 total_loss = 14.5842 = -0.0081 + 0.5 * 29.1899 + 0.01 * -0.2653\n",
            "-----------------\n",
            "Finished episode: 996 Reward: -1337.3402 total_loss = 14.5638 = 0.1013 + 0.5 * 28.9303 + 0.01 * -0.2732\n",
            "-----------------\n",
            "Finished episode: 997 Reward: -1328.8062 total_loss = 15.4783 = -0.0685 + 0.5 * 31.0990 + 0.01 * -0.2669\n",
            "-----------------\n",
            "Finished episode: 998 Reward: -1328.3727 total_loss = 12.1870 = -0.0383 + 0.5 * 24.4561 + 0.01 * -0.2728\n",
            "-----------------\n",
            "Finished episode: 999 Reward: -1333.0358 total_loss = 14.4668 = 0.2111 + 0.5 * 28.5170 + 0.01 * -0.2764\n",
            "-----------------\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cVV1-fJWArMt"
      },
      "source": [
        "# DDPG and TD3\n",
        "\n",
        "The Deterministic Policy Gradient method was proposed by Silver et. al. 2014 (http://proceedings.mlr.press/v32/silver14.pdf), and DDPG is its deep version.\n",
        "\n",
        "The DPG also uses the actor-critic paradigm, but maitains a deterministic version of policy. It optimizes the critic through the Bellman Equation, and optimize the actor through the chain rule. \n",
        "\n",
        "In this assignment, you may need to import some python files like DDPG.py and TD3.py to insert the method into training.\n",
        "Here are some solutions from stackoverflow: https://stackoverflow.com/questions/48905127/importing-py-files-in-google-colab.\n",
        "\n",
        "It is easier to just copy it from Drive than upload it.\n",
        "1. Store MYLIB.py in your Drive. (for this assignment, it will be the utils.py, DDPG.py and TD3.py)\n",
        "2. Open the Colab.\n",
        "3. Open the left side pane, select Files view (the file icon).\n",
        "4. Click Mount Drive then Connect to Google Drive (the folder with google drive icon).\n",
        "5. Copy it by running \"! cp drive/My\\ Drive/MYLIB.py . \" in your Colab file code line.\n",
        "6. import MYLIB\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "g6OJdsG_MJz_"
      },
      "source": [
        "## TODOs for You (Please write down the answer in this block)\n",
        "\n",
        "The TD3 is short for *Twin Delayed Deep Deterministic Policy Gradient*, their official open-source implementation is extremely clear and easy to follow! So I believe there is no need for you to build up the wheels one more time.\n",
        "\n",
        "However, you really need to know about how this method works!\n",
        "TD3 proposes several improvements based on the method of DDPG to improve its sample efficiency.\n",
        "\n",
        "- Q6. In this part, your task is to read the paper, and read the code of the official implementation of TD3 and DDPG at:\n",
        "\n",
        "https://github.com/sfujim/TD3/blob/master/DDPG.py\n",
        "\n",
        "https://github.com/sfujim/TD3/blob/master/TD3.py\n",
        "\n",
        "Then, please try to find the proposed improvements in TD3 over DDPG and summary them HERE:\n",
        "\n",
        "1. clipped double-Q learning: \n",
        " TD3 learns two Q-functions instead of one (hence \"twin\"), and uses the smaller of the two Q-values to form the targets in the Bellman error loss functions.\n",
        "    - code: see the below block"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "X1BMcG201UCM",
        "outputId": "73e080e4-f938-41bb-c93a-5c1840608091"
      },
      "source": [
        "class Critic(nn.Module):\n",
        " def __init__(self, state_dim, action_dim):\n",
        "  super(Critic, self).__init__()\n",
        "\n",
        "  # Q1 architecture\n",
        "  self.l1 = nn.Linear(state_dim + action_dim, 256)\n",
        "  self.l2 = nn.Linear(256, 256)\n",
        "  self.l3 = nn.Linear(256, 1)\n",
        "\n",
        "  # Q2 architecture\n",
        "  self.l4 = nn.Linear(state_dim + action_dim, 256)\n",
        "  self.l5 = nn.Linear(256, 256)\n",
        "  self.l6 = nn.Linear(256, 1)\n",
        "\n",
        "\n",
        " def forward(self, state, action):\n",
        "  sa = torch.cat([state, action], 1)\n",
        "\n",
        "  q1 = F.relu(self.l1(sa))\n",
        "  q1 = F.relu(self.l2(q1))\n",
        "  q1 = self.l3(q1)\n",
        "\n",
        "  q2 = F.relu(self.l4(sa))\n",
        "  q2 = F.relu(self.l5(q2))\n",
        "  q2 = self.l6(q2)\n",
        "  return q1, q2\n",
        "\n",
        "self.critic = Critic(state_dim, action_dim).to(device)\n",
        "self.critic_target = copy.deepcopy(self.critic)\n",
        " # Compute the target Q value\n",
        "target_Q1, target_Q2 = self.critic_target(next_state, next_action)\n",
        "target_Q = torch.min(target_Q1, target_Q2)\n",
        "target_Q = reward + not_done * self.discount * target_Q"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-27-fba06eac1e64>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     26\u001b[0m   \u001b[0;32mreturn\u001b[0m \u001b[0mq1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mq2\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     27\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 28\u001b[0;31m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcritic\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mCritic\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstate_dim\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maction_dim\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     29\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcritic_target\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcopy\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdeepcopy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcritic\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     30\u001b[0m  \u001b[0;31m# Compute the target Q value\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mNameError\u001b[0m: name 'state_dim' is not defined"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SB3fezF61UCM"
      },
      "source": [
        "2. \"Delayed\" Policy updates\n",
        " TD3 updates the policy (and target networks) less frequently than the Q-function.\n",
        "    - code: see the below block"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vDRmkLAD1UCM"
      },
      "source": [
        "    # Delayed policy updates\n",
        "if self.total_it % self.policy_freq == 0:\n",
        "\n",
        "      # Compute actor loss\n",
        "    actor_loss = -self.critic.Q1(state, self.actor(state)).mean()\n",
        "      \n",
        "      # Optimize the actor \n",
        "    self.actor_optimizer.zero_grad()\n",
        "    actor_loss.backward()\n",
        "    self.actor_optimizer.step()\n",
        "\n",
        "      # Update the frozen target models\n",
        "    for param, target_param in zip(self.critic.parameters(), self.critic_target.parameters()):\n",
        "        target_param.data.copy_(self.tau * param.data + (1 - self.tau) * target_param.data)\n",
        "\n",
        "    for param, target_param in zip(self.actor.parameters(), self.actor_target.parameters()):\n",
        "        target_param.data.copy_(self.tau * param.data + (1 - self.tau) * target_param.data)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pVHPBfVy1UCM"
      },
      "source": [
        "3. Target Policy Smoothing. \n",
        " TD3 adds noise to the target action, to make it harder for the policy to exploit Q-function errors by smoothing out Q along changes in action.\n",
        "    - code: see the below block"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rVRkLSUi1UCM"
      },
      "source": [
        "# Select action according to policy and add clipped noise\n",
        "noise = (\n",
        "        torch.randn_like(action) * self.policy_noise\n",
        "      ).clamp(-self.noise_clip, self.noise_clip)\n",
        "      \n",
        "next_action = (\n",
        "        self.actor_target(next_state) + noise\n",
        "      ).clamp(-self.max_action, self.max_action)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "p4ut31n81UCM"
      },
      "source": [
        "4. Critic Structure (TD3/OursDDPG/DDPG)\n",
        "    - code: see the code below\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nupEnRoS1UCM"
      },
      "source": [
        "class Critic(nn.Module):\n",
        "\tdef __init__(self, state_dim, action_dim):\n",
        "\t\tsuper(Critic, self).__init__()\n",
        "\n",
        "\t\tself.l1 = nn.Linear(state_dim, 400)\n",
        "\t\tself.l2 = nn.Linear(400 + action_dim, 300)\n",
        "\t\tself.l3 = nn.Linear(300, 1)\n",
        "\n",
        "class Critic(nn.Module):\n",
        "\tdef __init__(self, state_dim, action_dim):\n",
        "\t\tsuper(Critic, self).__init__()\n",
        "\n",
        "\t\tself.l1 = nn.Linear(state_dim + action_dim, 400)\n",
        "\t\tself.l2 = nn.Linear(400, 300)\n",
        "\t\tself.l3 = nn.Linear(300, 1)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QIGgcTG31UCM"
      },
      "source": [
        "- Q7. Among all those improvements, which do you believe is the most important one? You may take some ablation studies to support your claim.  (i.e., draw some learning curves with different settings together and draw your conclusions)\n",
        "\n",
        "- Q8. What is the difference between TD3(DDPG) and PPO in the OPTIMIZATION step (including but not restricted in terms of the sampling-training proportion)? Actually the improvements of PPO over TRPO was pointed  as a benefit of more training iterations, can you further improve the sample efficiency of TD3?\n",
        "\n",
        "- Q9. (i) Please describe the difference of the exploration strategies between PPO, DDPG and TD3. (ii) Provide a comparison between the exploration strategies of those continuous control algorithms and DQN.\n",
        "\n",
        "- Q10. (Bonus, 20 points) An open question. Do you think an epsilon-greedy-like exploration strategy you used in DQN/Q-learning is useful for continuous control? Will there be any problem of applying epsilon-greedy method in DDPG/TD3/PPO? Try to implement the idea and report the results."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "h3_McxdG1UCN"
      },
      "source": [
        "The following four blocks download the code in official implementation to your google drive so that the following script can run them. Note that the downloaded files may disappear due to some colab mechansim."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "R2jXxeaWoCPM"
      },
      "source": [
        "!git clone https://github.com/sfujim/TD3.git"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "66Sy-dOQ8Qo5"
      },
      "source": [
        "!cp TD3/DDPG.py ."
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5TuZqwcHottp"
      },
      "source": [
        "!cp TD3/TD3.py ."
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "259dqcCpotrJ"
      },
      "source": [
        "!cp TD3/utils.py ."
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2X3lvH_5pKkS"
      },
      "source": [
        "from os import makedirs as mkdir\n",
        "mkdir('results', exist_ok=True)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Lboiuk9vHDhD"
      },
      "source": [
        "# The following scripts run the DDPG algorithm.\n",
        "\n",
        "alias = 'ddpg' # an alias of your experiment, used as a label\n",
        "\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "import torch\n",
        "import gym\n",
        "import argparse\n",
        "import os\n",
        "import torch.nn.functional as F\n",
        "import utils\n",
        "import TD3\n",
        "import DDPG\n",
        "\n",
        "def eval_policy(policy, eval_episodes=10):\n",
        "    eval_env = gym.make(ENV_NAME)\n",
        "\n",
        "    avg_reward = 0.\n",
        "    for _ in range(eval_episodes):\n",
        "        state, done = eval_env.reset(), False\n",
        "        while not done:\n",
        "            action = policy.select_action(np.array(state))\n",
        "            state, reward, done,_ = eval_env.step(action)\n",
        "            avg_reward += reward\n",
        "\n",
        "    avg_reward /= eval_episodes\n",
        "    #print(\"---------------------------------------\")\n",
        "    #print(f\"Evaluation over {eval_episodes} episodes: {avg_reward:.3f}\")\n",
        "    #print(\"---------------------------------------\")\n",
        "    return avg_reward\n",
        "\n",
        "env = gym.make(ENV_NAME)\n",
        "torch.manual_seed(0)\n",
        "np.random.seed(0)\n",
        "\n",
        "state_dim = env.observation_space.shape[0]\n",
        "action_dim = env.action_space.shape[0]\n",
        "max_action = env.action_space.high[0]\n",
        "\n",
        "args_policy_noise = 0.2\n",
        "args_noise_clip = 0.5\n",
        "args_policy_freq = 2\n",
        "args_max_timesteps = 100000\n",
        "args_expl_noise = 0.1\n",
        "args_batch_size = 25\n",
        "args_eval_freq = 1000\n",
        "args_start_timesteps = 0\n",
        "\n",
        "kwargs = {\n",
        "    \"state_dim\": state_dim,\n",
        "    \"action_dim\": action_dim,\n",
        "    \"max_action\": max_action,\n",
        "    \"discount\": 0.99,\n",
        "    \"tau\": 0.005\n",
        "}\n",
        "\n",
        "\n",
        "args_policy = 'DDPG'\n",
        "\n",
        "if args_policy == \"TD3\":\n",
        "    # Target policy smoothing is scaled wrt the action scale\n",
        "    kwargs[\"policy_noise\"] = args_policy_noise * max_action\n",
        "    kwargs[\"noise_clip\"] = args_noise_clip * max_action\n",
        "    kwargs[\"policy_freq\"] = args_policy_freq\n",
        "    policy = TD3.TD3(**kwargs)\n",
        "elif args_policy == \"DDPG\":\n",
        "    policy = DDPG.DDPG(**kwargs)\n",
        "replay_buffer = utils.ReplayBuffer(state_dim, action_dim)\n",
        "\n",
        "# Evaluate untrained policy\n",
        "evaluations = [eval_policy(policy)]\n",
        "\n",
        "state, done = env.reset(), False\n",
        "episode_reward = 0\n",
        "episode_timesteps = 0\n",
        "episode_num = 0\n",
        "counter = 0\n",
        "msk_list = []        \n",
        "temp_curve = [eval_policy(policy)]\n",
        "temp_val = []\n",
        "for t in range(int(args_max_timesteps)):\n",
        "    episode_timesteps += 1\n",
        "    counter += 1\n",
        "    # Select action randomly or according to policy\n",
        "    if t < args_start_timesteps:\n",
        "        action = np.random.uniform(-max_action,max_action,action_dim)\n",
        "    else:\n",
        "        if np.random.uniform(0,1) < 0.1:\n",
        "            action = np.random.uniform(-max_action,max_action,action_dim)\n",
        "        else:\n",
        "            action = (\n",
        "                policy.select_action(np.array(state))\n",
        "                + np.random.normal(0, max_action * args_expl_noise, size=action_dim)\n",
        "            ).clip(-max_action, max_action)\n",
        "\n",
        "    # Perform action\n",
        "    next_state, reward, done,_ = env.step(action) \n",
        "    done_bool = float(done) if episode_timesteps < env._max_episode_steps else 0\n",
        "\n",
        "    replay_buffer.add(state, action, next_state, reward, done_bool)\n",
        "\n",
        "    state = next_state\n",
        "    episode_reward += reward\n",
        "\n",
        "    if t >= args_start_timesteps:\n",
        "        '''TD3'''\n",
        "        last_val = 999.\n",
        "        patient = 5\n",
        "        for i in range(1):\n",
        "            policy.train(replay_buffer, args_batch_size)\n",
        "                \n",
        "\n",
        "    # Train agent after collecting sufficient data\n",
        "    if done: \n",
        "        print(f\"Total T: {t+1} Episode Num: {episode_num+1} Episode T: {episode_timesteps} Reward: {episode_reward:.3f}\")\n",
        "        msk_list = []\n",
        "        state, done = env.reset(), False\n",
        "        episode_reward = 0\n",
        "        episode_timesteps = 0\n",
        "        episode_num += 1 \n",
        "\n",
        "    # Evaluate episode\n",
        "    if (t + 1) % args_eval_freq == 0:\n",
        "        evaluations.append(eval_policy(policy))\n",
        "        print('recent Evaluation:',evaluations[-1])\n",
        "        np.save('results/evaluations_alias{}_ENV{}'.format(alias,ENV_NAME),evaluations)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YAVa6S3yG5m5"
      },
      "source": [
        "# The following scripts run the TD3 algorithm.\n",
        "\n",
        "alias = 'td3'\n",
        "\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "import torch\n",
        "import gym\n",
        "import argparse\n",
        "import os\n",
        "import torch.nn.functional as F\n",
        "import utils\n",
        "import TD3\n",
        "import DDPG\n",
        "\n",
        "def eval_policy(policy, eval_episodes=10):\n",
        "    eval_env = gym.make(ENV_NAME)\n",
        "\n",
        "    avg_reward = 0.\n",
        "    for _ in range(eval_episodes):\n",
        "        state, done = eval_env.reset(), False\n",
        "        while not done:\n",
        "            action = policy.select_action(np.array(state))\n",
        "            state, reward, done,_ = eval_env.step(action)\n",
        "            avg_reward += reward\n",
        "\n",
        "    avg_reward /= eval_episodes\n",
        "    #print(\"---------------------------------------\")\n",
        "    #print(f\"Evaluation over {eval_episodes} episodes: {avg_reward:.3f}\")\n",
        "    #print(\"---------------------------------------\")\n",
        "    return avg_reward\n",
        "\n",
        "env = gym.make(ENV_NAME)\n",
        "torch.manual_seed(0)\n",
        "np.random.seed(0)\n",
        "\n",
        "state_dim = env.observation_space.shape[0]\n",
        "action_dim = env.action_space.shape[0]\n",
        "max_action = env.action_space.high[0]\n",
        "\n",
        "args_policy_noise = 0.2\n",
        "args_noise_clip = 0.5\n",
        "args_policy_freq = 2\n",
        "args_max_timesteps = 100000\n",
        "args_expl_noise = 0.1\n",
        "args_batch_size = 25\n",
        "args_eval_freq = 1000\n",
        "args_start_timesteps = 0\n",
        "\n",
        "kwargs = {\n",
        "    \"state_dim\": state_dim,\n",
        "    \"action_dim\": action_dim,\n",
        "    \"max_action\": max_action,\n",
        "    \"discount\": 0.99,\n",
        "    \"tau\": 0.005\n",
        "}\n",
        "\n",
        "\n",
        "args_policy = 'TD3'\n",
        "\n",
        "if args_policy == \"TD3\":\n",
        "    # Target policy smoothing is scaled wrt the action scale\n",
        "    kwargs[\"policy_noise\"] = args_policy_noise * max_action\n",
        "    kwargs[\"noise_clip\"] = args_noise_clip * max_action\n",
        "    kwargs[\"policy_freq\"] = args_policy_freq\n",
        "    policy = TD3.TD3(**kwargs)\n",
        "elif args_policy == \"OurDDPG\":\n",
        "    policy = OurDDPG.DDPG(**kwargs)\n",
        "elif args_policy == \"DDPG\":\n",
        "    policy = DDPG.DDPG(**kwargs)\n",
        "replay_buffer = utils.ReplayBuffer(state_dim, action_dim)\n",
        "\n",
        "# Evaluate untrained policy\n",
        "evaluations = [eval_policy(policy)]\n",
        "\n",
        "state, done = env.reset(), False\n",
        "episode_reward = 0\n",
        "episode_timesteps = 0\n",
        "episode_num = 0\n",
        "counter = 0\n",
        "msk_list = []        \n",
        "temp_curve = [eval_policy(policy)]\n",
        "temp_val = []\n",
        "for t in range(int(args_max_timesteps)):\n",
        "    episode_timesteps += 1\n",
        "    counter += 1\n",
        "    # Select action randomly or according to policy\n",
        "    if t < args_start_timesteps:\n",
        "        action = np.random.uniform(-max_action,max_action,action_dim)\n",
        "    else:\n",
        "        if np.random.uniform(0,1) < 0.1:\n",
        "            action = np.random.uniform(-max_action,max_action,action_dim)\n",
        "        else:\n",
        "            action = (\n",
        "                policy.select_action(np.array(state))\n",
        "                + np.random.normal(0, max_action * args_expl_noise, size=action_dim)\n",
        "            ).clip(-max_action, max_action)\n",
        "\n",
        "    # Perform action\n",
        "    next_state, reward, done,_ = env.step(action)\n",
        "    done_bool = float(done) if episode_timesteps < env._max_episode_steps else 0\n",
        "\n",
        "    replay_buffer.add(state, action, next_state, reward, done_bool)\n",
        "\n",
        "    state = next_state\n",
        "    episode_reward += reward\n",
        "\n",
        "    if t >= args_start_timesteps:\n",
        "        '''TD3'''\n",
        "        last_val = 999.\n",
        "        patient = 5\n",
        "        for i in range(1):\n",
        "            policy.train(replay_buffer, args_batch_size)\n",
        "                \n",
        "\n",
        "    # Train agent after collecting sufficient data\n",
        "    if done: \n",
        "        print(f\"Total T: {t+1} Episode Num: {episode_num+1} Episode T: {episode_timesteps} Reward: {episode_reward:.3f}\")\n",
        "        msk_list = []\n",
        "        state, done = env.reset(), False\n",
        "        episode_reward = 0\n",
        "        episode_timesteps = 0\n",
        "        episode_num += 1 \n",
        "\n",
        "    # Evaluate episode\n",
        "    if (t + 1) % args_eval_freq == 0:\n",
        "        evaluations.append(eval_policy(policy))\n",
        "        print('recent Evaluation:',evaluations[-1])\n",
        "        np.save('results/evaluations_alias{}_ENV{}'.format(alias,ENV_NAME),evaluations)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KboX9hB5eEVv"
      },
      "source": [
        "# Four-Solution-Maze Environment (optional)\n",
        "\n",
        "## TODOs for you:\n",
        "\n",
        "- Q11. (bonus) In this section, another environment named Four-Solution-Maze is provided for you to evaluate your algorithms.\n",
        "\n",
        "The task is quite simple, yet never easy for even PPO/TD3.\n",
        "\n",
        "The default size of the maze is 64x64, and in each game (espisode), the agent is initialized randomly in the maze. There are 4 positions in the maze that has non-trivial reward of +10, while reaching other region will recieve only a tiny punishment of -0.1. An optimal policy should be able to find the shortest path to the most recent reward region (i.e., one of the four high-reward regions.).\n",
        "\n",
        "The action space is continuous with range [-1,1], larger actions will be clipped."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kX6V-0bfMSev"
      },
      "source": [
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "from pylab import *\n",
        "from numpy import *\n",
        "import copy\n",
        "\n",
        "class FourWayGridWorld:\n",
        "    def __init__(self, N=17,left = 10,right = 10, up=10, down = 10):\n",
        "        self.N = N\n",
        "        self.left = left\n",
        "        self.right = right\n",
        "        self.up = up\n",
        "        self.down = down\n",
        "        self.map = np.ones((N,N))*(-0.1)\n",
        "        self.map[int((N-1)/2),0] = self.left\n",
        "        self.map[0,int((N-1)/2)] = self.up\n",
        "        self.map[N-1,int((N-1)/2)] = self.down\n",
        "        self.map[int((N-1)/2),N-1] = self.right\n",
        "        self.loc = np.asarray([np.random.randint(N),np.random.randint(N)])\n",
        "        self.step_num = 0\n",
        "    def step(self,action):\n",
        "        action = np.clip(action,-1,1)\n",
        "        new_loc = np.clip(self.loc + action,0,self.N-1)\n",
        "        self.loc = new_loc\n",
        "        reward = self.map[int(round(self.loc[0])),int(round(self.loc[1]))]\n",
        "        self.step_num+=1\n",
        "        return self.loc,reward,self.ifdone()\n",
        "    def ifdone(self):\n",
        "        if self.step_num >= 2*self.N:\n",
        "            return True\n",
        "        else:\n",
        "            return False\n",
        "    def render(self):\n",
        "        map_self = copy.deepcopy(self.map)\n",
        "        map_self[int(self.loc[0]),int(self.loc[1])] = -5\n",
        "        plt.imshow(map_self)\n",
        "    def reset(self):\n",
        "        self.map = np.ones((self.N,self.N))*(-0.1)\n",
        "        self.map[int((self.N-1)/2),0] = self.left\n",
        "        self.map[0,int((self.N-1)/2)] = self.up\n",
        "        self.map[self.N-1,int((self.N-1)/2)] = self.down\n",
        "        self.map[int((self.N-1)/2),self.N-1] = self.right\n",
        "        self.loc = np.asarray([np.random.randint(self.N),np.random.randint(self.N)])\n",
        "        self.step_num = 0\n",
        "        return self.loc\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hoO_Ti-zhykw"
      },
      "source": [
        "env = FourWayGridWorld(33)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "id": "rBzb2qw3h2ts",
        "outputId": "458ee588-3f5b-4f19-d45c-952b6b330a73"
      },
      "source": [
        "env.reset()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([30,  2])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 3
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 266
        },
        "id": "SBcwx2WPh3nT",
        "outputId": "1b60ea64-3859-4feb-8bbf-db6e72ca287e"
      },
      "source": [
        "env.render()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7WrHjDQmqC7K"
      },
      "source": [
        "# This section is used to visualize your learned policy\n",
        "from torch import Tensor\n",
        "output_i = np.zeros((33,33))\n",
        "output_j = np.zeros((33,33))\n",
        "output_i_m = np.zeros((33,33))\n",
        "output_j_m = np.zeros((33,33))\n",
        "value_ij = np.zeros((33,33))\n",
        "for i in range(33):\n",
        "    for j in range(33):\n",
        "        states = Tensor(np.asarray([i,j])).float().unsqueeze(0)\n",
        "        \n",
        "        '''\n",
        "        you need to revise the following line, \n",
        "        to fit your policy/network outputs\n",
        "        '''\n",
        "        action, value = policy(states)\n",
        "        output_i[i,j] = action[0]\n",
        "        output_j[i,j] = action[1]\n",
        "        value_ij[i,j] = value\n",
        "        \n",
        "plt.figure(figsize= (5,5))\n",
        "for i in range(33):\n",
        "    for j in range(33):\n",
        "        plt.arrow(j,-i,output_j[i,j],-output_i[i,j],head_width=0.2,shape='left')\n",
        "xlim(-1,33)\n",
        "ylim(-33,1)\n",
        "yticks([2*i-32 for i in range(17)],[2*i for i in range(17)])\n",
        "plt.xticks([])\n",
        "plt.yticks([])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "l5YEdvVdqDd7"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}